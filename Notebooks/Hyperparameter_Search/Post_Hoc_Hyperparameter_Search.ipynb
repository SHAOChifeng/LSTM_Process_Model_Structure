{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z29lKAJfqu2I",
        "outputId": "ab027d39-0886-445b-d33c-3e9867b78e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Bidirectional, LSTM, Activation, Dropout, Embedding, Input\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "\n",
        "import os.path\n",
        "\n",
        "def save_log(loglist, filename): #save a list of lists \n",
        "  df = pd.DataFrame.from_records(loglist)\n",
        "  df.to_csv(filename, index=False)\n",
        "\n",
        "def remove_nan(lists):\n",
        "  newlists = []\n",
        "  for tr in lists:\n",
        "    newlists.append([int(x) for x in tr if str(x) != 'nan'])\n",
        "  return(newlists)\n",
        "\n",
        "def import_log(filepath):\n",
        "  df = pd.read_csv(filepath)\n",
        "  return(remove_nan(df.values.tolist()))\n",
        "\n",
        "\n",
        "\n",
        "def number_to_one_hot_X(X, dict_size): #if we want \n",
        "  newX = []\n",
        "  for example in X:\n",
        "    new_ex = []\n",
        "    for i in range(len(example)):\n",
        "      onehot = [0]*dict_size #changed\n",
        "      if example[i] != 0:\n",
        "        onehot[example[i] - 1] = 1 #-1 because begin counting at 0\n",
        "      new_ex.append(onehot)\n",
        "    newX.append(new_ex)\n",
        "  return(np.array(newX))\n",
        "\n",
        "def create_XY_prefix(log, mappingsize, prefixlen):\n",
        "  X = []\n",
        "  Y = []\n",
        "  for i in range(0, len(log)):\n",
        "    for k in range(1, len(log[i])):\n",
        "      X.append(log[i][max(0, k-prefixlen):k]) #get the prefix of 'encoded' activities\n",
        "      y = [0] *(mappingsize)\n",
        "      y[int(log[i][k])-1] = 1\n",
        "      Y.append(y)        \n",
        "  X = keras.preprocessing.sequence.pad_sequences(X, maxlen=prefixlen, padding='pre')\n",
        "  X = number_to_one_hot_X(X, mappingsize)\n",
        "  return(np.array(X), np.array(Y))\n",
        "\n",
        "def get_startend(log): \n",
        "  return log[0][0], log[0][-1]\n",
        "\n",
        "def get_model(maxlen, num_chars, bidirec, n_layers, lstmsize, dropout, l1, l2):\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(maxlen, num_chars))) #If you don't use an embedding layer input should be one-hot-encoded\n",
        "  if bidirec == False:   \n",
        "    model.add(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=(n_layers != 1),kernel_regularizer=regularizers.l1_l2(l1,l2),\n",
        "                   recurrent_regularizer=regularizers.l1_l2(l1,l2),input_shape=(maxlen, num_chars)))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(1, n_layers):\n",
        "      return_sequences = (i+1 != n_layers)\n",
        "      model.add(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=return_sequences,\n",
        "                     kernel_regularizer=regularizers.l1_l2(l1,l2),recurrent_regularizer=regularizers.l1_l2(l1,l2)))\n",
        "      model.add(Dropout(dropout))\n",
        "  else:\n",
        "    model.add(Bidirectional(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=(n_layers != 1),kernel_regularizer=regularizers.l1_l2(l1,l2),\n",
        "                   recurrent_regularizer=regularizers.l1_l2(l1,l2),input_shape=(maxlen, num_chars))))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(1, n_layers):\n",
        "      return_sequences = (i+1 != n_layers)\n",
        "      model.add(Bidirectional(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=return_sequences,\n",
        "                     kernel_regularizer=regularizers.l1_l2(l1,l2),recurrent_regularizer=regularizers.l1_l2(l1,l2))))\n",
        "      model.add(Dropout(dropout))\n",
        "  model.add(Dense(num_chars, kernel_initializer='glorot_uniform',activation='softmax'))\n",
        "  opt = Adam(learning_rate=0.005)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics='accuracy')\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(X_train, y_train,batch_size, maxlen, num_chars, bidirec, n_layers, lstmsize, dropout, l1, l2):\n",
        "  model = get_model(maxlen, num_chars, bidirec, n_layers, lstmsize, dropout, l1, l2)\n",
        "  model.summary()\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "  #train_model\n",
        "  history = model.fit(X_train, y_train, validation_split=0.2, callbacks=[early_stopping, lr_reducer], batch_size=batch_size, epochs=600, verbose=2)\n",
        "  return model\n",
        "\n",
        "def cut_end(log, endact):\n",
        "  logsize, tracesize = log.shape\n",
        "  print(log.shape)\n",
        "  newlog = []\n",
        "  for i in range(0, logsize):\n",
        "    trace = []\n",
        "    for j in range(0, tracesize):\n",
        "      if log[i][j] == endact:\n",
        "        trace.append(log[i][j])\n",
        "        break\n",
        "      else:\n",
        "        trace.append(log[i][j])\n",
        "    newlog.append(trace)\n",
        "  return(newlog)\n",
        "\n",
        "def normalize(probs): #normalize probabilities to sum to 1\n",
        "  examplesize, actsize = probs.shape\n",
        "  newy = []\n",
        "  for i in range(examplesize):\n",
        "    normalizer = 1 / float( sum(probs[i]) )\n",
        "    ynorm = [float(l) * normalizer for l in probs[i]]\n",
        "    newy.append(ynorm)\n",
        "  return newy\n",
        "\n",
        "\n",
        "def choose_act_all(all_y): #randomly choose an activity, stochastically\n",
        "  #p want a list of probabilities    \n",
        "  chosen_acts = []\n",
        "  for i in range(len(all_y)):\n",
        "      chosen_acts.append(np.random.choice(np.arange(0, len(all_y[i])), p=all_y[i])+1)  \n",
        "  return(chosen_acts)   # +1 because number encodig starts at 1 not 0\n",
        "\n",
        "def OHget_probabilities(rnnmodel, xlists,  nr_act, maxlen, prefixlen):\n",
        "  #assume xlist is a list with the x (prefix) untill now \n",
        "  all_x = keras.preprocessing.sequence.pad_sequences(xlists, maxlen=maxlen, padding=\"pre\")\n",
        "  all_x = all_x[:,-(prefixlen):]\n",
        "  all_x = number_to_one_hot_X(all_x, nr_act)\n",
        "  results = rnnmodel.predict(all_x)\n",
        "  return results\n",
        "\n",
        "def OHsimulate_log(RNNmodel, logsize, startact, endact, maxlen, mapping, prefixlen): #Use RNN to simulate log\n",
        "  log = np.zeros((logsize, maxlen+1), int)\n",
        "  for i in range(0, logsize): #start every trace with the start activity\n",
        "    log[i][0] = startact\n",
        "  print(log)\n",
        "  for j in range(1,maxlen+1): #check if 0 or 1 and ml or ml - 1 #we took 50 for with loops   \n",
        "    print(\"finding activity nr\", j+1)   \n",
        "    prefixes = np.array([log[i][0:j] for i in range(0, logsize)])\n",
        "    print(prefixes)\n",
        "    probs = OHget_probabilities(RNNmodel, prefixes, len(mapping), maxlen, prefixlen)\n",
        "    #we need to do this because otherwise probabilities sum over 1 \n",
        "    ynorm = normalize(probs) \n",
        "    nextacts = choose_act_all(ynorm) \n",
        "    for i in range(0, logsize):\n",
        "      log[i][j] = nextacts[i]\n",
        "  print(log)\n",
        "  corrected_log = cut_end(log, endact)      \n",
        "  return(corrected_log) \n",
        "\n",
        "\n",
        "def do_grid_search_without_embedding_one_variant(variant, prefixlen):\n",
        "  trainname = '/content/drive/MyDrive/Grid/Training_logs/log_'+ str(variant) + '.csv'\n",
        "  #reload mapping\n",
        "  mappingfilename = '/content/drive/MyDrive/Grid/mapping.txt' \n",
        "  with open(mappingfilename) as f:\n",
        "    mapping = json.loads(f.read())\n",
        "\n",
        "  train_log = remove_nan(import_log(trainname))\n",
        "  X_train, y_train = create_XY_prefix(train_log, len(mapping), prefixlen)\n",
        "\n",
        "  maxlen = len(max(train_log,key=len))\n",
        "\n",
        "  bidirec = [True, False]\n",
        "  grid_nr_layers =  [1, 2]\n",
        "  grid_layersize = [16, 32, 64]\n",
        "  grid_dropout = [0.0, 0.2, 0.4, 0.6]\n",
        "  grid_reg = [0.0, 0.00001, 0.0001, 0.001, 0.01]\n",
        "  for bi in bidirec:\n",
        "    for num_layers in grid_nr_layers:\n",
        "      for layersize in grid_layersize:\n",
        "        for dropout in grid_dropout:\n",
        "          for reg in grid_reg:\n",
        "            if bi == False:\n",
        "              filename = '/content/drive/MyDrive/Grid/Simulated_logs/Var'+str(variant)+'/SIMLOG_NL'+str(num_layers)+'emb'+'N'+'LS'+str(layersize)+'D'+str(dropout).replace('0.', '')+'R'+str(reg).replace('.', '')+'.csv'\n",
        "            else:\n",
        "              filename = '/content/drive/MyDrive/Grid/Simulated_logs/Var'+str(variant)+'/SIMLOG_BIDIREC_NL'+str(num_layers)+'emb'+'N'+'LS'+str(layersize)+'D'+str(dropout).replace('0.', '')+'R'+str(reg).replace('.', '')+'.csv'\n",
        "            if os.path.exists(filename): \n",
        "              print(\"Already exists:\", num_layers, layersize, dropout, reg)\n",
        "              continue\n",
        "\n",
        "            batch_size = 128\n",
        "            model = train_model(X_train, y_train,batch_size, maxlen=prefixlen, num_chars=len(mapping), bidirec=bi, n_layers=num_layers, lstmsize=layersize, dropout=dropout, l1=reg, l2=reg)\n",
        "            model.summary()\n",
        "           \n",
        "            #save RNN model in case we ever need it\n",
        "            #modelfilename = '/content/drive/MyDrive/DataExperiment/Models/Var'+str(variant)+'/model_NL'+str(num_layers)+'emb'+'N'+'LS'+str(layersize)+'D'+str(dropout).replace('0.', '')+'R'+str(reg).replace('.', '')\n",
        "            #model.save(modelfilename)\n",
        "\n",
        "            start,end = get_startend(train_log)\n",
        "\n",
        "            simlog = OHsimulate_log(model, 12000, start, end, maxlen-1, mapping, prefixlen)\n",
        "\n",
        "            #save simulated log\n",
        "            save_log(simlog, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "37yu70kbuCfE",
        "outputId": "647f86be-cfc2-47fe-d091-cb38838ad62a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  4  1 10]\n",
            " [ 6  7  8 ...  4 11 10]\n",
            " [ 6  7 13 ...  2  2 11]\n",
            " ...\n",
            " [ 6  7  8 ...  1  1 10]\n",
            " [ 6  7  8 ...  2  4 10]\n",
            " [ 6  7  8 ...  4  3 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  1 10  5]\n",
            " [ 6  7  8 ... 11 10  5]\n",
            " [ 6  7 13 ...  2 11 10]\n",
            " ...\n",
            " [ 6  7  8 ...  1 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  3 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7 13 ... 11 10 11]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7 13 ... 10 11  7]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Already exists: 1 64 0.0 0.0\n",
            "Already exists: 1 64 0.0 1e-05\n",
            "Already exists: 1 64 0.0 0.0001\n",
            "Already exists: 1 64 0.0 0.001\n",
            "Already exists: 1 64 0.0 0.01\n",
            "Already exists: 1 64 0.2 0.0\n",
            "Already exists: 1 64 0.2 1e-05\n",
            "Already exists: 1 64 0.2 0.0001\n",
            "Already exists: 1 64 0.2 0.001\n",
            "Already exists: 1 64 0.2 0.01\n",
            "Already exists: 1 64 0.4 0.0\n",
            "Already exists: 1 64 0.4 1e-05\n",
            "Already exists: 1 64 0.4 0.0001\n",
            "Already exists: 1 64 0.4 0.001\n",
            "Already exists: 1 64 0.4 0.01\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_28 (LSTM)              (None, 64)                19968     \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,813\n",
            "Trainable params: 20,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 15s - loss: 0.5639 - accuracy: 0.7442 - val_loss: 0.4105 - val_accuracy: 0.7736 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 12s - loss: 0.4384 - accuracy: 0.7734 - val_loss: 0.4061 - val_accuracy: 0.7757 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 12s - loss: 0.4230 - accuracy: 0.7734 - val_loss: 0.4029 - val_accuracy: 0.7757 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 12s - loss: 0.4175 - accuracy: 0.7729 - val_loss: 0.4035 - val_accuracy: 0.7745 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 12s - loss: 0.4142 - accuracy: 0.7734 - val_loss: 0.4026 - val_accuracy: 0.7726 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 12s - loss: 0.4159 - accuracy: 0.7749 - val_loss: 0.3995 - val_accuracy: 0.7774 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 12s - loss: 0.4107 - accuracy: 0.7731 - val_loss: 0.3996 - val_accuracy: 0.7766 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 12s - loss: 0.4095 - accuracy: 0.7735 - val_loss: 0.4007 - val_accuracy: 0.7730 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 12s - loss: 0.4083 - accuracy: 0.7740 - val_loss: 0.3998 - val_accuracy: 0.7745 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 12s - loss: 0.4062 - accuracy: 0.7733 - val_loss: 0.3997 - val_accuracy: 0.7759 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 12s - loss: 0.4056 - accuracy: 0.7757 - val_loss: 0.4001 - val_accuracy: 0.7723 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 13s - loss: 0.4051 - accuracy: 0.7743 - val_loss: 0.3988 - val_accuracy: 0.7749 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 12s - loss: 0.4050 - accuracy: 0.7748 - val_loss: 0.3994 - val_accuracy: 0.7758 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 12s - loss: 0.4046 - accuracy: 0.7741 - val_loss: 0.3989 - val_accuracy: 0.7765 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 12s - loss: 0.4043 - accuracy: 0.7746 - val_loss: 0.3993 - val_accuracy: 0.7735 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 13s - loss: 0.4034 - accuracy: 0.7738 - val_loss: 0.3986 - val_accuracy: 0.7759 - lr: 0.0012 - 13s/epoch - 14ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 13s - loss: 0.4030 - accuracy: 0.7753 - val_loss: 0.3984 - val_accuracy: 0.7765 - lr: 0.0012 - 13s/epoch - 15ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 12s - loss: 0.4027 - accuracy: 0.7755 - val_loss: 0.3987 - val_accuracy: 0.7747 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 12s - loss: 0.4029 - accuracy: 0.7747 - val_loss: 0.3987 - val_accuracy: 0.7748 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 13s - loss: 0.4022 - accuracy: 0.7770 - val_loss: 0.3991 - val_accuracy: 0.7729 - lr: 0.0012 - 13s/epoch - 14ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 12s - loss: 0.4021 - accuracy: 0.7752 - val_loss: 0.3984 - val_accuracy: 0.7760 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 13s - loss: 0.4021 - accuracy: 0.7752 - val_loss: 0.3985 - val_accuracy: 0.7755 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 13s - loss: 0.4022 - accuracy: 0.7746 - val_loss: 0.3984 - val_accuracy: 0.7757 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 13s - loss: 0.4016 - accuracy: 0.7771 - val_loss: 0.3984 - val_accuracy: 0.7758 - lr: 3.1250e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 12s - loss: 0.4019 - accuracy: 0.7758 - val_loss: 0.3983 - val_accuracy: 0.7736 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 12s - loss: 0.4017 - accuracy: 0.7766 - val_loss: 0.3984 - val_accuracy: 0.7744 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 13s - loss: 0.4012 - accuracy: 0.7773 - val_loss: 0.3984 - val_accuracy: 0.7744 - lr: 1.5625e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 13s - loss: 0.4013 - accuracy: 0.7773 - val_loss: 0.3983 - val_accuracy: 0.7749 - lr: 1.5625e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 13s - loss: 0.4014 - accuracy: 0.7773 - val_loss: 0.3984 - val_accuracy: 0.7761 - lr: 1.5625e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 12s - loss: 0.4012 - accuracy: 0.7780 - val_loss: 0.3984 - val_accuracy: 0.7758 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 12s - loss: 0.4012 - accuracy: 0.7768 - val_loss: 0.3984 - val_accuracy: 0.7752 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_28 (LSTM)              (None, 64)                19968     \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,813\n",
            "Trainable params: 20,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13  4]\n",
            " [ 6  7  8 13  2]\n",
            " [ 6  7  8 13  1]\n",
            " ...\n",
            " [ 6  7  8 13 11]\n",
            " [ 6  7  8 13  2]\n",
            " [ 6  7  8 13  1]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13  4  2]\n",
            " [ 6  7  8 13  2  1]\n",
            " [ 6  7  8 13  1  2]\n",
            " ...\n",
            " [ 6  7  8 13 11  3]\n",
            " [ 6  7  8 13  2  4]\n",
            " [ 6  7  8 13  1  3]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ...  4  2  1]\n",
            " [ 6  7  8 ...  2  1  3]\n",
            " [ 6  7  8 ...  1  2  3]\n",
            " ...\n",
            " [ 6  7  8 ... 11  3  4]\n",
            " [ 6  7  8 ...  2  4 11]\n",
            " [ 6  7  8 ...  1  3  2]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  2  1  3]\n",
            " [ 6  7  8 ...  1  3 11]\n",
            " [ 6  7  8 ...  2  3  4]\n",
            " ...\n",
            " [ 6  7  8 ...  3  4  1]\n",
            " [ 6  7  8 ...  4 11  1]\n",
            " [ 6  7  8 ...  3  2  4]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  1  3 11]\n",
            " [ 6  7  8 ...  3 11  4]\n",
            " [ 6  7  8 ...  3  4 11]\n",
            " ...\n",
            " [ 6  7  8 ...  4  1  2]\n",
            " [ 6  7  8 ... 11  1  3]\n",
            " [ 6  7  8 ...  2  4 11]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  3 11 10]\n",
            " [ 6  7  8 ... 11  4 10]\n",
            " [ 6  7  8 ...  4 11 10]\n",
            " ...\n",
            " [ 6  7  8 ...  1  2 10]\n",
            " [ 6  7  8 ...  1  3 10]\n",
            " [ 6  7  8 ...  4 11 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ... 11 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ... 11 10  5]\n",
            " ...\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ...  3 10  5]\n",
            " [ 6  7  8 ... 11 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_29 (LSTM)              (None, 64)                19968     \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,813\n",
            "Trainable params: 20,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 17s - loss: 0.6123 - accuracy: 0.7409 - val_loss: 0.4398 - val_accuracy: 0.7744 - lr: 0.0050 - 17s/epoch - 19ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 12s - loss: 0.4665 - accuracy: 0.7744 - val_loss: 0.4357 - val_accuracy: 0.7755 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 13s - loss: 0.4704 - accuracy: 0.7730 - val_loss: 0.4349 - val_accuracy: 0.7757 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 12s - loss: 0.4517 - accuracy: 0.7731 - val_loss: 0.4293 - val_accuracy: 0.7751 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 12s - loss: 0.4462 - accuracy: 0.7736 - val_loss: 0.4263 - val_accuracy: 0.7741 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 12s - loss: 0.4541 - accuracy: 0.7735 - val_loss: 0.4322 - val_accuracy: 0.7717 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 12s - loss: 0.4438 - accuracy: 0.7725 - val_loss: 0.4248 - val_accuracy: 0.7743 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 13s - loss: 0.4398 - accuracy: 0.7741 - val_loss: 0.4239 - val_accuracy: 0.7734 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 12s - loss: 0.4388 - accuracy: 0.7755 - val_loss: 0.4251 - val_accuracy: 0.7755 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 12s - loss: 0.4442 - accuracy: 0.7735 - val_loss: 0.4266 - val_accuracy: 0.7746 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 12s - loss: 0.4379 - accuracy: 0.7744 - val_loss: 0.4216 - val_accuracy: 0.7763 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 12s - loss: 0.4355 - accuracy: 0.7726 - val_loss: 0.4219 - val_accuracy: 0.7758 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 12s - loss: 0.4344 - accuracy: 0.7738 - val_loss: 0.4225 - val_accuracy: 0.7718 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 13s - loss: 0.4483 - accuracy: 0.7735 - val_loss: 0.4339 - val_accuracy: 0.7736 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 13s - loss: 0.4388 - accuracy: 0.7737 - val_loss: 0.4251 - val_accuracy: 0.7749 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 12s - loss: 0.4337 - accuracy: 0.7748 - val_loss: 0.4228 - val_accuracy: 0.7736 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 12s - loss: 0.4311 - accuracy: 0.7745 - val_loss: 0.4191 - val_accuracy: 0.7757 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 12s - loss: 0.4295 - accuracy: 0.7749 - val_loss: 0.4177 - val_accuracy: 0.7726 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 12s - loss: 0.4284 - accuracy: 0.7727 - val_loss: 0.4174 - val_accuracy: 0.7752 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 12s - loss: 0.4277 - accuracy: 0.7751 - val_loss: 0.4160 - val_accuracy: 0.7749 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 12s - loss: 0.4271 - accuracy: 0.7743 - val_loss: 0.4160 - val_accuracy: 0.7746 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 12s - loss: 0.4271 - accuracy: 0.7738 - val_loss: 0.4160 - val_accuracy: 0.7767 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 12s - loss: 0.4255 - accuracy: 0.7735 - val_loss: 0.4150 - val_accuracy: 0.7738 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 12s - loss: 0.4257 - accuracy: 0.7737 - val_loss: 0.4161 - val_accuracy: 0.7745 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 12s - loss: 0.4250 - accuracy: 0.7739 - val_loss: 0.4150 - val_accuracy: 0.7754 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 12s - loss: 0.4279 - accuracy: 0.7729 - val_loss: 0.4169 - val_accuracy: 0.7753 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 13s - loss: 0.4229 - accuracy: 0.7755 - val_loss: 0.4143 - val_accuracy: 0.7753 - lr: 0.0012 - 13s/epoch - 14ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 13s - loss: 0.4212 - accuracy: 0.7756 - val_loss: 0.4135 - val_accuracy: 0.7746 - lr: 0.0012 - 13s/epoch - 14ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 12s - loss: 0.4207 - accuracy: 0.7760 - val_loss: 0.4129 - val_accuracy: 0.7745 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 12s - loss: 0.4206 - accuracy: 0.7738 - val_loss: 0.4126 - val_accuracy: 0.7737 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 12s - loss: 0.4200 - accuracy: 0.7747 - val_loss: 0.4129 - val_accuracy: 0.7756 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 12s - loss: 0.4199 - accuracy: 0.7744 - val_loss: 0.4123 - val_accuracy: 0.7744 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 12s - loss: 0.4197 - accuracy: 0.7746 - val_loss: 0.4120 - val_accuracy: 0.7751 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 13s - loss: 0.4193 - accuracy: 0.7753 - val_loss: 0.4123 - val_accuracy: 0.7746 - lr: 0.0012 - 13s/epoch - 14ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 12s - loss: 0.4187 - accuracy: 0.7747 - val_loss: 0.4131 - val_accuracy: 0.7749 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 12s - loss: 0.4186 - accuracy: 0.7760 - val_loss: 0.4127 - val_accuracy: 0.7748 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 13s - loss: 0.4174 - accuracy: 0.7752 - val_loss: 0.4105 - val_accuracy: 0.7745 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 12s - loss: 0.4168 - accuracy: 0.7742 - val_loss: 0.4102 - val_accuracy: 0.7749 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 12s - loss: 0.4168 - accuracy: 0.7737 - val_loss: 0.4099 - val_accuracy: 0.7750 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 12s - loss: 0.4159 - accuracy: 0.7754 - val_loss: 0.4098 - val_accuracy: 0.7758 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 12s - loss: 0.4161 - accuracy: 0.7750 - val_loss: 0.4100 - val_accuracy: 0.7751 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 12s - loss: 0.4160 - accuracy: 0.7749 - val_loss: 0.4099 - val_accuracy: 0.7744 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 12s - loss: 0.4149 - accuracy: 0.7748 - val_loss: 0.4090 - val_accuracy: 0.7754 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 12s - loss: 0.4146 - accuracy: 0.7763 - val_loss: 0.4096 - val_accuracy: 0.7743 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 12s - loss: 0.4141 - accuracy: 0.7752 - val_loss: 0.4092 - val_accuracy: 0.7729 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 12s - loss: 0.4144 - accuracy: 0.7756 - val_loss: 0.4086 - val_accuracy: 0.7762 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 12s - loss: 0.4144 - accuracy: 0.7742 - val_loss: 0.4090 - val_accuracy: 0.7752 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 12s - loss: 0.4142 - accuracy: 0.7750 - val_loss: 0.4088 - val_accuracy: 0.7732 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 12s - loss: 0.4142 - accuracy: 0.7754 - val_loss: 0.4086 - val_accuracy: 0.7731 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 12s - loss: 0.4137 - accuracy: 0.7758 - val_loss: 0.4082 - val_accuracy: 0.7739 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 12s - loss: 0.4135 - accuracy: 0.7748 - val_loss: 0.4081 - val_accuracy: 0.7742 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 12s - loss: 0.4134 - accuracy: 0.7757 - val_loss: 0.4083 - val_accuracy: 0.7731 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 12s - loss: 0.4131 - accuracy: 0.7751 - val_loss: 0.4080 - val_accuracy: 0.7764 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 12s - loss: 0.4131 - accuracy: 0.7759 - val_loss: 0.4083 - val_accuracy: 0.7746 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 12s - loss: 0.4135 - accuracy: 0.7739 - val_loss: 0.4079 - val_accuracy: 0.7742 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 12s - loss: 0.4126 - accuracy: 0.7757 - val_loss: 0.4081 - val_accuracy: 0.7750 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 13s - loss: 0.4126 - accuracy: 0.7752 - val_loss: 0.4080 - val_accuracy: 0.7763 - lr: 7.8125e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 12s - loss: 0.4127 - accuracy: 0.7746 - val_loss: 0.4079 - val_accuracy: 0.7737 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 12s - loss: 0.4122 - accuracy: 0.7766 - val_loss: 0.4078 - val_accuracy: 0.7747 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 12s - loss: 0.4122 - accuracy: 0.7773 - val_loss: 0.4079 - val_accuracy: 0.7735 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 13s - loss: 0.4125 - accuracy: 0.7754 - val_loss: 0.4078 - val_accuracy: 0.7739 - lr: 7.8125e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 12s - loss: 0.4124 - accuracy: 0.7764 - val_loss: 0.4078 - val_accuracy: 0.7726 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 13s - loss: 0.4124 - accuracy: 0.7759 - val_loss: 0.4076 - val_accuracy: 0.7739 - lr: 3.9062e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 13s - loss: 0.4127 - accuracy: 0.7743 - val_loss: 0.4076 - val_accuracy: 0.7739 - lr: 3.9062e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 12s - loss: 0.4123 - accuracy: 0.7760 - val_loss: 0.4077 - val_accuracy: 0.7740 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 13s - loss: 0.4123 - accuracy: 0.7746 - val_loss: 0.4076 - val_accuracy: 0.7727 - lr: 3.9062e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 13s - loss: 0.4122 - accuracy: 0.7759 - val_loss: 0.4076 - val_accuracy: 0.7743 - lr: 1.9531e-05 - 13s/epoch - 15ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 12s - loss: 0.4119 - accuracy: 0.7768 - val_loss: 0.4075 - val_accuracy: 0.7740 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 13s - loss: 0.4120 - accuracy: 0.7749 - val_loss: 0.4075 - val_accuracy: 0.7749 - lr: 1.9531e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 70/600\n",
            "892/892 - 12s - loss: 0.4123 - accuracy: 0.7759 - val_loss: 0.4075 - val_accuracy: 0.7758 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 71/600\n",
            "892/892 - 13s - loss: 0.4122 - accuracy: 0.7756 - val_loss: 0.4075 - val_accuracy: 0.7742 - lr: 1.9531e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 72/600\n",
            "892/892 - 13s - loss: 0.4119 - accuracy: 0.7775 - val_loss: 0.4075 - val_accuracy: 0.7738 - lr: 9.7656e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 73/600\n",
            "892/892 - 13s - loss: 0.4119 - accuracy: 0.7767 - val_loss: 0.4075 - val_accuracy: 0.7742 - lr: 9.7656e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 74/600\n",
            "892/892 - 12s - loss: 0.4121 - accuracy: 0.7762 - val_loss: 0.4075 - val_accuracy: 0.7738 - lr: 9.7656e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 75/600\n",
            "892/892 - 13s - loss: 0.4117 - accuracy: 0.7768 - val_loss: 0.4075 - val_accuracy: 0.7738 - lr: 4.8828e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 76/600\n",
            "892/892 - 13s - loss: 0.4117 - accuracy: 0.7767 - val_loss: 0.4075 - val_accuracy: 0.7741 - lr: 4.8828e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 77/600\n",
            "892/892 - 13s - loss: 0.4121 - accuracy: 0.7758 - val_loss: 0.4075 - val_accuracy: 0.7739 - lr: 4.8828e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 78/600\n",
            "892/892 - 13s - loss: 0.4117 - accuracy: 0.7764 - val_loss: 0.4075 - val_accuracy: 0.7744 - lr: 2.4414e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 79/600\n",
            "892/892 - 12s - loss: 0.4121 - accuracy: 0.7752 - val_loss: 0.4075 - val_accuracy: 0.7744 - lr: 2.4414e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 80/600\n",
            "892/892 - 13s - loss: 0.4118 - accuracy: 0.7763 - val_loss: 0.4075 - val_accuracy: 0.7744 - lr: 2.4414e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 81/600\n",
            "892/892 - 12s - loss: 0.4117 - accuracy: 0.7775 - val_loss: 0.4075 - val_accuracy: 0.7744 - lr: 1.2207e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 82/600\n",
            "892/892 - 12s - loss: 0.4118 - accuracy: 0.7772 - val_loss: 0.4075 - val_accuracy: 0.7744 - lr: 1.2207e-06 - 12s/epoch - 14ms/step\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_29 (LSTM)              (None, 64)                19968     \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,813\n",
            "Trainable params: 20,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13  4]\n",
            " [ 6  7  8 13  4]\n",
            " [ 6  7  8 13  3]\n",
            " ...\n",
            " [ 6  7  8 13  4]\n",
            " [ 6  7  8 13  2]\n",
            " [ 6  7  8 13  3]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13  4 11]\n",
            " [ 6  7  8 13  4 11]\n",
            " [ 6  7  8 13  3  1]\n",
            " ...\n",
            " [ 6  7  8 13  4  1]\n",
            " [ 6  7  8 13  2  4]\n",
            " [ 6  7  8 13  3  2]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ...  4 11  2]\n",
            " [ 6  7  8 ...  4 11  3]\n",
            " [ 6  7  8 ...  3  1 11]\n",
            " ...\n",
            " [ 6  7  8 ...  4  1  3]\n",
            " [ 6  7  8 ...  2  4  3]\n",
            " [ 6  7  8 ...  3  2 11]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ... 11  2  3]\n",
            " [ 6  7  8 ... 11  3  2]\n",
            " [ 6  7  8 ...  1 11  4]\n",
            " ...\n",
            " [ 6  7  8 ...  1  3  2]\n",
            " [ 6  7  8 ...  4  3 11]\n",
            " [ 6  7  8 ...  2 11  4]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  2  3  1]\n",
            " [ 6  7  8 ...  3  2  1]\n",
            " [ 6  7  8 ... 11  4  2]\n",
            " ...\n",
            " [ 6  7  8 ...  3  2 11]\n",
            " [ 6  7  8 ...  3 11  1]\n",
            " [ 6  7  8 ... 11  4  1]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  3  1 10]\n",
            " [ 6  7  8 ...  2  1 10]\n",
            " [ 6  7  8 ...  4  2 10]\n",
            " ...\n",
            " [ 6  7  8 ...  2 11 10]\n",
            " [ 6  7  8 ... 11  1 10]\n",
            " [ 6  7  8 ...  4  1 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  1 10  5]\n",
            " [ 6  7  8 ...  1 10  5]\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " ...\n",
            " [ 6  7  8 ... 11 10  5]\n",
            " [ 6  7  8 ...  1 10  5]\n",
            " [ 6  7  8 ...  1 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_30 (LSTM)              (None, 64)                19968     \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,813\n",
            "Trainable params: 20,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 15s - loss: 0.7029 - accuracy: 0.7392 - val_loss: 0.5041 - val_accuracy: 0.7746 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 12s - loss: 0.5487 - accuracy: 0.7718 - val_loss: 0.4856 - val_accuracy: 0.7760 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 12s - loss: 0.5302 - accuracy: 0.7712 - val_loss: 0.5619 - val_accuracy: 0.7730 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 13s - loss: 0.5221 - accuracy: 0.7730 - val_loss: 0.4741 - val_accuracy: 0.7743 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 13s - loss: 0.4989 - accuracy: 0.7731 - val_loss: 0.4628 - val_accuracy: 0.7737 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 13s - loss: 0.4902 - accuracy: 0.7737 - val_loss: 0.4581 - val_accuracy: 0.7783 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 12s - loss: 0.5163 - accuracy: 0.7702 - val_loss: 0.5011 - val_accuracy: 0.7751 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 13s - loss: 0.4939 - accuracy: 0.7747 - val_loss: 0.4556 - val_accuracy: 0.7734 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 13s - loss: 0.4808 - accuracy: 0.7736 - val_loss: 0.4562 - val_accuracy: 0.7703 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 13s - loss: 0.4784 - accuracy: 0.7750 - val_loss: 0.4558 - val_accuracy: 0.7749 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 13s - loss: 0.4760 - accuracy: 0.7746 - val_loss: 0.4530 - val_accuracy: 0.7724 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 12s - loss: 0.4927 - accuracy: 0.7716 - val_loss: 0.4580 - val_accuracy: 0.7731 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 13s - loss: 0.4721 - accuracy: 0.7741 - val_loss: 0.4537 - val_accuracy: 0.7760 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 13s - loss: 0.4694 - accuracy: 0.7740 - val_loss: 0.4449 - val_accuracy: 0.7756 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 13s - loss: 0.4670 - accuracy: 0.7749 - val_loss: 0.4477 - val_accuracy: 0.7733 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 13s - loss: 0.4990 - accuracy: 0.7706 - val_loss: 0.4651 - val_accuracy: 0.7747 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 13s - loss: 0.4731 - accuracy: 0.7740 - val_loss: 0.4462 - val_accuracy: 0.7757 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 13s - loss: 0.4573 - accuracy: 0.7748 - val_loss: 0.4367 - val_accuracy: 0.7763 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 13s - loss: 0.4553 - accuracy: 0.7748 - val_loss: 0.4369 - val_accuracy: 0.7743 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 13s - loss: 0.4541 - accuracy: 0.7723 - val_loss: 0.4333 - val_accuracy: 0.7744 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 12s - loss: 0.4524 - accuracy: 0.7742 - val_loss: 0.4350 - val_accuracy: 0.7742 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 13s - loss: 0.4514 - accuracy: 0.7741 - val_loss: 0.4339 - val_accuracy: 0.7739 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 13s - loss: 0.4502 - accuracy: 0.7755 - val_loss: 0.4388 - val_accuracy: 0.7715 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 12s - loss: 0.4426 - accuracy: 0.7748 - val_loss: 0.4265 - val_accuracy: 0.7746 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 13s - loss: 0.4409 - accuracy: 0.7745 - val_loss: 0.4266 - val_accuracy: 0.7749 - lr: 0.0012 - 13s/epoch - 15ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 12s - loss: 0.4401 - accuracy: 0.7740 - val_loss: 0.4262 - val_accuracy: 0.7745 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 13s - loss: 0.4390 - accuracy: 0.7743 - val_loss: 0.4269 - val_accuracy: 0.7741 - lr: 0.0012 - 13s/epoch - 14ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 13s - loss: 0.4383 - accuracy: 0.7749 - val_loss: 0.4246 - val_accuracy: 0.7758 - lr: 0.0012 - 13s/epoch - 15ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 13s - loss: 0.4383 - accuracy: 0.7742 - val_loss: 0.4252 - val_accuracy: 0.7756 - lr: 0.0012 - 13s/epoch - 14ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 12s - loss: 0.4377 - accuracy: 0.7735 - val_loss: 0.4273 - val_accuracy: 0.7733 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 12s - loss: 0.4372 - accuracy: 0.7740 - val_loss: 0.4256 - val_accuracy: 0.7728 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 13s - loss: 0.4327 - accuracy: 0.7742 - val_loss: 0.4244 - val_accuracy: 0.7755 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 13s - loss: 0.4319 - accuracy: 0.7730 - val_loss: 0.4198 - val_accuracy: 0.7745 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 12s - loss: 0.4309 - accuracy: 0.7751 - val_loss: 0.4209 - val_accuracy: 0.7768 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 13s - loss: 0.4307 - accuracy: 0.7745 - val_loss: 0.4217 - val_accuracy: 0.7749 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 12s - loss: 0.4301 - accuracy: 0.7745 - val_loss: 0.4193 - val_accuracy: 0.7739 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 13s - loss: 0.4302 - accuracy: 0.7735 - val_loss: 0.4206 - val_accuracy: 0.7743 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 13s - loss: 0.4294 - accuracy: 0.7746 - val_loss: 0.4181 - val_accuracy: 0.7754 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 12s - loss: 0.4293 - accuracy: 0.7745 - val_loss: 0.4220 - val_accuracy: 0.7752 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 12s - loss: 0.4292 - accuracy: 0.7739 - val_loss: 0.4201 - val_accuracy: 0.7702 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 13s - loss: 0.4291 - accuracy: 0.7735 - val_loss: 0.4176 - val_accuracy: 0.7732 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 13s - loss: 0.4289 - accuracy: 0.7738 - val_loss: 0.4203 - val_accuracy: 0.7740 - lr: 6.2500e-04 - 13s/epoch - 15ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 12s - loss: 0.4287 - accuracy: 0.7736 - val_loss: 0.4180 - val_accuracy: 0.7756 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 13s - loss: 0.4286 - accuracy: 0.7736 - val_loss: 0.4198 - val_accuracy: 0.7733 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 14s - loss: 0.4252 - accuracy: 0.7762 - val_loss: 0.4159 - val_accuracy: 0.7752 - lr: 3.1250e-04 - 14s/epoch - 16ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 13s - loss: 0.4256 - accuracy: 0.7724 - val_loss: 0.4157 - val_accuracy: 0.7757 - lr: 3.1250e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 13s - loss: 0.4247 - accuracy: 0.7752 - val_loss: 0.4157 - val_accuracy: 0.7761 - lr: 3.1250e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 12s - loss: 0.4243 - accuracy: 0.7746 - val_loss: 0.4154 - val_accuracy: 0.7749 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 13s - loss: 0.4240 - accuracy: 0.7750 - val_loss: 0.4158 - val_accuracy: 0.7736 - lr: 3.1250e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 13s - loss: 0.4240 - accuracy: 0.7742 - val_loss: 0.4155 - val_accuracy: 0.7757 - lr: 3.1250e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 13s - loss: 0.4240 - accuracy: 0.7746 - val_loss: 0.4149 - val_accuracy: 0.7738 - lr: 3.1250e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 13s - loss: 0.4239 - accuracy: 0.7724 - val_loss: 0.4162 - val_accuracy: 0.7726 - lr: 3.1250e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 12s - loss: 0.4237 - accuracy: 0.7744 - val_loss: 0.4153 - val_accuracy: 0.7762 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 12s - loss: 0.4238 - accuracy: 0.7740 - val_loss: 0.4151 - val_accuracy: 0.7745 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 12s - loss: 0.4218 - accuracy: 0.7749 - val_loss: 0.4142 - val_accuracy: 0.7765 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 13s - loss: 0.4213 - accuracy: 0.7750 - val_loss: 0.4142 - val_accuracy: 0.7735 - lr: 1.5625e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 12s - loss: 0.4214 - accuracy: 0.7738 - val_loss: 0.4135 - val_accuracy: 0.7748 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 12s - loss: 0.4214 - accuracy: 0.7742 - val_loss: 0.4135 - val_accuracy: 0.7762 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 12s - loss: 0.4206 - accuracy: 0.7750 - val_loss: 0.4139 - val_accuracy: 0.7760 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 12s - loss: 0.4207 - accuracy: 0.7745 - val_loss: 0.4143 - val_accuracy: 0.7753 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 13s - loss: 0.4199 - accuracy: 0.7742 - val_loss: 0.4129 - val_accuracy: 0.7756 - lr: 7.8125e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 12s - loss: 0.4199 - accuracy: 0.7748 - val_loss: 0.4134 - val_accuracy: 0.7735 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 12s - loss: 0.4199 - accuracy: 0.7739 - val_loss: 0.4133 - val_accuracy: 0.7730 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 13s - loss: 0.4193 - accuracy: 0.7763 - val_loss: 0.4132 - val_accuracy: 0.7739 - lr: 7.8125e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 12s - loss: 0.4188 - accuracy: 0.7753 - val_loss: 0.4126 - val_accuracy: 0.7738 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 13s - loss: 0.4186 - accuracy: 0.7753 - val_loss: 0.4126 - val_accuracy: 0.7757 - lr: 3.9062e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 12s - loss: 0.4187 - accuracy: 0.7750 - val_loss: 0.4125 - val_accuracy: 0.7753 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 13s - loss: 0.4189 - accuracy: 0.7739 - val_loss: 0.4126 - val_accuracy: 0.7747 - lr: 3.9062e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 12s - loss: 0.4186 - accuracy: 0.7759 - val_loss: 0.4127 - val_accuracy: 0.7752 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 70/600\n",
            "892/892 - 12s - loss: 0.4189 - accuracy: 0.7745 - val_loss: 0.4126 - val_accuracy: 0.7740 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 71/600\n",
            "892/892 - 13s - loss: 0.4182 - accuracy: 0.7756 - val_loss: 0.4125 - val_accuracy: 0.7733 - lr: 1.9531e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 72/600\n",
            "892/892 - 12s - loss: 0.4182 - accuracy: 0.7748 - val_loss: 0.4123 - val_accuracy: 0.7762 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 73/600\n",
            "892/892 - 13s - loss: 0.4183 - accuracy: 0.7751 - val_loss: 0.4124 - val_accuracy: 0.7744 - lr: 1.9531e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 74/600\n",
            "892/892 - 12s - loss: 0.4183 - accuracy: 0.7759 - val_loss: 0.4123 - val_accuracy: 0.7755 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 75/600\n",
            "892/892 - 13s - loss: 0.4178 - accuracy: 0.7775 - val_loss: 0.4123 - val_accuracy: 0.7758 - lr: 1.9531e-05 - 13s/epoch - 14ms/step\n",
            "Epoch 76/600\n",
            "892/892 - 13s - loss: 0.4177 - accuracy: 0.7759 - val_loss: 0.4122 - val_accuracy: 0.7767 - lr: 9.7656e-06 - 13s/epoch - 15ms/step\n",
            "Epoch 77/600\n",
            "892/892 - 13s - loss: 0.4179 - accuracy: 0.7757 - val_loss: 0.4122 - val_accuracy: 0.7761 - lr: 9.7656e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 78/600\n",
            "892/892 - 13s - loss: 0.4179 - accuracy: 0.7756 - val_loss: 0.4121 - val_accuracy: 0.7760 - lr: 9.7656e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 79/600\n",
            "892/892 - 12s - loss: 0.4177 - accuracy: 0.7768 - val_loss: 0.4121 - val_accuracy: 0.7754 - lr: 9.7656e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 80/600\n",
            "892/892 - 13s - loss: 0.4179 - accuracy: 0.7759 - val_loss: 0.4122 - val_accuracy: 0.7749 - lr: 9.7656e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 81/600\n",
            "892/892 - 13s - loss: 0.4176 - accuracy: 0.7762 - val_loss: 0.4122 - val_accuracy: 0.7751 - lr: 9.7656e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 82/600\n",
            "892/892 - 12s - loss: 0.4178 - accuracy: 0.7760 - val_loss: 0.4122 - val_accuracy: 0.7763 - lr: 4.8828e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 83/600\n",
            "892/892 - 13s - loss: 0.4177 - accuracy: 0.7751 - val_loss: 0.4121 - val_accuracy: 0.7760 - lr: 4.8828e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 84/600\n",
            "892/892 - 13s - loss: 0.4180 - accuracy: 0.7745 - val_loss: 0.4121 - val_accuracy: 0.7765 - lr: 4.8828e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 85/600\n",
            "892/892 - 16s - loss: 0.4176 - accuracy: 0.7768 - val_loss: 0.4121 - val_accuracy: 0.7758 - lr: 2.4414e-06 - 16s/epoch - 18ms/step\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_30 (LSTM)              (None, 64)                19968     \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,813\n",
            "Trainable params: 20,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13 11]\n",
            " [ 6  7  8 13 11]\n",
            " [ 6  7  8 13 11]\n",
            " ...\n",
            " [ 6  7  8 13  4]\n",
            " [ 6  7  8 13  3]\n",
            " [ 6  7  8 13  4]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13 11  3]\n",
            " [ 6  7  8 13 11  1]\n",
            " [ 6  7  8 13 11  4]\n",
            " ...\n",
            " [ 6  7  8 13  4 11]\n",
            " [ 6  7  8 13  3 11]\n",
            " [ 6  7  8 13  4  3]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ... 11  3  2]\n",
            " [ 6  7  8 ... 11  1  2]\n",
            " [ 6  7  8 ... 11  4  1]\n",
            " ...\n",
            " [ 6  7  8 ...  4 11  3]\n",
            " [ 6  7  8 ...  3 11  4]\n",
            " [ 6  7  8 ...  4  3  2]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  3  2  1]\n",
            " [ 6  7  8 ...  1  2  3]\n",
            " [ 6  7  8 ...  4  1  3]\n",
            " ...\n",
            " [ 6  7  8 ... 11  3  1]\n",
            " [ 6  7  8 ... 11  4  1]\n",
            " [ 6  7  8 ...  3  2 11]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  2  1  4]\n",
            " [ 6  7  8 ...  2  3  4]\n",
            " [ 6  7  8 ...  1  3  2]\n",
            " ...\n",
            " [ 6  7  8 ...  3  1  2]\n",
            " [ 6  7  8 ...  4  1  2]\n",
            " [ 6  7  8 ...  2 11  1]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  1  4 10]\n",
            " [ 6  7  8 ...  3  4 10]\n",
            " [ 6  7  8 ...  3  2 10]\n",
            " ...\n",
            " [ 6  7  8 ...  1  2 10]\n",
            " [ 6  7  8 ...  1  2 10]\n",
            " [ 6  7  8 ... 11  1 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " ...\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ...  1 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_31 (LSTM)              (None, 64)                19968     \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,813\n",
            "Trainable params: 20,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 19s - loss: 0.8897 - accuracy: 0.7296 - val_loss: 0.6029 - val_accuracy: 0.7743 - lr: 0.0050 - 19s/epoch - 21ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 13s - loss: 0.6267 - accuracy: 0.7717 - val_loss: 0.5481 - val_accuracy: 0.7732 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 13s - loss: 0.5985 - accuracy: 0.7712 - val_loss: 0.5589 - val_accuracy: 0.7743 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 12s - loss: 0.5858 - accuracy: 0.7713 - val_loss: 0.5118 - val_accuracy: 0.7746 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 12s - loss: 0.5996 - accuracy: 0.7688 - val_loss: 0.5417 - val_accuracy: 0.7758 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 12s - loss: 0.5637 - accuracy: 0.7716 - val_loss: 0.5248 - val_accuracy: 0.7740 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 12s - loss: 0.5585 - accuracy: 0.7726 - val_loss: 0.5065 - val_accuracy: 0.7740 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 12s - loss: 0.5580 - accuracy: 0.7719 - val_loss: 0.5171 - val_accuracy: 0.7749 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 12s - loss: 0.5560 - accuracy: 0.7726 - val_loss: 0.5118 - val_accuracy: 0.7745 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 12s - loss: 0.5730 - accuracy: 0.7710 - val_loss: 0.5034 - val_accuracy: 0.7730 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 13s - loss: 0.5454 - accuracy: 0.7715 - val_loss: 0.5022 - val_accuracy: 0.7717 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 12s - loss: 0.5469 - accuracy: 0.7724 - val_loss: 0.5167 - val_accuracy: 0.7727 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 12s - loss: 0.5460 - accuracy: 0.7721 - val_loss: 0.5141 - val_accuracy: 0.7764 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 12s - loss: 0.5482 - accuracy: 0.7708 - val_loss: 0.4945 - val_accuracy: 0.7732 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 12s - loss: 0.5448 - accuracy: 0.7712 - val_loss: 0.4940 - val_accuracy: 0.7745 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 12s - loss: 0.5401 - accuracy: 0.7736 - val_loss: 0.5009 - val_accuracy: 0.7761 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 12s - loss: 0.5451 - accuracy: 0.7731 - val_loss: 0.5001 - val_accuracy: 0.7749 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 12s - loss: 0.5648 - accuracy: 0.7712 - val_loss: 0.4962 - val_accuracy: 0.7756 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 12s - loss: 0.5092 - accuracy: 0.7717 - val_loss: 0.4711 - val_accuracy: 0.7731 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 12s - loss: 0.5068 - accuracy: 0.7728 - val_loss: 0.4672 - val_accuracy: 0.7771 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 12s - loss: 0.5072 - accuracy: 0.7732 - val_loss: 0.4801 - val_accuracy: 0.7746 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 12s - loss: 0.5063 - accuracy: 0.7725 - val_loss: 0.4729 - val_accuracy: 0.7746 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 12s - loss: 0.5047 - accuracy: 0.7729 - val_loss: 0.4606 - val_accuracy: 0.7740 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 12s - loss: 0.5039 - accuracy: 0.7735 - val_loss: 0.4756 - val_accuracy: 0.7754 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 12s - loss: 0.5039 - accuracy: 0.7730 - val_loss: 0.4742 - val_accuracy: 0.7741 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 12s - loss: 0.5029 - accuracy: 0.7717 - val_loss: 0.4715 - val_accuracy: 0.7748 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 12s - loss: 0.4839 - accuracy: 0.7719 - val_loss: 0.4587 - val_accuracy: 0.7750 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 12s - loss: 0.4830 - accuracy: 0.7721 - val_loss: 0.4497 - val_accuracy: 0.7733 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 12s - loss: 0.4816 - accuracy: 0.7748 - val_loss: 0.4518 - val_accuracy: 0.7732 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 12s - loss: 0.4792 - accuracy: 0.7737 - val_loss: 0.4535 - val_accuracy: 0.7725 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 12s - loss: 0.4781 - accuracy: 0.7751 - val_loss: 0.4498 - val_accuracy: 0.7746 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 12s - loss: 0.4670 - accuracy: 0.7746 - val_loss: 0.4410 - val_accuracy: 0.7762 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 12s - loss: 0.4670 - accuracy: 0.7724 - val_loss: 0.4394 - val_accuracy: 0.7753 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 12s - loss: 0.4656 - accuracy: 0.7739 - val_loss: 0.4433 - val_accuracy: 0.7742 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 12s - loss: 0.4661 - accuracy: 0.7719 - val_loss: 0.4395 - val_accuracy: 0.7775 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 12s - loss: 0.4644 - accuracy: 0.7728 - val_loss: 0.4375 - val_accuracy: 0.7730 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 12s - loss: 0.4643 - accuracy: 0.7736 - val_loss: 0.4384 - val_accuracy: 0.7731 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 12s - loss: 0.4637 - accuracy: 0.7734 - val_loss: 0.4411 - val_accuracy: 0.7765 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 12s - loss: 0.4641 - accuracy: 0.7728 - val_loss: 0.4369 - val_accuracy: 0.7745 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 12s - loss: 0.4627 - accuracy: 0.7732 - val_loss: 0.4396 - val_accuracy: 0.7736 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 12s - loss: 0.4635 - accuracy: 0.7739 - val_loss: 0.4384 - val_accuracy: 0.7744 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 12s - loss: 0.4632 - accuracy: 0.7712 - val_loss: 0.4383 - val_accuracy: 0.7746 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 12s - loss: 0.4554 - accuracy: 0.7741 - val_loss: 0.4343 - val_accuracy: 0.7735 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 12s - loss: 0.4546 - accuracy: 0.7738 - val_loss: 0.4354 - val_accuracy: 0.7718 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 12s - loss: 0.4547 - accuracy: 0.7739 - val_loss: 0.4344 - val_accuracy: 0.7716 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 12s - loss: 0.4548 - accuracy: 0.7727 - val_loss: 0.4334 - val_accuracy: 0.7746 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 12s - loss: 0.4542 - accuracy: 0.7734 - val_loss: 0.4323 - val_accuracy: 0.7751 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 12s - loss: 0.4543 - accuracy: 0.7738 - val_loss: 0.4305 - val_accuracy: 0.7752 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 12s - loss: 0.4535 - accuracy: 0.7733 - val_loss: 0.4321 - val_accuracy: 0.7711 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 12s - loss: 0.4537 - accuracy: 0.7732 - val_loss: 0.4360 - val_accuracy: 0.7728 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 12s - loss: 0.4536 - accuracy: 0.7733 - val_loss: 0.4305 - val_accuracy: 0.7737 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 12s - loss: 0.4496 - accuracy: 0.7742 - val_loss: 0.4300 - val_accuracy: 0.7758 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 12s - loss: 0.4494 - accuracy: 0.7730 - val_loss: 0.4283 - val_accuracy: 0.7737 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 12s - loss: 0.4490 - accuracy: 0.7732 - val_loss: 0.4286 - val_accuracy: 0.7733 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 12s - loss: 0.4482 - accuracy: 0.7744 - val_loss: 0.4292 - val_accuracy: 0.7742 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 12s - loss: 0.4486 - accuracy: 0.7726 - val_loss: 0.4284 - val_accuracy: 0.7743 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 12s - loss: 0.4465 - accuracy: 0.7736 - val_loss: 0.4269 - val_accuracy: 0.7775 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 12s - loss: 0.4463 - accuracy: 0.7735 - val_loss: 0.4288 - val_accuracy: 0.7732 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 12s - loss: 0.4465 - accuracy: 0.7723 - val_loss: 0.4273 - val_accuracy: 0.7762 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 12s - loss: 0.4460 - accuracy: 0.7729 - val_loss: 0.4271 - val_accuracy: 0.7777 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 12s - loss: 0.4449 - accuracy: 0.7742 - val_loss: 0.4267 - val_accuracy: 0.7748 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 12s - loss: 0.4446 - accuracy: 0.7746 - val_loss: 0.4269 - val_accuracy: 0.7758 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 12s - loss: 0.4446 - accuracy: 0.7739 - val_loss: 0.4264 - val_accuracy: 0.7743 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 12s - loss: 0.4441 - accuracy: 0.7748 - val_loss: 0.4266 - val_accuracy: 0.7736 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 12s - loss: 0.4450 - accuracy: 0.7739 - val_loss: 0.4265 - val_accuracy: 0.7749 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 12s - loss: 0.4449 - accuracy: 0.7732 - val_loss: 0.4264 - val_accuracy: 0.7749 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 12s - loss: 0.4443 - accuracy: 0.7728 - val_loss: 0.4262 - val_accuracy: 0.7738 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 12s - loss: 0.4440 - accuracy: 0.7736 - val_loss: 0.4259 - val_accuracy: 0.7766 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 12s - loss: 0.4444 - accuracy: 0.7732 - val_loss: 0.4262 - val_accuracy: 0.7748 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 70/600\n",
            "892/892 - 12s - loss: 0.4437 - accuracy: 0.7748 - val_loss: 0.4262 - val_accuracy: 0.7744 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 71/600\n",
            "892/892 - 12s - loss: 0.4437 - accuracy: 0.7752 - val_loss: 0.4261 - val_accuracy: 0.7739 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 72/600\n",
            "892/892 - 12s - loss: 0.4434 - accuracy: 0.7747 - val_loss: 0.4259 - val_accuracy: 0.7742 - lr: 9.7656e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 73/600\n",
            "892/892 - 13s - loss: 0.4429 - accuracy: 0.7751 - val_loss: 0.4258 - val_accuracy: 0.7744 - lr: 9.7656e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 74/600\n",
            "892/892 - 12s - loss: 0.4435 - accuracy: 0.7737 - val_loss: 0.4263 - val_accuracy: 0.7721 - lr: 9.7656e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 75/600\n",
            "892/892 - 13s - loss: 0.4434 - accuracy: 0.7731 - val_loss: 0.4257 - val_accuracy: 0.7744 - lr: 4.8828e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 76/600\n",
            "892/892 - 12s - loss: 0.4435 - accuracy: 0.7730 - val_loss: 0.4258 - val_accuracy: 0.7750 - lr: 4.8828e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 77/600\n",
            "892/892 - 13s - loss: 0.4435 - accuracy: 0.7746 - val_loss: 0.4258 - val_accuracy: 0.7753 - lr: 4.8828e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 78/600\n",
            "892/892 - 13s - loss: 0.4431 - accuracy: 0.7747 - val_loss: 0.4257 - val_accuracy: 0.7749 - lr: 4.8828e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 79/600\n",
            "892/892 - 13s - loss: 0.4431 - accuracy: 0.7738 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 2.4414e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 80/600\n",
            "892/892 - 13s - loss: 0.4428 - accuracy: 0.7751 - val_loss: 0.4257 - val_accuracy: 0.7752 - lr: 2.4414e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 81/600\n",
            "892/892 - 12s - loss: 0.4437 - accuracy: 0.7743 - val_loss: 0.4257 - val_accuracy: 0.7740 - lr: 2.4414e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 82/600\n",
            "892/892 - 12s - loss: 0.4431 - accuracy: 0.7744 - val_loss: 0.4257 - val_accuracy: 0.7741 - lr: 1.2207e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 83/600\n",
            "892/892 - 13s - loss: 0.4434 - accuracy: 0.7744 - val_loss: 0.4257 - val_accuracy: 0.7735 - lr: 1.2207e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 84/600\n",
            "892/892 - 12s - loss: 0.4433 - accuracy: 0.7748 - val_loss: 0.4257 - val_accuracy: 0.7750 - lr: 1.2207e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 85/600\n",
            "892/892 - 13s - loss: 0.4433 - accuracy: 0.7738 - val_loss: 0.4257 - val_accuracy: 0.7739 - lr: 6.1035e-07 - 13s/epoch - 14ms/step\n",
            "Epoch 86/600\n",
            "892/892 - 12s - loss: 0.4434 - accuracy: 0.7738 - val_loss: 0.4257 - val_accuracy: 0.7740 - lr: 6.1035e-07 - 12s/epoch - 14ms/step\n",
            "Epoch 87/600\n",
            "892/892 - 13s - loss: 0.4432 - accuracy: 0.7739 - val_loss: 0.4257 - val_accuracy: 0.7740 - lr: 6.1035e-07 - 13s/epoch - 14ms/step\n",
            "Epoch 88/600\n",
            "892/892 - 12s - loss: 0.4433 - accuracy: 0.7743 - val_loss: 0.4257 - val_accuracy: 0.7740 - lr: 3.0518e-07 - 12s/epoch - 14ms/step\n",
            "Epoch 89/600\n",
            "892/892 - 13s - loss: 0.4427 - accuracy: 0.7764 - val_loss: 0.4257 - val_accuracy: 0.7742 - lr: 3.0518e-07 - 13s/epoch - 14ms/step\n",
            "Epoch 90/600\n",
            "892/892 - 13s - loss: 0.4427 - accuracy: 0.7747 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 3.0518e-07 - 13s/epoch - 14ms/step\n",
            "Epoch 91/600\n",
            "892/892 - 13s - loss: 0.4435 - accuracy: 0.7741 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 1.5259e-07 - 13s/epoch - 14ms/step\n",
            "Epoch 92/600\n",
            "892/892 - 13s - loss: 0.4429 - accuracy: 0.7744 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 1.5259e-07 - 13s/epoch - 14ms/step\n",
            "Epoch 93/600\n",
            "892/892 - 12s - loss: 0.4432 - accuracy: 0.7741 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 1.5259e-07 - 12s/epoch - 14ms/step\n",
            "Epoch 94/600\n",
            "892/892 - 13s - loss: 0.4436 - accuracy: 0.7744 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 7.6294e-08 - 13s/epoch - 14ms/step\n",
            "Epoch 95/600\n",
            "892/892 - 14s - loss: 0.4433 - accuracy: 0.7746 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 7.6294e-08 - 14s/epoch - 15ms/step\n",
            "Epoch 96/600\n",
            "892/892 - 12s - loss: 0.4428 - accuracy: 0.7753 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 7.6294e-08 - 12s/epoch - 14ms/step\n",
            "Epoch 97/600\n",
            "892/892 - 13s - loss: 0.4431 - accuracy: 0.7756 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 3.8147e-08 - 13s/epoch - 14ms/step\n",
            "Epoch 98/600\n",
            "892/892 - 13s - loss: 0.4431 - accuracy: 0.7747 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 3.8147e-08 - 13s/epoch - 14ms/step\n",
            "Epoch 99/600\n",
            "892/892 - 12s - loss: 0.4430 - accuracy: 0.7743 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 3.8147e-08 - 12s/epoch - 14ms/step\n",
            "Epoch 100/600\n",
            "892/892 - 12s - loss: 0.4431 - accuracy: 0.7749 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 1.9073e-08 - 12s/epoch - 14ms/step\n",
            "Epoch 101/600\n",
            "892/892 - 12s - loss: 0.4436 - accuracy: 0.7731 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 1.9073e-08 - 12s/epoch - 14ms/step\n",
            "Epoch 102/600\n",
            "892/892 - 13s - loss: 0.4432 - accuracy: 0.7741 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 1.9073e-08 - 13s/epoch - 14ms/step\n",
            "Epoch 103/600\n",
            "892/892 - 12s - loss: 0.4434 - accuracy: 0.7730 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 9.5367e-09 - 12s/epoch - 14ms/step\n",
            "Epoch 104/600\n",
            "892/892 - 12s - loss: 0.4435 - accuracy: 0.7745 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 9.5367e-09 - 12s/epoch - 14ms/step\n",
            "Epoch 105/600\n",
            "892/892 - 12s - loss: 0.4433 - accuracy: 0.7738 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 9.5367e-09 - 12s/epoch - 14ms/step\n",
            "Epoch 106/600\n",
            "892/892 - 12s - loss: 0.4427 - accuracy: 0.7736 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 4.7684e-09 - 12s/epoch - 14ms/step\n",
            "Epoch 107/600\n",
            "892/892 - 12s - loss: 0.4430 - accuracy: 0.7740 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 4.7684e-09 - 12s/epoch - 14ms/step\n",
            "Epoch 108/600\n",
            "892/892 - 12s - loss: 0.4432 - accuracy: 0.7740 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 4.7684e-09 - 12s/epoch - 14ms/step\n",
            "Epoch 109/600\n",
            "892/892 - 12s - loss: 0.4427 - accuracy: 0.7750 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 2.3842e-09 - 12s/epoch - 14ms/step\n",
            "Epoch 110/600\n",
            "892/892 - 12s - loss: 0.4436 - accuracy: 0.7737 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 2.3842e-09 - 12s/epoch - 14ms/step\n",
            "Epoch 111/600\n",
            "892/892 - 12s - loss: 0.4430 - accuracy: 0.7747 - val_loss: 0.4257 - val_accuracy: 0.7729 - lr: 2.3842e-09 - 12s/epoch - 14ms/step\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_31 (LSTM)              (None, 64)                19968     \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,813\n",
            "Trainable params: 20,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13  3]\n",
            " [ 6  7  8 13  3]\n",
            " [ 6  7  8 13  1]\n",
            " ...\n",
            " [ 6  7  8 13  2]\n",
            " [ 6  7  8 13  1]\n",
            " [ 6  7  8 13 11]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13  3  4]\n",
            " [ 6  7  8 13  3 11]\n",
            " [ 6  7  8 13  1  2]\n",
            " ...\n",
            " [ 6  7  8 13  2 11]\n",
            " [ 6  7  8 13  1  4]\n",
            " [ 6  7  8 13 11  3]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ...  3  4 11]\n",
            " [ 6  7  8 ...  3 11  1]\n",
            " [ 6  7  8 ...  1  2  4]\n",
            " ...\n",
            " [ 6  7  8 ...  2 11  1]\n",
            " [ 6  7  8 ...  1  4  2]\n",
            " [ 6  7  8 ... 11  3  2]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  4 11  1]\n",
            " [ 6  7  8 ... 11  1  2]\n",
            " [ 6  7  8 ...  2  4  3]\n",
            " ...\n",
            " [ 6  7  8 ... 11  1  3]\n",
            " [ 6  7  8 ...  4  2  3]\n",
            " [ 6  7  8 ...  3  2  4]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ... 11  1  2]\n",
            " [ 6  7  8 ...  1  2  4]\n",
            " [ 6  7  8 ...  4  3 11]\n",
            " ...\n",
            " [ 6  7  8 ...  1  3  4]\n",
            " [ 6  7  8 ...  2  3 11]\n",
            " [ 6  7  8 ...  2  4  1]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  1  2 10]\n",
            " [ 6  7  8 ...  2  4 10]\n",
            " [ 6  7  8 ...  3 11 10]\n",
            " ...\n",
            " [ 6  7  8 ...  3  4 10]\n",
            " [ 6  7  8 ...  3 11 10]\n",
            " [ 6  7  8 ...  4  1 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ... 11 10  5]\n",
            " ...\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ... 11 10  5]\n",
            " [ 6  7  8 ...  1 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_32 (LSTM)              (None, 64)                19968     \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,813\n",
            "Trainable params: 20,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 15s - loss: 1.6593 - accuracy: 0.5023 - val_loss: 1.0365 - val_accuracy: 0.6667 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 12s - loss: 1.1140 - accuracy: 0.6152 - val_loss: 0.9737 - val_accuracy: 0.6666 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 13s - loss: 1.0924 - accuracy: 0.6155 - val_loss: 0.9581 - val_accuracy: 0.6667 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 12s - loss: 1.0457 - accuracy: 0.6260 - val_loss: 0.9405 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 12s - loss: 1.0414 - accuracy: 0.6255 - val_loss: 0.9291 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 12s - loss: 1.0445 - accuracy: 0.6241 - val_loss: 0.9311 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 12s - loss: 1.0181 - accuracy: 0.6285 - val_loss: 0.9332 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 12s - loss: 1.0182 - accuracy: 0.6281 - val_loss: 0.9106 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 12s - loss: 1.0119 - accuracy: 0.6288 - val_loss: 0.9088 - val_accuracy: 0.6651 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 12s - loss: 1.0139 - accuracy: 0.6291 - val_loss: 0.9136 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 12s - loss: 1.0286 - accuracy: 0.6302 - val_loss: 0.8967 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 12s - loss: 0.9913 - accuracy: 0.6355 - val_loss: 0.9063 - val_accuracy: 0.6673 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 12s - loss: 1.0035 - accuracy: 0.6384 - val_loss: 0.8951 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 12s - loss: 0.9810 - accuracy: 0.6407 - val_loss: 0.8965 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 12s - loss: 0.9908 - accuracy: 0.6379 - val_loss: 0.8989 - val_accuracy: 0.6673 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 12s - loss: 0.9869 - accuracy: 0.6370 - val_loss: 0.8948 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 13s - loss: 0.9941 - accuracy: 0.6379 - val_loss: 0.8867 - val_accuracy: 0.6667 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 12s - loss: 0.9771 - accuracy: 0.6412 - val_loss: 0.8892 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 13s - loss: 0.9763 - accuracy: 0.6409 - val_loss: 0.8848 - val_accuracy: 0.6672 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 13s - loss: 0.9738 - accuracy: 0.6400 - val_loss: 0.8895 - val_accuracy: 0.6667 - lr: 0.0050 - 13s/epoch - 14ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 12s - loss: 0.9915 - accuracy: 0.6371 - val_loss: 0.8899 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 12s - loss: 0.9860 - accuracy: 0.6387 - val_loss: 0.8860 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 12s - loss: 0.8861 - accuracy: 0.6454 - val_loss: 0.8138 - val_accuracy: 0.6645 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 12s - loss: 0.8888 - accuracy: 0.6455 - val_loss: 0.8178 - val_accuracy: 0.6667 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 13s - loss: 0.8902 - accuracy: 0.6439 - val_loss: 0.8188 - val_accuracy: 0.6667 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 12s - loss: 0.8878 - accuracy: 0.6446 - val_loss: 0.8136 - val_accuracy: 0.6667 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 13s - loss: 0.9000 - accuracy: 0.6444 - val_loss: 0.8163 - val_accuracy: 0.6667 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 13s - loss: 0.8856 - accuracy: 0.6451 - val_loss: 0.8096 - val_accuracy: 0.6698 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 13s - loss: 0.8911 - accuracy: 0.6449 - val_loss: 0.8160 - val_accuracy: 0.6667 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 13s - loss: 0.8856 - accuracy: 0.6455 - val_loss: 0.8125 - val_accuracy: 0.6667 - lr: 0.0025 - 13s/epoch - 14ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 12s - loss: 0.8867 - accuracy: 0.6448 - val_loss: 0.8107 - val_accuracy: 0.6648 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 13s - loss: 0.8489 - accuracy: 0.6460 - val_loss: 0.7838 - val_accuracy: 0.6660 - lr: 0.0012 - 13s/epoch - 14ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 13s - loss: 0.8514 - accuracy: 0.6463 - val_loss: 0.7845 - val_accuracy: 0.6667 - lr: 0.0012 - 13s/epoch - 14ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 12s - loss: 0.8491 - accuracy: 0.6456 - val_loss: 0.7867 - val_accuracy: 0.6673 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 12s - loss: 0.8493 - accuracy: 0.6470 - val_loss: 0.7850 - val_accuracy: 0.6667 - lr: 0.0012 - 12s/epoch - 14ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 13s - loss: 0.8315 - accuracy: 0.6477 - val_loss: 0.7684 - val_accuracy: 0.6658 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 13s - loss: 0.8316 - accuracy: 0.6458 - val_loss: 0.7680 - val_accuracy: 0.6667 - lr: 6.2500e-04 - 13s/epoch - 15ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 13s - loss: 0.8303 - accuracy: 0.6467 - val_loss: 0.7675 - val_accuracy: 0.6667 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 13s - loss: 0.8306 - accuracy: 0.6482 - val_loss: 0.7662 - val_accuracy: 0.6648 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 12s - loss: 0.8308 - accuracy: 0.6486 - val_loss: 0.7685 - val_accuracy: 0.6673 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 13s - loss: 0.8298 - accuracy: 0.6486 - val_loss: 0.7681 - val_accuracy: 0.6653 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 13s - loss: 0.8301 - accuracy: 0.6476 - val_loss: 0.7661 - val_accuracy: 0.6674 - lr: 6.2500e-04 - 13s/epoch - 14ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 12s - loss: 0.8220 - accuracy: 0.6481 - val_loss: 0.7593 - val_accuracy: 0.6667 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 12s - loss: 0.8222 - accuracy: 0.6480 - val_loss: 0.7602 - val_accuracy: 0.6655 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 12s - loss: 0.8218 - accuracy: 0.6479 - val_loss: 0.7603 - val_accuracy: 0.6667 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 12s - loss: 0.8213 - accuracy: 0.6489 - val_loss: 0.7583 - val_accuracy: 0.6667 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 12s - loss: 0.8197 - accuracy: 0.6483 - val_loss: 0.7596 - val_accuracy: 0.6667 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 12s - loss: 0.8202 - accuracy: 0.6494 - val_loss: 0.7584 - val_accuracy: 0.6648 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 12s - loss: 0.8204 - accuracy: 0.6478 - val_loss: 0.7590 - val_accuracy: 0.6660 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 12s - loss: 0.8168 - accuracy: 0.6494 - val_loss: 0.7543 - val_accuracy: 0.6658 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 12s - loss: 0.8175 - accuracy: 0.6494 - val_loss: 0.7549 - val_accuracy: 0.6631 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 12s - loss: 0.8153 - accuracy: 0.6489 - val_loss: 0.7546 - val_accuracy: 0.6667 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 12s - loss: 0.8166 - accuracy: 0.6480 - val_loss: 0.7549 - val_accuracy: 0.6642 - lr: 1.5625e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 12s - loss: 0.8143 - accuracy: 0.6492 - val_loss: 0.7527 - val_accuracy: 0.6667 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 12s - loss: 0.8140 - accuracy: 0.6495 - val_loss: 0.7528 - val_accuracy: 0.6669 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 12s - loss: 0.8135 - accuracy: 0.6510 - val_loss: 0.7531 - val_accuracy: 0.6667 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 12s - loss: 0.8130 - accuracy: 0.6493 - val_loss: 0.7530 - val_accuracy: 0.6667 - lr: 7.8125e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 12s - loss: 0.8124 - accuracy: 0.6510 - val_loss: 0.7519 - val_accuracy: 0.6667 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 12s - loss: 0.8121 - accuracy: 0.6490 - val_loss: 0.7519 - val_accuracy: 0.6667 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 12s - loss: 0.8126 - accuracy: 0.6486 - val_loss: 0.7518 - val_accuracy: 0.6667 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 12s - loss: 0.8118 - accuracy: 0.6500 - val_loss: 0.7517 - val_accuracy: 0.6667 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 12s - loss: 0.8122 - accuracy: 0.6497 - val_loss: 0.7517 - val_accuracy: 0.6650 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 12s - loss: 0.8118 - accuracy: 0.6506 - val_loss: 0.7517 - val_accuracy: 0.6660 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 12s - loss: 0.8120 - accuracy: 0.6503 - val_loss: 0.7523 - val_accuracy: 0.6660 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 12s - loss: 0.8121 - accuracy: 0.6492 - val_loss: 0.7512 - val_accuracy: 0.6650 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 12s - loss: 0.8120 - accuracy: 0.6503 - val_loss: 0.7515 - val_accuracy: 0.6660 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 12s - loss: 0.8121 - accuracy: 0.6507 - val_loss: 0.7512 - val_accuracy: 0.6667 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 12s - loss: 0.8121 - accuracy: 0.6495 - val_loss: 0.7513 - val_accuracy: 0.6650 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 12s - loss: 0.8119 - accuracy: 0.6491 - val_loss: 0.7509 - val_accuracy: 0.6667 - lr: 9.7656e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 70/600\n",
            "892/892 - 12s - loss: 0.8112 - accuracy: 0.6502 - val_loss: 0.7510 - val_accuracy: 0.6650 - lr: 9.7656e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 71/600\n",
            "892/892 - 12s - loss: 0.8108 - accuracy: 0.6508 - val_loss: 0.7510 - val_accuracy: 0.6667 - lr: 9.7656e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 72/600\n",
            "892/892 - 12s - loss: 0.8119 - accuracy: 0.6492 - val_loss: 0.7510 - val_accuracy: 0.6667 - lr: 9.7656e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 73/600\n",
            "892/892 - 12s - loss: 0.8116 - accuracy: 0.6502 - val_loss: 0.7509 - val_accuracy: 0.6667 - lr: 4.8828e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 74/600\n",
            "892/892 - 12s - loss: 0.8117 - accuracy: 0.6500 - val_loss: 0.7508 - val_accuracy: 0.6667 - lr: 4.8828e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 75/600\n",
            "892/892 - 12s - loss: 0.8116 - accuracy: 0.6510 - val_loss: 0.7508 - val_accuracy: 0.6650 - lr: 4.8828e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 76/600\n",
            "892/892 - 12s - loss: 0.8120 - accuracy: 0.6497 - val_loss: 0.7509 - val_accuracy: 0.6650 - lr: 4.8828e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 77/600\n",
            "892/892 - 12s - loss: 0.8120 - accuracy: 0.6492 - val_loss: 0.7508 - val_accuracy: 0.6667 - lr: 4.8828e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 78/600\n",
            "892/892 - 12s - loss: 0.8112 - accuracy: 0.6496 - val_loss: 0.7508 - val_accuracy: 0.6667 - lr: 2.4414e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 79/600\n",
            "892/892 - 12s - loss: 0.8118 - accuracy: 0.6501 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 2.4414e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 80/600\n",
            "892/892 - 12s - loss: 0.8111 - accuracy: 0.6492 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 2.4414e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 81/600\n",
            "892/892 - 12s - loss: 0.8107 - accuracy: 0.6517 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 1.2207e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 82/600\n",
            "892/892 - 12s - loss: 0.8110 - accuracy: 0.6509 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 1.2207e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 83/600\n",
            "892/892 - 12s - loss: 0.8124 - accuracy: 0.6495 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 1.2207e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 84/600\n",
            "892/892 - 12s - loss: 0.8123 - accuracy: 0.6500 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 6.1035e-07 - 12s/epoch - 13ms/step\n",
            "Epoch 85/600\n",
            "892/892 - 12s - loss: 0.8108 - accuracy: 0.6503 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 6.1035e-07 - 12s/epoch - 14ms/step\n",
            "Epoch 86/600\n",
            "892/892 - 12s - loss: 0.8115 - accuracy: 0.6493 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 6.1035e-07 - 12s/epoch - 14ms/step\n",
            "Epoch 87/600\n",
            "892/892 - 12s - loss: 0.8107 - accuracy: 0.6508 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 3.0518e-07 - 12s/epoch - 14ms/step\n",
            "Epoch 88/600\n",
            "892/892 - 12s - loss: 0.8112 - accuracy: 0.6487 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 3.0518e-07 - 12s/epoch - 14ms/step\n",
            "Epoch 89/600\n",
            "892/892 - 12s - loss: 0.8122 - accuracy: 0.6503 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 3.0518e-07 - 12s/epoch - 14ms/step\n",
            "Epoch 90/600\n",
            "892/892 - 12s - loss: 0.8114 - accuracy: 0.6508 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 1.5259e-07 - 12s/epoch - 14ms/step\n",
            "Epoch 91/600\n",
            "892/892 - 13s - loss: 0.8117 - accuracy: 0.6497 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 1.5259e-07 - 13s/epoch - 14ms/step\n",
            "Epoch 92/600\n",
            "892/892 - 12s - loss: 0.8102 - accuracy: 0.6496 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 1.5259e-07 - 12s/epoch - 14ms/step\n",
            "Epoch 93/600\n",
            "892/892 - 12s - loss: 0.8106 - accuracy: 0.6504 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 7.6294e-08 - 12s/epoch - 14ms/step\n",
            "Epoch 94/600\n",
            "892/892 - 12s - loss: 0.8116 - accuracy: 0.6497 - val_loss: 0.7507 - val_accuracy: 0.6667 - lr: 7.6294e-08 - 12s/epoch - 14ms/step\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_32 (LSTM)              (None, 64)                19968     \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,813\n",
            "Trainable params: 20,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13  2]\n",
            " [ 6  7  8 13  1]\n",
            " [ 6  7  8 13  1]\n",
            " ...\n",
            " [ 6  7  8 13  3]\n",
            " [ 6  7  8 13 11]\n",
            " [ 6  7  8 13  2]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13  2  4]\n",
            " [ 6  7  8 13  1 11]\n",
            " [ 6  7  8 13  1  4]\n",
            " ...\n",
            " [ 6  7  8 13  3 11]\n",
            " [ 6  7  8 13 11  4]\n",
            " [ 6  7  8 13  2  2]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ...  2  4  2]\n",
            " [ 6  7  8 ...  1 11  2]\n",
            " [ 6  7  8 ...  1  4  3]\n",
            " ...\n",
            " [ 6  7  8 ...  3 11  1]\n",
            " [ 6  7  8 ... 11  4 11]\n",
            " [ 6  7  8 ...  2  2  1]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  4  2  2]\n",
            " [ 6  7  8 ... 11  2  3]\n",
            " [ 6  7  8 ...  4  3  1]\n",
            " ...\n",
            " [ 6  7  8 ... 11  1  4]\n",
            " [ 6  7  8 ...  4 11  2]\n",
            " [ 6  7  8 ...  2  1  3]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  2  2  2]\n",
            " [ 6  7  8 ...  2  3  4]\n",
            " [ 6  7  8 ...  3  1  3]\n",
            " ...\n",
            " [ 6  7  8 ...  1  4  1]\n",
            " [ 6  7  8 ... 11  2  3]\n",
            " [ 6  7  8 ...  1  3  1]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  2  2 10]\n",
            " [ 6  7  8 ...  3  4 10]\n",
            " [ 6  7  8 ...  1  3 10]\n",
            " ...\n",
            " [ 6  7  8 ...  4  1 10]\n",
            " [ 6  7  8 ...  2  3 10]\n",
            " [ 6  7  8 ...  3  1 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  3 10  5]\n",
            " ...\n",
            " [ 6  7  8 ...  1 10  5]\n",
            " [ 6  7  8 ...  3 10  5]\n",
            " [ 6  7  8 ...  1 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Already exists: 2 16 0.0 0.0\n",
            "Already exists: 2 16 0.0 1e-05\n",
            "Already exists: 2 16 0.0 0.0001\n",
            "Already exists: 2 16 0.0 0.001\n",
            "Already exists: 2 16 0.0 0.01\n",
            "Already exists: 2 16 0.2 0.0\n",
            "Already exists: 2 16 0.2 1e-05\n",
            "Already exists: 2 16 0.2 0.0001\n",
            "Already exists: 2 16 0.2 0.001\n",
            "Already exists: 2 16 0.2 0.01\n",
            "Already exists: 2 16 0.4 0.0\n",
            "Already exists: 2 16 0.4 1e-05\n",
            "Already exists: 2 16 0.4 0.0001\n",
            "Already exists: 2 16 0.4 0.001\n",
            "Already exists: 2 16 0.4 0.01\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_33 (LSTM)              (None, 10, 16)            1920      \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 10, 16)            0         \n",
            "                                                                 \n",
            " lstm_34 (LSTM)              (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 13)                221       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,253\n",
            "Trainable params: 4,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 16s - loss: 1.0031 - accuracy: 0.5560 - val_loss: 0.6760 - val_accuracy: 0.6667 - lr: 0.0050 - 16s/epoch - 18ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 13s - loss: 0.8010 - accuracy: 0.6175 - val_loss: 0.6733 - val_accuracy: 0.6667 - lr: 0.0050 - 13s/epoch - 15ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 11s - loss: 0.7741 - accuracy: 0.6278 - val_loss: 0.6726 - val_accuracy: 0.6791 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 12s - loss: 0.7719 - accuracy: 0.6302 - val_loss: 0.6710 - val_accuracy: 0.6678 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 11s - loss: 0.7527 - accuracy: 0.6386 - val_loss: 0.6490 - val_accuracy: 0.7067 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 11s - loss: 0.7299 - accuracy: 0.6593 - val_loss: 0.6247 - val_accuracy: 0.7162 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 11s - loss: 0.7146 - accuracy: 0.6711 - val_loss: 0.6107 - val_accuracy: 0.7202 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 11s - loss: 0.6944 - accuracy: 0.6829 - val_loss: 0.5780 - val_accuracy: 0.7288 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 11s - loss: 0.6766 - accuracy: 0.6917 - val_loss: 0.5516 - val_accuracy: 0.7408 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 11s - loss: 0.6565 - accuracy: 0.6975 - val_loss: 0.5370 - val_accuracy: 0.7507 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 11s - loss: 0.6529 - accuracy: 0.6966 - val_loss: 0.5354 - val_accuracy: 0.7275 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 11s - loss: 0.6400 - accuracy: 0.6975 - val_loss: 0.5248 - val_accuracy: 0.7297 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 11s - loss: 0.6365 - accuracy: 0.6982 - val_loss: 0.5185 - val_accuracy: 0.7409 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 11s - loss: 0.6386 - accuracy: 0.6971 - val_loss: 0.5174 - val_accuracy: 0.7291 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 11s - loss: 0.6325 - accuracy: 0.6977 - val_loss: 0.5127 - val_accuracy: 0.7512 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 11s - loss: 0.6296 - accuracy: 0.6998 - val_loss: 0.5128 - val_accuracy: 0.7312 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 11s - loss: 0.6335 - accuracy: 0.6989 - val_loss: 0.5121 - val_accuracy: 0.7628 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 11s - loss: 0.6236 - accuracy: 0.7040 - val_loss: 0.5110 - val_accuracy: 0.7461 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 11s - loss: 0.6230 - accuracy: 0.7037 - val_loss: 0.5072 - val_accuracy: 0.7446 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 12s - loss: 0.6218 - accuracy: 0.7060 - val_loss: 0.5052 - val_accuracy: 0.7611 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 11s - loss: 0.6185 - accuracy: 0.7082 - val_loss: 0.4947 - val_accuracy: 0.7555 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 11s - loss: 0.6142 - accuracy: 0.7129 - val_loss: 0.4868 - val_accuracy: 0.7606 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 11s - loss: 0.6103 - accuracy: 0.7147 - val_loss: 0.4863 - val_accuracy: 0.7580 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 11s - loss: 0.6109 - accuracy: 0.7156 - val_loss: 0.4828 - val_accuracy: 0.7687 - lr: 0.0050 - 11s/epoch - 12ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 11s - loss: 0.6035 - accuracy: 0.7180 - val_loss: 0.4770 - val_accuracy: 0.7576 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 11s - loss: 0.6029 - accuracy: 0.7181 - val_loss: 0.4725 - val_accuracy: 0.7699 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 11s - loss: 0.6021 - accuracy: 0.7200 - val_loss: 0.4768 - val_accuracy: 0.7596 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 11s - loss: 0.5987 - accuracy: 0.7222 - val_loss: 0.4702 - val_accuracy: 0.7714 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 11s - loss: 0.5968 - accuracy: 0.7235 - val_loss: 0.4595 - val_accuracy: 0.7736 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 11s - loss: 0.5947 - accuracy: 0.7250 - val_loss: 0.4602 - val_accuracy: 0.7736 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 12s - loss: 0.5947 - accuracy: 0.7268 - val_loss: 0.4612 - val_accuracy: 0.7728 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 11s - loss: 0.5917 - accuracy: 0.7255 - val_loss: 0.4540 - val_accuracy: 0.7727 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 12s - loss: 0.5910 - accuracy: 0.7270 - val_loss: 0.4528 - val_accuracy: 0.7705 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 11s - loss: 0.5901 - accuracy: 0.7264 - val_loss: 0.4488 - val_accuracy: 0.7730 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 11s - loss: 0.5897 - accuracy: 0.7271 - val_loss: 0.4496 - val_accuracy: 0.7747 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 12s - loss: 0.5887 - accuracy: 0.7272 - val_loss: 0.4501 - val_accuracy: 0.7729 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 11s - loss: 0.5889 - accuracy: 0.7270 - val_loss: 0.4511 - val_accuracy: 0.7742 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 11s - loss: 0.5833 - accuracy: 0.7305 - val_loss: 0.4474 - val_accuracy: 0.7744 - lr: 0.0025 - 11s/epoch - 13ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 12s - loss: 0.5822 - accuracy: 0.7295 - val_loss: 0.4471 - val_accuracy: 0.7727 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 11s - loss: 0.5842 - accuracy: 0.7289 - val_loss: 0.4481 - val_accuracy: 0.7744 - lr: 0.0025 - 11s/epoch - 13ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 12s - loss: 0.5831 - accuracy: 0.7305 - val_loss: 0.4488 - val_accuracy: 0.7727 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 11s - loss: 0.5827 - accuracy: 0.7294 - val_loss: 0.4471 - val_accuracy: 0.7720 - lr: 0.0025 - 11s/epoch - 13ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 11s - loss: 0.5793 - accuracy: 0.7307 - val_loss: 0.4472 - val_accuracy: 0.7740 - lr: 0.0012 - 11s/epoch - 13ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 11s - loss: 0.5805 - accuracy: 0.7301 - val_loss: 0.4450 - val_accuracy: 0.7728 - lr: 0.0012 - 11s/epoch - 13ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 11s - loss: 0.5819 - accuracy: 0.7288 - val_loss: 0.4432 - val_accuracy: 0.7722 - lr: 0.0012 - 11s/epoch - 13ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 12s - loss: 0.5789 - accuracy: 0.7314 - val_loss: 0.4434 - val_accuracy: 0.7735 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 12s - loss: 0.5794 - accuracy: 0.7307 - val_loss: 0.4430 - val_accuracy: 0.7738 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 11s - loss: 0.5801 - accuracy: 0.7299 - val_loss: 0.4425 - val_accuracy: 0.7726 - lr: 0.0012 - 11s/epoch - 13ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 11s - loss: 0.5792 - accuracy: 0.7294 - val_loss: 0.4428 - val_accuracy: 0.7732 - lr: 0.0012 - 11s/epoch - 13ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 11s - loss: 0.5790 - accuracy: 0.7303 - val_loss: 0.4408 - val_accuracy: 0.7731 - lr: 0.0012 - 11s/epoch - 13ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 11s - loss: 0.5785 - accuracy: 0.7305 - val_loss: 0.4424 - val_accuracy: 0.7734 - lr: 0.0012 - 11s/epoch - 13ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 12s - loss: 0.5766 - accuracy: 0.7313 - val_loss: 0.4417 - val_accuracy: 0.7733 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 11s - loss: 0.5757 - accuracy: 0.7310 - val_loss: 0.4413 - val_accuracy: 0.7726 - lr: 0.0012 - 11s/epoch - 13ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 12s - loss: 0.5776 - accuracy: 0.7303 - val_loss: 0.4413 - val_accuracy: 0.7732 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 11s - loss: 0.5752 - accuracy: 0.7325 - val_loss: 0.4419 - val_accuracy: 0.7729 - lr: 6.2500e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 11s - loss: 0.5790 - accuracy: 0.7307 - val_loss: 0.4419 - val_accuracy: 0.7733 - lr: 6.2500e-04 - 11s/epoch - 13ms/step\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_33 (LSTM)              (None, 10, 16)            1920      \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 10, 16)            0         \n",
            "                                                                 \n",
            " lstm_34 (LSTM)              (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 13)                221       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,253\n",
            "Trainable params: 4,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13  2]\n",
            " [ 6  7  8 13  3]\n",
            " [ 6  7  8 13  2]\n",
            " ...\n",
            " [ 6  7  8 13 11]\n",
            " [ 6  7  8 13  3]\n",
            " [ 6  7  8 13  2]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13  2  1]\n",
            " [ 6  7  8 13  3  4]\n",
            " [ 6  7  8 13  2 11]\n",
            " ...\n",
            " [ 6  7  8 13 11 11]\n",
            " [ 6  7  8 13  3 11]\n",
            " [ 6  7  8 13  2 11]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ...  2  1  1]\n",
            " [ 6  7  8 ...  3  4  2]\n",
            " [ 6  7  8 ...  2 11  4]\n",
            " ...\n",
            " [ 6  7  8 ... 11 11  1]\n",
            " [ 6  7  8 ...  3 11  1]\n",
            " [ 6  7  8 ...  2 11  3]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  1  1  3]\n",
            " [ 6  7  8 ...  4  2  1]\n",
            " [ 6  7  8 ... 11  4  1]\n",
            " ...\n",
            " [ 6  7  8 ... 11  1 11]\n",
            " [ 6  7  8 ... 11  1  2]\n",
            " [ 6  7  8 ... 11  3  1]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  1  3  4]\n",
            " [ 6  7  8 ...  2  1 11]\n",
            " [ 6  7  8 ...  4  1  3]\n",
            " ...\n",
            " [ 6  7  8 ...  1 11  4]\n",
            " [ 6  7  8 ...  1  2  4]\n",
            " [ 6  7  8 ...  3  1  4]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  3  4 10]\n",
            " [ 6  7  8 ...  1 11 10]\n",
            " [ 6  7  8 ...  1  3 10]\n",
            " ...\n",
            " [ 6  7  8 ... 11  4 10]\n",
            " [ 6  7  8 ...  2  4 10]\n",
            " [ 6  7  8 ...  1  4 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ... 11 10  5]\n",
            " [ 6  7  8 ...  3 10  5]\n",
            " ...\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  4 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5  5]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5  5  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_35 (LSTM)              (None, 10, 16)            1920      \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 10, 16)            0         \n",
            "                                                                 \n",
            " lstm_36 (LSTM)              (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 13)                221       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,253\n",
            "Trainable params: 4,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 17s - loss: 1.0679 - accuracy: 0.5490 - val_loss: 0.6943 - val_accuracy: 0.6742 - lr: 0.0050 - 17s/epoch - 19ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 12s - loss: 0.8165 - accuracy: 0.6404 - val_loss: 0.6432 - val_accuracy: 0.7051 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 12s - loss: 0.7769 - accuracy: 0.6546 - val_loss: 0.6267 - val_accuracy: 0.7061 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 12s - loss: 0.7538 - accuracy: 0.6621 - val_loss: 0.6164 - val_accuracy: 0.7115 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 12s - loss: 0.7349 - accuracy: 0.6680 - val_loss: 0.5890 - val_accuracy: 0.7231 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 11s - loss: 0.7189 - accuracy: 0.6749 - val_loss: 0.5804 - val_accuracy: 0.7254 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 12s - loss: 0.7067 - accuracy: 0.6791 - val_loss: 0.5744 - val_accuracy: 0.7272 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 12s - loss: 0.7021 - accuracy: 0.6845 - val_loss: 0.5725 - val_accuracy: 0.7174 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 12s - loss: 0.6891 - accuracy: 0.6866 - val_loss: 0.5663 - val_accuracy: 0.7182 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 12s - loss: 0.6811 - accuracy: 0.6894 - val_loss: 0.5654 - val_accuracy: 0.7201 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 12s - loss: 0.6713 - accuracy: 0.6932 - val_loss: 0.5534 - val_accuracy: 0.7329 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 11s - loss: 0.6669 - accuracy: 0.6941 - val_loss: 0.5511 - val_accuracy: 0.7375 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 12s - loss: 0.6624 - accuracy: 0.6946 - val_loss: 0.5469 - val_accuracy: 0.7310 - lr: 0.0050 - 12s/epoch - 14ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 12s - loss: 0.6583 - accuracy: 0.6955 - val_loss: 0.5462 - val_accuracy: 0.7299 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 12s - loss: 0.6562 - accuracy: 0.6972 - val_loss: 0.5454 - val_accuracy: 0.7275 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 12s - loss: 0.6518 - accuracy: 0.6980 - val_loss: 0.5424 - val_accuracy: 0.7313 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 12s - loss: 0.6543 - accuracy: 0.6991 - val_loss: 0.5429 - val_accuracy: 0.7308 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 12s - loss: 0.6484 - accuracy: 0.6981 - val_loss: 0.5417 - val_accuracy: 0.7321 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 12s - loss: 0.6472 - accuracy: 0.6981 - val_loss: 0.5408 - val_accuracy: 0.7328 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 11s - loss: 0.6406 - accuracy: 0.7000 - val_loss: 0.5395 - val_accuracy: 0.7313 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 12s - loss: 0.6428 - accuracy: 0.7006 - val_loss: 0.5399 - val_accuracy: 0.7401 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 12s - loss: 0.6393 - accuracy: 0.7005 - val_loss: 0.5374 - val_accuracy: 0.7321 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 12s - loss: 0.6417 - accuracy: 0.7004 - val_loss: 0.5395 - val_accuracy: 0.7457 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 12s - loss: 0.6395 - accuracy: 0.7019 - val_loss: 0.5358 - val_accuracy: 0.7450 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 12s - loss: 0.6413 - accuracy: 0.7018 - val_loss: 0.5353 - val_accuracy: 0.7412 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 12s - loss: 0.6326 - accuracy: 0.7083 - val_loss: 0.5298 - val_accuracy: 0.7445 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 12s - loss: 0.6377 - accuracy: 0.7069 - val_loss: 0.5271 - val_accuracy: 0.7484 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 12s - loss: 0.6300 - accuracy: 0.7104 - val_loss: 0.5232 - val_accuracy: 0.7482 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 12s - loss: 0.6330 - accuracy: 0.7107 - val_loss: 0.5267 - val_accuracy: 0.7440 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 12s - loss: 0.6264 - accuracy: 0.7131 - val_loss: 0.5161 - val_accuracy: 0.7472 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 12s - loss: 0.6267 - accuracy: 0.7129 - val_loss: 0.5117 - val_accuracy: 0.7537 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 12s - loss: 0.6242 - accuracy: 0.7146 - val_loss: 0.5054 - val_accuracy: 0.7554 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 12s - loss: 0.6221 - accuracy: 0.7157 - val_loss: 0.5058 - val_accuracy: 0.7512 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 12s - loss: 0.6262 - accuracy: 0.7132 - val_loss: 0.5062 - val_accuracy: 0.7545 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 12s - loss: 0.6210 - accuracy: 0.7163 - val_loss: 0.5012 - val_accuracy: 0.7544 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 12s - loss: 0.6196 - accuracy: 0.7167 - val_loss: 0.5043 - val_accuracy: 0.7553 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 12s - loss: 0.6220 - accuracy: 0.7179 - val_loss: 0.5007 - val_accuracy: 0.7548 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 11s - loss: 0.6214 - accuracy: 0.7153 - val_loss: 0.5012 - val_accuracy: 0.7567 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 12s - loss: 0.6166 - accuracy: 0.7174 - val_loss: 0.5006 - val_accuracy: 0.7556 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 12s - loss: 0.6173 - accuracy: 0.7177 - val_loss: 0.5022 - val_accuracy: 0.7540 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 12s - loss: 0.6149 - accuracy: 0.7181 - val_loss: 0.4992 - val_accuracy: 0.7554 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 12s - loss: 0.6099 - accuracy: 0.7212 - val_loss: 0.4984 - val_accuracy: 0.7577 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 11s - loss: 0.6094 - accuracy: 0.7210 - val_loss: 0.4969 - val_accuracy: 0.7535 - lr: 0.0025 - 11s/epoch - 13ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 12s - loss: 0.6084 - accuracy: 0.7209 - val_loss: 0.4956 - val_accuracy: 0.7563 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 12s - loss: 0.6072 - accuracy: 0.7213 - val_loss: 0.4936 - val_accuracy: 0.7557 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 11s - loss: 0.6044 - accuracy: 0.7208 - val_loss: 0.4927 - val_accuracy: 0.7550 - lr: 0.0025 - 11s/epoch - 13ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 12s - loss: 0.6034 - accuracy: 0.7198 - val_loss: 0.4893 - val_accuracy: 0.7551 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 11s - loss: 0.6018 - accuracy: 0.7204 - val_loss: 0.4888 - val_accuracy: 0.7544 - lr: 0.0025 - 11s/epoch - 13ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 12s - loss: 0.6018 - accuracy: 0.7211 - val_loss: 0.4873 - val_accuracy: 0.7570 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 12s - loss: 0.6016 - accuracy: 0.7225 - val_loss: 0.4864 - val_accuracy: 0.7584 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 11s - loss: 0.6022 - accuracy: 0.7206 - val_loss: 0.4873 - val_accuracy: 0.7577 - lr: 0.0025 - 11s/epoch - 13ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 12s - loss: 0.5990 - accuracy: 0.7222 - val_loss: 0.4870 - val_accuracy: 0.7556 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 11s - loss: 0.6029 - accuracy: 0.7208 - val_loss: 0.4871 - val_accuracy: 0.7541 - lr: 0.0025 - 11s/epoch - 13ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 12s - loss: 0.6006 - accuracy: 0.7206 - val_loss: 0.4850 - val_accuracy: 0.7552 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 12s - loss: 0.5968 - accuracy: 0.7228 - val_loss: 0.4845 - val_accuracy: 0.7539 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 12s - loss: 0.5963 - accuracy: 0.7242 - val_loss: 0.4850 - val_accuracy: 0.7584 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 12s - loss: 0.5968 - accuracy: 0.7242 - val_loss: 0.4845 - val_accuracy: 0.7603 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 12s - loss: 0.5983 - accuracy: 0.7223 - val_loss: 0.4851 - val_accuracy: 0.7563 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 12s - loss: 0.5953 - accuracy: 0.7236 - val_loss: 0.4835 - val_accuracy: 0.7577 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 12s - loss: 0.5959 - accuracy: 0.7245 - val_loss: 0.4823 - val_accuracy: 0.7555 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 12s - loss: 0.5949 - accuracy: 0.7235 - val_loss: 0.4831 - val_accuracy: 0.7554 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 12s - loss: 0.5933 - accuracy: 0.7251 - val_loss: 0.4827 - val_accuracy: 0.7603 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 12s - loss: 0.5944 - accuracy: 0.7240 - val_loss: 0.4826 - val_accuracy: 0.7599 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 12s - loss: 0.5950 - accuracy: 0.7249 - val_loss: 0.4821 - val_accuracy: 0.7638 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 12s - loss: 0.5941 - accuracy: 0.7240 - val_loss: 0.4816 - val_accuracy: 0.7634 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 12s - loss: 0.5931 - accuracy: 0.7260 - val_loss: 0.4818 - val_accuracy: 0.7600 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 12s - loss: 0.5942 - accuracy: 0.7244 - val_loss: 0.4819 - val_accuracy: 0.7636 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 12s - loss: 0.5959 - accuracy: 0.7251 - val_loss: 0.4821 - val_accuracy: 0.7646 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 11s - loss: 0.5942 - accuracy: 0.7243 - val_loss: 0.4814 - val_accuracy: 0.7641 - lr: 1.5625e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 70/600\n",
            "892/892 - 12s - loss: 0.5917 - accuracy: 0.7257 - val_loss: 0.4817 - val_accuracy: 0.7641 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 71/600\n",
            "892/892 - 12s - loss: 0.5899 - accuracy: 0.7274 - val_loss: 0.4814 - val_accuracy: 0.7638 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 72/600\n",
            "892/892 - 12s - loss: 0.5927 - accuracy: 0.7260 - val_loss: 0.4814 - val_accuracy: 0.7639 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 73/600\n",
            "892/892 - 12s - loss: 0.5940 - accuracy: 0.7249 - val_loss: 0.4814 - val_accuracy: 0.7644 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 74/600\n",
            "892/892 - 12s - loss: 0.5922 - accuracy: 0.7273 - val_loss: 0.4813 - val_accuracy: 0.7648 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 75/600\n",
            "892/892 - 12s - loss: 0.5912 - accuracy: 0.7274 - val_loss: 0.4812 - val_accuracy: 0.7632 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 76/600\n",
            "892/892 - 12s - loss: 0.5936 - accuracy: 0.7247 - val_loss: 0.4815 - val_accuracy: 0.7647 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 77/600\n",
            "892/892 - 12s - loss: 0.5919 - accuracy: 0.7270 - val_loss: 0.4812 - val_accuracy: 0.7648 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 78/600\n",
            "892/892 - 12s - loss: 0.5934 - accuracy: 0.7267 - val_loss: 0.4813 - val_accuracy: 0.7645 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 79/600\n",
            "892/892 - 12s - loss: 0.5903 - accuracy: 0.7255 - val_loss: 0.4812 - val_accuracy: 0.7644 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 80/600\n",
            "892/892 - 12s - loss: 0.5911 - accuracy: 0.7265 - val_loss: 0.4811 - val_accuracy: 0.7647 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 81/600\n",
            "892/892 - 12s - loss: 0.5936 - accuracy: 0.7236 - val_loss: 0.4812 - val_accuracy: 0.7646 - lr: 3.9062e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 82/600\n",
            "892/892 - 12s - loss: 0.5905 - accuracy: 0.7269 - val_loss: 0.4811 - val_accuracy: 0.7647 - lr: 1.9531e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 83/600\n",
            "892/892 - 12s - loss: 0.5932 - accuracy: 0.7269 - val_loss: 0.4811 - val_accuracy: 0.7647 - lr: 1.9531e-05 - 12s/epoch - 14ms/step\n",
            "Epoch 84/600\n",
            "892/892 - 12s - loss: 0.5916 - accuracy: 0.7264 - val_loss: 0.4811 - val_accuracy: 0.7658 - lr: 1.9531e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 85/600\n",
            "892/892 - 12s - loss: 0.5921 - accuracy: 0.7267 - val_loss: 0.4811 - val_accuracy: 0.7658 - lr: 9.7656e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 86/600\n",
            "892/892 - 12s - loss: 0.5906 - accuracy: 0.7277 - val_loss: 0.4811 - val_accuracy: 0.7658 - lr: 9.7656e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 87/600\n",
            "892/892 - 12s - loss: 0.5948 - accuracy: 0.7247 - val_loss: 0.4811 - val_accuracy: 0.7653 - lr: 9.7656e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 88/600\n",
            "892/892 - 12s - loss: 0.5926 - accuracy: 0.7265 - val_loss: 0.4811 - val_accuracy: 0.7653 - lr: 4.8828e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 89/600\n",
            "892/892 - 12s - loss: 0.5904 - accuracy: 0.7282 - val_loss: 0.4811 - val_accuracy: 0.7653 - lr: 4.8828e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 90/600\n",
            "892/892 - 13s - loss: 0.5919 - accuracy: 0.7255 - val_loss: 0.4811 - val_accuracy: 0.7653 - lr: 4.8828e-06 - 13s/epoch - 14ms/step\n",
            "Epoch 91/600\n",
            "892/892 - 12s - loss: 0.5921 - accuracy: 0.7268 - val_loss: 0.4811 - val_accuracy: 0.7653 - lr: 2.4414e-06 - 12s/epoch - 14ms/step\n",
            "Epoch 92/600\n",
            "892/892 - 12s - loss: 0.5929 - accuracy: 0.7265 - val_loss: 0.4811 - val_accuracy: 0.7653 - lr: 2.4414e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 93/600\n",
            "892/892 - 12s - loss: 0.5937 - accuracy: 0.7255 - val_loss: 0.4811 - val_accuracy: 0.7653 - lr: 2.4414e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 94/600\n",
            "892/892 - 12s - loss: 0.5915 - accuracy: 0.7258 - val_loss: 0.4811 - val_accuracy: 0.7653 - lr: 1.2207e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 95/600\n",
            "892/892 - 12s - loss: 0.5908 - accuracy: 0.7260 - val_loss: 0.4811 - val_accuracy: 0.7653 - lr: 1.2207e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 96/600\n",
            "892/892 - 12s - loss: 0.5907 - accuracy: 0.7272 - val_loss: 0.4811 - val_accuracy: 0.7653 - lr: 1.2207e-06 - 12s/epoch - 13ms/step\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_35 (LSTM)              (None, 10, 16)            1920      \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 10, 16)            0         \n",
            "                                                                 \n",
            " lstm_36 (LSTM)              (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 13)                221       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,253\n",
            "Trainable params: 4,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13 11]\n",
            " [ 6  7  8 13  2]\n",
            " [ 6  7  8 13  2]\n",
            " ...\n",
            " [ 6  7  8 13 11]\n",
            " [ 6  7  8 13  1]\n",
            " [ 6  7  8 13  2]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13 11 11]\n",
            " [ 6  7  8 13  2 11]\n",
            " [ 6  7  8 13  2  4]\n",
            " ...\n",
            " [ 6  7  8 13 11  3]\n",
            " [ 6  7  8 13  1  4]\n",
            " [ 6  7  8 13  2  3]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ... 11 11  2]\n",
            " [ 6  7  8 ...  2 11  4]\n",
            " [ 6  7  8 ...  2  4 11]\n",
            " ...\n",
            " [ 6  7  8 ... 11  3  3]\n",
            " [ 6  7  8 ...  1  4  1]\n",
            " [ 6  7  8 ...  2  3  4]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ... 11  2  3]\n",
            " [ 6  7  8 ... 11  4  1]\n",
            " [ 6  7  8 ...  4 11 11]\n",
            " ...\n",
            " [ 6  7  8 ...  3  3  1]\n",
            " [ 6  7  8 ...  4  1  3]\n",
            " [ 6  7  8 ...  3  4 11]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  2  3  4]\n",
            " [ 6  7  8 ...  4  1  3]\n",
            " [ 6  7  8 ... 11 11  3]\n",
            " ...\n",
            " [ 6  7  8 ...  3  1  4]\n",
            " [ 6  7  8 ...  1  3  2]\n",
            " [ 6  7  8 ...  4 11 11]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  3  4 10]\n",
            " [ 6  7  8 ...  1  3 10]\n",
            " [ 6  7  8 ... 11  3 10]\n",
            " ...\n",
            " [ 6  7  8 ...  1  4 10]\n",
            " [ 6  7  8 ...  3  2 10]\n",
            " [ 6  7  8 ... 11 11 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  3 10  5]\n",
            " [ 6  7  8 ...  3 10  5]\n",
            " ...\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ... 11 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_37 (LSTM)              (None, 10, 16)            1920      \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 10, 16)            0         \n",
            "                                                                 \n",
            " lstm_38 (LSTM)              (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 13)                221       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,253\n",
            "Trainable params: 4,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 17s - loss: 1.1164 - accuracy: 0.5405 - val_loss: 0.7485 - val_accuracy: 0.6667 - lr: 0.0050 - 17s/epoch - 19ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 12s - loss: 0.8742 - accuracy: 0.6191 - val_loss: 0.7301 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 12s - loss: 0.8286 - accuracy: 0.6292 - val_loss: 0.7202 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 12s - loss: 0.8258 - accuracy: 0.6321 - val_loss: 0.7206 - val_accuracy: 0.6841 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 12s - loss: 0.7913 - accuracy: 0.6542 - val_loss: 0.6789 - val_accuracy: 0.7135 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 12s - loss: 0.7818 - accuracy: 0.6620 - val_loss: 0.6627 - val_accuracy: 0.7026 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 12s - loss: 0.7610 - accuracy: 0.6657 - val_loss: 0.6458 - val_accuracy: 0.7156 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 12s - loss: 0.7758 - accuracy: 0.6726 - val_loss: 0.6704 - val_accuracy: 0.7481 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 12s - loss: 0.7474 - accuracy: 0.6854 - val_loss: 0.6081 - val_accuracy: 0.7587 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 12s - loss: 0.7545 - accuracy: 0.6878 - val_loss: 0.6022 - val_accuracy: 0.7608 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 12s - loss: 0.7293 - accuracy: 0.6903 - val_loss: 0.6051 - val_accuracy: 0.7412 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 12s - loss: 0.7211 - accuracy: 0.6915 - val_loss: 0.5878 - val_accuracy: 0.7489 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 12s - loss: 0.7108 - accuracy: 0.6943 - val_loss: 0.5844 - val_accuracy: 0.7473 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 12s - loss: 0.7181 - accuracy: 0.6938 - val_loss: 0.5699 - val_accuracy: 0.7519 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 12s - loss: 0.7208 - accuracy: 0.6974 - val_loss: 0.5780 - val_accuracy: 0.7543 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 12s - loss: 0.7027 - accuracy: 0.7011 - val_loss: 0.5576 - val_accuracy: 0.7555 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 12s - loss: 0.6981 - accuracy: 0.7011 - val_loss: 0.5644 - val_accuracy: 0.7538 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 12s - loss: 0.7055 - accuracy: 0.7015 - val_loss: 0.5563 - val_accuracy: 0.7545 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 12s - loss: 0.6884 - accuracy: 0.7046 - val_loss: 0.5510 - val_accuracy: 0.7563 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 12s - loss: 0.6938 - accuracy: 0.7040 - val_loss: 0.5520 - val_accuracy: 0.7536 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 12s - loss: 0.6868 - accuracy: 0.7040 - val_loss: 0.5553 - val_accuracy: 0.7471 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 12s - loss: 0.6975 - accuracy: 0.7043 - val_loss: 0.5461 - val_accuracy: 0.7588 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 12s - loss: 0.6827 - accuracy: 0.7047 - val_loss: 0.5483 - val_accuracy: 0.7551 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 12s - loss: 0.6919 - accuracy: 0.7058 - val_loss: 0.5575 - val_accuracy: 0.7602 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 12s - loss: 0.6848 - accuracy: 0.7076 - val_loss: 0.5425 - val_accuracy: 0.7684 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 12s - loss: 0.6841 - accuracy: 0.7059 - val_loss: 0.5401 - val_accuracy: 0.7633 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 12s - loss: 0.6863 - accuracy: 0.7059 - val_loss: 0.5425 - val_accuracy: 0.7611 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 12s - loss: 0.6868 - accuracy: 0.7082 - val_loss: 0.5417 - val_accuracy: 0.7624 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 12s - loss: 0.6755 - accuracy: 0.7101 - val_loss: 0.5314 - val_accuracy: 0.7695 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 12s - loss: 0.6738 - accuracy: 0.7096 - val_loss: 0.5359 - val_accuracy: 0.7721 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 12s - loss: 0.6865 - accuracy: 0.7098 - val_loss: 0.5383 - val_accuracy: 0.7631 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 12s - loss: 0.6737 - accuracy: 0.7094 - val_loss: 0.5349 - val_accuracy: 0.7734 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 12s - loss: 0.6663 - accuracy: 0.7122 - val_loss: 0.5250 - val_accuracy: 0.7708 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 11s - loss: 0.6615 - accuracy: 0.7126 - val_loss: 0.5259 - val_accuracy: 0.7718 - lr: 0.0025 - 11s/epoch - 13ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 12s - loss: 0.6621 - accuracy: 0.7149 - val_loss: 0.5193 - val_accuracy: 0.7694 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 12s - loss: 0.6639 - accuracy: 0.7135 - val_loss: 0.5189 - val_accuracy: 0.7721 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 12s - loss: 0.6600 - accuracy: 0.7152 - val_loss: 0.5229 - val_accuracy: 0.7716 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 12s - loss: 0.6595 - accuracy: 0.7134 - val_loss: 0.5166 - val_accuracy: 0.7700 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 12s - loss: 0.6663 - accuracy: 0.7114 - val_loss: 0.5231 - val_accuracy: 0.7726 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 12s - loss: 0.6600 - accuracy: 0.7123 - val_loss: 0.5161 - val_accuracy: 0.7699 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 12s - loss: 0.6577 - accuracy: 0.7126 - val_loss: 0.5155 - val_accuracy: 0.7709 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 12s - loss: 0.6658 - accuracy: 0.7114 - val_loss: 0.5202 - val_accuracy: 0.7712 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 12s - loss: 0.6557 - accuracy: 0.7147 - val_loss: 0.5164 - val_accuracy: 0.7712 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 12s - loss: 0.6566 - accuracy: 0.7121 - val_loss: 0.5131 - val_accuracy: 0.7712 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 12s - loss: 0.6594 - accuracy: 0.7135 - val_loss: 0.5156 - val_accuracy: 0.7728 - lr: 0.0025 - 12s/epoch - 14ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 12s - loss: 0.6561 - accuracy: 0.7121 - val_loss: 0.5116 - val_accuracy: 0.7689 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 12s - loss: 0.6557 - accuracy: 0.7136 - val_loss: 0.5262 - val_accuracy: 0.7635 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 12s - loss: 0.6563 - accuracy: 0.7134 - val_loss: 0.5119 - val_accuracy: 0.7716 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 12s - loss: 0.6566 - accuracy: 0.7141 - val_loss: 0.5250 - val_accuracy: 0.7710 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 12s - loss: 0.6554 - accuracy: 0.7134 - val_loss: 0.5115 - val_accuracy: 0.7711 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 12s - loss: 0.6494 - accuracy: 0.7161 - val_loss: 0.5075 - val_accuracy: 0.7712 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 12s - loss: 0.6483 - accuracy: 0.7164 - val_loss: 0.5063 - val_accuracy: 0.7717 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 12s - loss: 0.6486 - accuracy: 0.7165 - val_loss: 0.5072 - val_accuracy: 0.7725 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 11s - loss: 0.6467 - accuracy: 0.7173 - val_loss: 0.5058 - val_accuracy: 0.7717 - lr: 0.0012 - 11s/epoch - 13ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 12s - loss: 0.6467 - accuracy: 0.7164 - val_loss: 0.5055 - val_accuracy: 0.7715 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 12s - loss: 0.6460 - accuracy: 0.7165 - val_loss: 0.5063 - val_accuracy: 0.7715 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 12s - loss: 0.6468 - accuracy: 0.7168 - val_loss: 0.5045 - val_accuracy: 0.7710 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 12s - loss: 0.6416 - accuracy: 0.7195 - val_loss: 0.5049 - val_accuracy: 0.7713 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 12s - loss: 0.6413 - accuracy: 0.7198 - val_loss: 0.5026 - val_accuracy: 0.7701 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 12s - loss: 0.6412 - accuracy: 0.7194 - val_loss: 0.5056 - val_accuracy: 0.7695 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 12s - loss: 0.6443 - accuracy: 0.7178 - val_loss: 0.5051 - val_accuracy: 0.7721 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 12s - loss: 0.6443 - accuracy: 0.7183 - val_loss: 0.5088 - val_accuracy: 0.7706 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 12s - loss: 0.6411 - accuracy: 0.7204 - val_loss: 0.5011 - val_accuracy: 0.7706 - lr: 6.2500e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 12s - loss: 0.6423 - accuracy: 0.7190 - val_loss: 0.5038 - val_accuracy: 0.7657 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 11s - loss: 0.6401 - accuracy: 0.7183 - val_loss: 0.5011 - val_accuracy: 0.7673 - lr: 6.2500e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 12s - loss: 0.6414 - accuracy: 0.7190 - val_loss: 0.5028 - val_accuracy: 0.7716 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 12s - loss: 0.6389 - accuracy: 0.7204 - val_loss: 0.5016 - val_accuracy: 0.7719 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 12s - loss: 0.6402 - accuracy: 0.7201 - val_loss: 0.4994 - val_accuracy: 0.7721 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 12s - loss: 0.6388 - accuracy: 0.7205 - val_loss: 0.5000 - val_accuracy: 0.7719 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 70/600\n",
            "892/892 - 11s - loss: 0.6364 - accuracy: 0.7210 - val_loss: 0.5000 - val_accuracy: 0.7716 - lr: 3.1250e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 71/600\n",
            "892/892 - 12s - loss: 0.6377 - accuracy: 0.7197 - val_loss: 0.4989 - val_accuracy: 0.7721 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 72/600\n",
            "892/892 - 12s - loss: 0.6382 - accuracy: 0.7215 - val_loss: 0.4989 - val_accuracy: 0.7714 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 73/600\n",
            "892/892 - 12s - loss: 0.6377 - accuracy: 0.7212 - val_loss: 0.5011 - val_accuracy: 0.7697 - lr: 3.1250e-04 - 12s/epoch - 14ms/step\n",
            "Epoch 74/600\n",
            "892/892 - 12s - loss: 0.6379 - accuracy: 0.7203 - val_loss: 0.5000 - val_accuracy: 0.7714 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 75/600\n",
            "892/892 - 11s - loss: 0.6355 - accuracy: 0.7210 - val_loss: 0.4992 - val_accuracy: 0.7719 - lr: 1.5625e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 76/600\n",
            "892/892 - 12s - loss: 0.6369 - accuracy: 0.7198 - val_loss: 0.4995 - val_accuracy: 0.7730 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 77/600\n",
            "892/892 - 12s - loss: 0.6354 - accuracy: 0.7212 - val_loss: 0.4987 - val_accuracy: 0.7725 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 78/600\n",
            "892/892 - 12s - loss: 0.6374 - accuracy: 0.7203 - val_loss: 0.4992 - val_accuracy: 0.7726 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 79/600\n",
            "892/892 - 12s - loss: 0.6382 - accuracy: 0.7200 - val_loss: 0.4997 - val_accuracy: 0.7723 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 80/600\n",
            "892/892 - 11s - loss: 0.6389 - accuracy: 0.7187 - val_loss: 0.4995 - val_accuracy: 0.7722 - lr: 1.5625e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 81/600\n",
            "892/892 - 12s - loss: 0.6389 - accuracy: 0.7190 - val_loss: 0.4994 - val_accuracy: 0.7726 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 82/600\n",
            "892/892 - 12s - loss: 0.6378 - accuracy: 0.7192 - val_loss: 0.4985 - val_accuracy: 0.7721 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 83/600\n",
            "892/892 - 12s - loss: 0.6368 - accuracy: 0.7205 - val_loss: 0.4981 - val_accuracy: 0.7724 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 84/600\n",
            "892/892 - 12s - loss: 0.6348 - accuracy: 0.7220 - val_loss: 0.4984 - val_accuracy: 0.7717 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 85/600\n",
            "892/892 - 12s - loss: 0.6374 - accuracy: 0.7204 - val_loss: 0.4985 - val_accuracy: 0.7723 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 86/600\n",
            "892/892 - 12s - loss: 0.6393 - accuracy: 0.7196 - val_loss: 0.4989 - val_accuracy: 0.7724 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 87/600\n",
            "892/892 - 12s - loss: 0.6349 - accuracy: 0.7210 - val_loss: 0.4985 - val_accuracy: 0.7723 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 88/600\n",
            "892/892 - 12s - loss: 0.6358 - accuracy: 0.7205 - val_loss: 0.4990 - val_accuracy: 0.7725 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 89/600\n",
            "892/892 - 12s - loss: 0.6360 - accuracy: 0.7207 - val_loss: 0.4981 - val_accuracy: 0.7729 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 90/600\n",
            "892/892 - 12s - loss: 0.6355 - accuracy: 0.7202 - val_loss: 0.4984 - val_accuracy: 0.7729 - lr: 1.9531e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 91/600\n",
            "892/892 - 12s - loss: 0.6364 - accuracy: 0.7211 - val_loss: 0.4981 - val_accuracy: 0.7724 - lr: 1.9531e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 92/600\n",
            "892/892 - 12s - loss: 0.6378 - accuracy: 0.7187 - val_loss: 0.4983 - val_accuracy: 0.7724 - lr: 1.9531e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 93/600\n",
            "892/892 - 12s - loss: 0.6374 - accuracy: 0.7194 - val_loss: 0.4984 - val_accuracy: 0.7724 - lr: 9.7656e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 94/600\n",
            "892/892 - 12s - loss: 0.6366 - accuracy: 0.7201 - val_loss: 0.4984 - val_accuracy: 0.7729 - lr: 9.7656e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 95/600\n",
            "892/892 - 12s - loss: 0.6362 - accuracy: 0.7199 - val_loss: 0.4985 - val_accuracy: 0.7724 - lr: 9.7656e-06 - 12s/epoch - 13ms/step\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_37 (LSTM)              (None, 10, 16)            1920      \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 10, 16)            0         \n",
            "                                                                 \n",
            " lstm_38 (LSTM)              (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 13)                221       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,253\n",
            "Trainable params: 4,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13  2]\n",
            " [ 6  7  8 13  1]\n",
            " [ 6  7  8 13 11]\n",
            " ...\n",
            " [ 6  7  8 13  1]\n",
            " [ 6  7  8 13  1]\n",
            " [ 6  7  8 13  1]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13  2  3]\n",
            " [ 6  7  8 13  1  2]\n",
            " [ 6  7  8 13 11  3]\n",
            " ...\n",
            " [ 6  7  8 13  1  2]\n",
            " [ 6  7  8 13  1  2]\n",
            " [ 6  7  8 13  1  1]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ...  2  3  1]\n",
            " [ 6  7  8 ...  1  2  2]\n",
            " [ 6  7  8 ... 11  3  1]\n",
            " ...\n",
            " [ 6  7  8 ...  1  2 11]\n",
            " [ 6  7  8 ...  1  2 11]\n",
            " [ 6  7  8 ...  1  1  2]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  3  1 11]\n",
            " [ 6  7  8 ...  2  2  4]\n",
            " [ 6  7  8 ...  3  1  4]\n",
            " ...\n",
            " [ 6  7  8 ...  2 11 11]\n",
            " [ 6  7  8 ...  2 11 11]\n",
            " [ 6  7  8 ...  1  2 11]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  1 11  4]\n",
            " [ 6  7  8 ...  2  4  3]\n",
            " [ 6  7  8 ...  1  4  2]\n",
            " ...\n",
            " [ 6  7  8 ... 11 11  1]\n",
            " [ 6  7  8 ... 11 11  1]\n",
            " [ 6  7  8 ...  2 11  4]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ... 11  4 10]\n",
            " [ 6  7  8 ...  4  3 10]\n",
            " [ 6  7  8 ...  4  2 10]\n",
            " ...\n",
            " [ 6  7  8 ... 11  1 10]\n",
            " [ 6  7  8 ... 11  1 10]\n",
            " [ 6  7  8 ... 11  4 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  3 10  5]\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " ...\n",
            " [ 6  7  8 ...  1 10  5]\n",
            " [ 6  7  8 ...  1 10  5]\n",
            " [ 6  7  8 ...  4 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_39 (LSTM)              (None, 10, 16)            1920      \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 10, 16)            0         \n",
            "                                                                 \n",
            " lstm_40 (LSTM)              (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 13)                221       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,253\n",
            "Trainable params: 4,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 17s - loss: 1.2828 - accuracy: 0.5081 - val_loss: 0.8519 - val_accuracy: 0.6673 - lr: 0.0050 - 17s/epoch - 19ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 12s - loss: 1.0103 - accuracy: 0.5791 - val_loss: 0.8001 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 12s - loss: 0.9653 - accuracy: 0.5896 - val_loss: 0.7776 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 11s - loss: 0.9609 - accuracy: 0.5916 - val_loss: 0.7767 - val_accuracy: 0.6637 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 12s - loss: 0.9490 - accuracy: 0.5940 - val_loss: 0.8284 - val_accuracy: 0.6679 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 12s - loss: 0.9376 - accuracy: 0.5987 - val_loss: 0.7677 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 12s - loss: 0.9305 - accuracy: 0.6030 - val_loss: 0.7740 - val_accuracy: 0.6654 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 12s - loss: 0.9222 - accuracy: 0.6029 - val_loss: 0.7665 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 12s - loss: 0.9213 - accuracy: 0.6028 - val_loss: 0.8250 - val_accuracy: 0.6664 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 12s - loss: 0.9217 - accuracy: 0.6070 - val_loss: 0.7670 - val_accuracy: 0.6667 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 12s - loss: 0.9113 - accuracy: 0.6108 - val_loss: 0.7674 - val_accuracy: 0.6674 - lr: 0.0050 - 12s/epoch - 13ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 12s - loss: 0.8934 - accuracy: 0.6146 - val_loss: 0.7548 - val_accuracy: 0.6667 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 12s - loss: 0.8984 - accuracy: 0.6131 - val_loss: 0.7546 - val_accuracy: 0.6661 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 12s - loss: 0.8919 - accuracy: 0.6131 - val_loss: 0.7548 - val_accuracy: 0.6660 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 12s - loss: 0.8902 - accuracy: 0.6130 - val_loss: 0.7568 - val_accuracy: 0.6667 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 12s - loss: 0.8909 - accuracy: 0.6137 - val_loss: 0.7573 - val_accuracy: 0.6667 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 12s - loss: 0.8810 - accuracy: 0.6177 - val_loss: 0.7476 - val_accuracy: 0.6667 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 12s - loss: 0.8768 - accuracy: 0.6183 - val_loss: 0.7501 - val_accuracy: 0.6667 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 12s - loss: 0.8763 - accuracy: 0.6195 - val_loss: 0.7550 - val_accuracy: 0.6667 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 12s - loss: 0.8750 - accuracy: 0.6191 - val_loss: 0.7440 - val_accuracy: 0.6679 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 12s - loss: 0.8782 - accuracy: 0.6178 - val_loss: 0.7473 - val_accuracy: 0.6667 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 12s - loss: 0.8749 - accuracy: 0.6186 - val_loss: 0.7444 - val_accuracy: 0.6684 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 12s - loss: 0.8758 - accuracy: 0.6182 - val_loss: 0.7469 - val_accuracy: 0.6666 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 11s - loss: 0.8719 - accuracy: 0.6179 - val_loss: 0.7430 - val_accuracy: 0.6667 - lr: 6.2500e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 12s - loss: 0.8680 - accuracy: 0.6201 - val_loss: 0.7431 - val_accuracy: 0.6667 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 12s - loss: 0.8682 - accuracy: 0.6185 - val_loss: 0.7421 - val_accuracy: 0.6666 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 12s - loss: 0.8678 - accuracy: 0.6198 - val_loss: 0.7432 - val_accuracy: 0.6673 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 11s - loss: 0.8682 - accuracy: 0.6183 - val_loss: 0.7417 - val_accuracy: 0.6667 - lr: 6.2500e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 12s - loss: 0.8680 - accuracy: 0.6180 - val_loss: 0.7419 - val_accuracy: 0.6666 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 12s - loss: 0.8678 - accuracy: 0.6182 - val_loss: 0.7419 - val_accuracy: 0.6667 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 12s - loss: 0.8678 - accuracy: 0.6195 - val_loss: 0.7434 - val_accuracy: 0.6667 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 12s - loss: 0.8646 - accuracy: 0.6200 - val_loss: 0.7407 - val_accuracy: 0.6667 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 11s - loss: 0.8628 - accuracy: 0.6208 - val_loss: 0.7399 - val_accuracy: 0.6667 - lr: 3.1250e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 12s - loss: 0.8616 - accuracy: 0.6208 - val_loss: 0.7395 - val_accuracy: 0.6667 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 11s - loss: 0.8620 - accuracy: 0.6231 - val_loss: 0.7398 - val_accuracy: 0.6667 - lr: 3.1250e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 12s - loss: 0.8635 - accuracy: 0.6193 - val_loss: 0.7405 - val_accuracy: 0.6667 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 12s - loss: 0.8620 - accuracy: 0.6211 - val_loss: 0.7395 - val_accuracy: 0.6653 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 11s - loss: 0.8635 - accuracy: 0.6202 - val_loss: 0.7397 - val_accuracy: 0.6667 - lr: 1.5625e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 12s - loss: 0.8610 - accuracy: 0.6210 - val_loss: 0.7397 - val_accuracy: 0.6667 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 11s - loss: 0.8618 - accuracy: 0.6216 - val_loss: 0.7390 - val_accuracy: 0.6667 - lr: 1.5625e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 12s - loss: 0.8630 - accuracy: 0.6205 - val_loss: 0.7385 - val_accuracy: 0.6667 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 12s - loss: 0.8625 - accuracy: 0.6201 - val_loss: 0.7386 - val_accuracy: 0.6667 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 11s - loss: 0.8622 - accuracy: 0.6221 - val_loss: 0.7383 - val_accuracy: 0.6667 - lr: 1.5625e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 12s - loss: 0.8582 - accuracy: 0.6223 - val_loss: 0.7387 - val_accuracy: 0.6677 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 11s - loss: 0.8621 - accuracy: 0.6203 - val_loss: 0.7387 - val_accuracy: 0.6658 - lr: 1.5625e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 12s - loss: 0.8601 - accuracy: 0.6230 - val_loss: 0.7388 - val_accuracy: 0.6673 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 12s - loss: 0.8599 - accuracy: 0.6210 - val_loss: 0.7381 - val_accuracy: 0.6667 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 12s - loss: 0.8605 - accuracy: 0.6188 - val_loss: 0.7381 - val_accuracy: 0.6673 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 12s - loss: 0.8601 - accuracy: 0.6209 - val_loss: 0.7381 - val_accuracy: 0.6667 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 12s - loss: 0.8584 - accuracy: 0.6212 - val_loss: 0.7385 - val_accuracy: 0.6673 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 12s - loss: 0.8584 - accuracy: 0.6226 - val_loss: 0.7380 - val_accuracy: 0.6674 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 12s - loss: 0.8596 - accuracy: 0.6221 - val_loss: 0.7382 - val_accuracy: 0.6631 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 12s - loss: 0.8580 - accuracy: 0.6214 - val_loss: 0.7378 - val_accuracy: 0.6667 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 12s - loss: 0.8599 - accuracy: 0.6215 - val_loss: 0.7379 - val_accuracy: 0.6673 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 12s - loss: 0.8616 - accuracy: 0.6221 - val_loss: 0.7386 - val_accuracy: 0.6667 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 11s - loss: 0.8610 - accuracy: 0.6206 - val_loss: 0.7379 - val_accuracy: 0.6673 - lr: 3.9062e-05 - 11s/epoch - 13ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 12s - loss: 0.8595 - accuracy: 0.6227 - val_loss: 0.7379 - val_accuracy: 0.6652 - lr: 1.9531e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 12s - loss: 0.8572 - accuracy: 0.6224 - val_loss: 0.7382 - val_accuracy: 0.6674 - lr: 1.9531e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 11s - loss: 0.8599 - accuracy: 0.6211 - val_loss: 0.7382 - val_accuracy: 0.6673 - lr: 1.9531e-05 - 11s/epoch - 13ms/step\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_39 (LSTM)              (None, 10, 16)            1920      \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 10, 16)            0         \n",
            "                                                                 \n",
            " lstm_40 (LSTM)              (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 13)                221       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,253\n",
            "Trainable params: 4,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13  2]\n",
            " [ 6  7  8 13  3]\n",
            " [ 6  7  8 13  4]\n",
            " ...\n",
            " [ 6  7  8 13  4]\n",
            " [ 6  7  8 13  3]\n",
            " [ 6  7  8 13  4]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13  2  3]\n",
            " [ 6  7  8 13  3  2]\n",
            " [ 6  7  8 13  4  2]\n",
            " ...\n",
            " [ 6  7  8 13  4 11]\n",
            " [ 6  7  8 13  3 11]\n",
            " [ 6  7  8 13  4  1]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ...  2  3  4]\n",
            " [ 6  7  8 ...  3  2 11]\n",
            " [ 6  7  8 ...  4  2  4]\n",
            " ...\n",
            " [ 6  7  8 ...  4 11  3]\n",
            " [ 6  7  8 ...  3 11  1]\n",
            " [ 6  7  8 ...  4  1 11]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  3  4  1]\n",
            " [ 6  7  8 ...  2 11  4]\n",
            " [ 6  7  8 ...  2  4  3]\n",
            " ...\n",
            " [ 6  7  8 ... 11  3  4]\n",
            " [ 6  7  8 ... 11  1  3]\n",
            " [ 6  7  8 ...  1 11  1]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  4  1  3]\n",
            " [ 6  7  8 ... 11  4  1]\n",
            " [ 6  7  8 ...  4  3  2]\n",
            " ...\n",
            " [ 6  7  8 ...  3  4 11]\n",
            " [ 6  7  8 ...  1  3  2]\n",
            " [ 6  7  8 ... 11  1  4]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  1  3 10]\n",
            " [ 6  7  8 ...  4  1 10]\n",
            " [ 6  7  8 ...  3  2 10]\n",
            " ...\n",
            " [ 6  7  8 ...  4 11 10]\n",
            " [ 6  7  8 ...  3  2 10]\n",
            " [ 6  7  8 ...  1  4 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  3 10  5]\n",
            " [ 6  7  8 ...  1 10  5]\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " ...\n",
            " [ 6  7  8 ... 11 10  5]\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ...  4 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_41 (LSTM)              (None, 10, 16)            1920      \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 10, 16)            0         \n",
            "                                                                 \n",
            " lstm_42 (LSTM)              (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 13)                221       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,253\n",
            "Trainable params: 4,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 17s - loss: 2.6344 - accuracy: 0.0832 - val_loss: 2.5107 - val_accuracy: 0.0833 - lr: 0.0050 - 17s/epoch - 19ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 11s - loss: 2.5119 - accuracy: 0.0821 - val_loss: 2.5096 - val_accuracy: 0.0833 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 11s - loss: 2.5113 - accuracy: 0.0831 - val_loss: 2.5104 - val_accuracy: 0.0833 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 11s - loss: 2.5109 - accuracy: 0.0838 - val_loss: 2.5107 - val_accuracy: 0.0833 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 11s - loss: 2.5105 - accuracy: 0.0829 - val_loss: 2.5104 - val_accuracy: 0.0833 - lr: 0.0050 - 11s/epoch - 13ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 12s - loss: 2.4970 - accuracy: 0.0813 - val_loss: 2.4958 - val_accuracy: 0.0833 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 11s - loss: 2.4966 - accuracy: 0.0840 - val_loss: 2.4967 - val_accuracy: 0.0833 - lr: 0.0025 - 11s/epoch - 13ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 11s - loss: 2.4967 - accuracy: 0.0817 - val_loss: 2.4961 - val_accuracy: 0.0833 - lr: 0.0025 - 11s/epoch - 13ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 12s - loss: 2.4966 - accuracy: 0.0834 - val_loss: 2.4959 - val_accuracy: 0.0833 - lr: 0.0025 - 12s/epoch - 13ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 12s - loss: 2.4908 - accuracy: 0.0818 - val_loss: 2.4905 - val_accuracy: 0.0833 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 12s - loss: 2.4908 - accuracy: 0.0824 - val_loss: 2.4908 - val_accuracy: 0.0833 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 12s - loss: 2.4910 - accuracy: 0.0831 - val_loss: 2.4909 - val_accuracy: 0.0833 - lr: 0.0012 - 12s/epoch - 13ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 11s - loss: 2.4910 - accuracy: 0.0818 - val_loss: 2.4904 - val_accuracy: 0.0833 - lr: 0.0012 - 11s/epoch - 13ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 12s - loss: 2.4880 - accuracy: 0.0815 - val_loss: 2.4879 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 12s - loss: 2.4879 - accuracy: 0.0818 - val_loss: 2.4878 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 12s - loss: 2.4879 - accuracy: 0.0819 - val_loss: 2.4877 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 12s - loss: 2.4879 - accuracy: 0.0828 - val_loss: 2.4877 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 11s - loss: 2.4880 - accuracy: 0.0810 - val_loss: 2.4880 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 12s - loss: 2.4880 - accuracy: 0.0821 - val_loss: 2.4877 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 11s - loss: 2.4864 - accuracy: 0.0816 - val_loss: 2.4864 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 12s - loss: 2.4863 - accuracy: 0.0829 - val_loss: 2.4865 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 12s - loss: 2.4863 - accuracy: 0.0829 - val_loss: 2.4862 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 11s - loss: 2.4863 - accuracy: 0.0812 - val_loss: 2.4862 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 12s - loss: 2.4863 - accuracy: 0.0830 - val_loss: 2.4864 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 12s - loss: 2.4863 - accuracy: 0.0832 - val_loss: 2.4862 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 11s - loss: 2.4856 - accuracy: 0.0822 - val_loss: 2.4856 - val_accuracy: 0.0833 - lr: 1.5625e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 12s - loss: 2.4856 - accuracy: 0.0820 - val_loss: 2.4856 - val_accuracy: 0.0833 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 11s - loss: 2.4856 - accuracy: 0.0831 - val_loss: 2.4856 - val_accuracy: 0.0833 - lr: 1.5625e-04 - 11s/epoch - 13ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 12s - loss: 2.4856 - accuracy: 0.0823 - val_loss: 2.4855 - val_accuracy: 0.0833 - lr: 1.5625e-04 - 12s/epoch - 13ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 12s - loss: 2.4852 - accuracy: 0.0841 - val_loss: 2.4852 - val_accuracy: 0.0833 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 11s - loss: 2.4852 - accuracy: 0.0832 - val_loss: 2.4853 - val_accuracy: 0.0833 - lr: 7.8125e-05 - 11s/epoch - 13ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 12s - loss: 2.4852 - accuracy: 0.0837 - val_loss: 2.4852 - val_accuracy: 0.0833 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 12s - loss: 2.4853 - accuracy: 0.0817 - val_loss: 2.4853 - val_accuracy: 0.0833 - lr: 7.8125e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 11s - loss: 2.4851 - accuracy: 0.0835 - val_loss: 2.4851 - val_accuracy: 0.0833 - lr: 3.9062e-05 - 11s/epoch - 13ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 12s - loss: 2.4851 - accuracy: 0.0825 - val_loss: 2.4851 - val_accuracy: 0.0833 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 11s - loss: 2.4851 - accuracy: 0.0836 - val_loss: 2.4851 - val_accuracy: 0.0833 - lr: 3.9062e-05 - 11s/epoch - 13ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 12s - loss: 2.4851 - accuracy: 0.0821 - val_loss: 2.4851 - val_accuracy: 0.0833 - lr: 3.9062e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 11s - loss: 2.4850 - accuracy: 0.0831 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 1.9531e-05 - 11s/epoch - 13ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 11s - loss: 2.4850 - accuracy: 0.0831 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 1.9531e-05 - 11s/epoch - 13ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 12s - loss: 2.4850 - accuracy: 0.0832 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 1.9531e-05 - 12s/epoch - 13ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 12s - loss: 2.4850 - accuracy: 0.0827 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 9.7656e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 12s - loss: 2.4850 - accuracy: 0.0843 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 9.7656e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 12s - loss: 2.4850 - accuracy: 0.0838 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 9.7656e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 11s - loss: 2.4850 - accuracy: 0.0838 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 9.7656e-06 - 11s/epoch - 13ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 11s - loss: 2.4849 - accuracy: 0.0825 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 4.8828e-06 - 11s/epoch - 13ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 11s - loss: 2.4849 - accuracy: 0.0831 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 4.8828e-06 - 11s/epoch - 13ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0830 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 4.8828e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0824 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 2.4414e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 11s - loss: 2.4849 - accuracy: 0.0828 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 2.4414e-06 - 11s/epoch - 13ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0834 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 2.4414e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 11s - loss: 2.4849 - accuracy: 0.0841 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.2207e-06 - 11s/epoch - 13ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0834 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.2207e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0840 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.2207e-06 - 12s/epoch - 13ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0834 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 6.1035e-07 - 12s/epoch - 13ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0823 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 6.1035e-07 - 12s/epoch - 13ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0833 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 6.1035e-07 - 12s/epoch - 13ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0825 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.0518e-07 - 12s/epoch - 13ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0814 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.0518e-07 - 12s/epoch - 13ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0830 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.0518e-07 - 12s/epoch - 13ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 11s - loss: 2.4849 - accuracy: 0.0833 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.5259e-07 - 11s/epoch - 13ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 11s - loss: 2.4849 - accuracy: 0.0817 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.5259e-07 - 11s/epoch - 13ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 11s - loss: 2.4849 - accuracy: 0.0825 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.5259e-07 - 11s/epoch - 13ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 11s - loss: 2.4849 - accuracy: 0.0841 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 7.6294e-08 - 11s/epoch - 13ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0837 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 7.6294e-08 - 12s/epoch - 13ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 11s - loss: 2.4849 - accuracy: 0.0832 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 7.6294e-08 - 11s/epoch - 13ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 11s - loss: 2.4849 - accuracy: 0.0835 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.8147e-08 - 11s/epoch - 13ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0827 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.8147e-08 - 12s/epoch - 13ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 11s - loss: 2.4849 - accuracy: 0.0832 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.8147e-08 - 11s/epoch - 13ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 12s - loss: 2.4849 - accuracy: 0.0848 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.9073e-08 - 12s/epoch - 13ms/step\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_41 (LSTM)              (None, 10, 16)            1920      \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 10, 16)            0         \n",
            "                                                                 \n",
            " lstm_42 (LSTM)              (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 13)                221       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,253\n",
            "Trainable params: 4,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[ 6  3]\n",
            " [ 6  4]\n",
            " [ 6  2]\n",
            " ...\n",
            " [ 6 13]\n",
            " [ 6 10]\n",
            " [ 6  9]]\n",
            "finding activity nr 4\n",
            "[[ 6  3  2]\n",
            " [ 6  4 10]\n",
            " [ 6  2  2]\n",
            " ...\n",
            " [ 6 13 10]\n",
            " [ 6 10 12]\n",
            " [ 6  9  9]]\n",
            "finding activity nr 5\n",
            "[[ 6  3  2 11]\n",
            " [ 6  4 10  1]\n",
            " [ 6  2  2  5]\n",
            " ...\n",
            " [ 6 13 10 12]\n",
            " [ 6 10 12  3]\n",
            " [ 6  9  9  5]]\n",
            "finding activity nr 6\n",
            "[[ 6  3  2 11 11]\n",
            " [ 6  4 10  1  2]\n",
            " [ 6  2  2  5  1]\n",
            " ...\n",
            " [ 6 13 10 12  5]\n",
            " [ 6 10 12  3 12]\n",
            " [ 6  9  9  5  8]]\n",
            "finding activity nr 7\n",
            "[[ 6  3  2 11 11  4]\n",
            " [ 6  4 10  1  2  4]\n",
            " [ 6  2  2  5  1  2]\n",
            " ...\n",
            " [ 6 13 10 12  5 12]\n",
            " [ 6 10 12  3 12  7]\n",
            " [ 6  9  9  5  8 12]]\n",
            "finding activity nr 8\n",
            "[[ 6  3  2 ... 11  4  1]\n",
            " [ 6  4 10 ...  2  4  8]\n",
            " [ 6  2  2 ...  1  2 10]\n",
            " ...\n",
            " [ 6 13 10 ...  5 12  2]\n",
            " [ 6 10 12 ... 12  7 12]\n",
            " [ 6  9  9 ...  8 12 10]]\n",
            "finding activity nr 9\n",
            "[[ 6  3  2 ...  4  1 13]\n",
            " [ 6  4 10 ...  4  8  5]\n",
            " [ 6  2  2 ...  2 10 11]\n",
            " ...\n",
            " [ 6 13 10 ... 12  2  4]\n",
            " [ 6 10 12 ...  7 12  5]\n",
            " [ 6  9  9 ... 12 10  7]]\n",
            "finding activity nr 10\n",
            "[[ 6  3  2 ...  1 13 13]\n",
            " [ 6  4 10 ...  8  5  1]\n",
            " [ 6  2  2 ... 10 11  1]\n",
            " ...\n",
            " [ 6 13 10 ...  2  4  4]\n",
            " [ 6 10 12 ... 12  5  9]\n",
            " [ 6  9  9 ... 10  7  2]]\n",
            "finding activity nr 11\n",
            "[[ 6  3  2 ... 13 13 11]\n",
            " [ 6  4 10 ...  5  1 13]\n",
            " [ 6  2  2 ... 11  1  3]\n",
            " ...\n",
            " [ 6 13 10 ...  4  4  1]\n",
            " [ 6 10 12 ...  5  9 11]\n",
            " [ 6  9  9 ...  7  2  1]]\n",
            "finding activity nr 12\n",
            "[[ 6  3  2 ... 13 11  9]\n",
            " [ 6  4 10 ...  1 13  3]\n",
            " [ 6  2  2 ...  1  3  9]\n",
            " ...\n",
            " [ 6 13 10 ...  4  1  8]\n",
            " [ 6 10 12 ...  9 11 11]\n",
            " [ 6  9  9 ...  2  1 10]]\n",
            "finding activity nr 13\n",
            "[[ 6  3  2 ... 11  9  2]\n",
            " [ 6  4 10 ... 13  3  9]\n",
            " [ 6  2  2 ...  3  9  3]\n",
            " ...\n",
            " [ 6 13 10 ...  1  8  3]\n",
            " [ 6 10 12 ... 11 11  9]\n",
            " [ 6  9  9 ...  1 10  1]]\n",
            "[[ 6  3  2 ...  9  2  5]\n",
            " [ 6  4 10 ...  3  9  7]\n",
            " [ 6  2  2 ...  9  3  5]\n",
            " ...\n",
            " [ 6 13 10 ...  8  3  1]\n",
            " [ 6 10 12 ... 11  9 10]\n",
            " [ 6  9  9 ... 10  1  2]]\n",
            "(12000, 13)\n",
            "Already exists: 2 32 0.0 0.0\n",
            "Already exists: 2 32 0.0 1e-05\n",
            "Already exists: 2 32 0.0 0.0001\n",
            "Already exists: 2 32 0.0 0.001\n",
            "Already exists: 2 32 0.0 0.01\n",
            "Already exists: 2 32 0.2 0.0\n",
            "Already exists: 2 32 0.2 1e-05\n",
            "Already exists: 2 32 0.2 0.0001\n",
            "Already exists: 2 32 0.2 0.001\n",
            "Already exists: 2 32 0.2 0.01\n",
            "Already exists: 2 32 0.4 0.0\n",
            "Already exists: 2 32 0.4 1e-05\n",
            "Already exists: 2 32 0.4 0.0001\n",
            "Already exists: 2 32 0.4 0.001\n",
            "Already exists: 2 32 0.4 0.01\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_43 (LSTM)              (None, 10, 32)            5888      \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 10, 32)            0         \n",
            "                                                                 \n",
            " lstm_44 (LSTM)              (None, 32)                8320      \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 13)                429       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,637\n",
            "Trainable params: 14,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 20s - loss: 0.8160 - accuracy: 0.6349 - val_loss: 0.6177 - val_accuracy: 0.6998 - lr: 0.0050 - 20s/epoch - 22ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 15s - loss: 0.6214 - accuracy: 0.7105 - val_loss: 0.5445 - val_accuracy: 0.7352 - lr: 0.0050 - 15s/epoch - 16ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 15s - loss: 0.5687 - accuracy: 0.7304 - val_loss: 0.4781 - val_accuracy: 0.7711 - lr: 0.0050 - 15s/epoch - 16ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 15s - loss: 0.5313 - accuracy: 0.7474 - val_loss: 0.4475 - val_accuracy: 0.7731 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 15s - loss: 0.4945 - accuracy: 0.7584 - val_loss: 0.4197 - val_accuracy: 0.7762 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 15s - loss: 0.4721 - accuracy: 0.7624 - val_loss: 0.4081 - val_accuracy: 0.7727 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 15s - loss: 0.4587 - accuracy: 0.7636 - val_loss: 0.4023 - val_accuracy: 0.7737 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 15s - loss: 0.4518 - accuracy: 0.7638 - val_loss: 0.4006 - val_accuracy: 0.7729 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 15s - loss: 0.4470 - accuracy: 0.7639 - val_loss: 0.4020 - val_accuracy: 0.7756 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 15s - loss: 0.4428 - accuracy: 0.7647 - val_loss: 0.4011 - val_accuracy: 0.7741 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 15s - loss: 0.4417 - accuracy: 0.7651 - val_loss: 0.4010 - val_accuracy: 0.7748 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 15s - loss: 0.4371 - accuracy: 0.7654 - val_loss: 0.4004 - val_accuracy: 0.7752 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 15s - loss: 0.4363 - accuracy: 0.7667 - val_loss: 0.3998 - val_accuracy: 0.7744 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 15s - loss: 0.4354 - accuracy: 0.7676 - val_loss: 0.4008 - val_accuracy: 0.7721 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 15s - loss: 0.4339 - accuracy: 0.7669 - val_loss: 0.4002 - val_accuracy: 0.7733 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 15s - loss: 0.4350 - accuracy: 0.7672 - val_loss: 0.3998 - val_accuracy: 0.7726 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 15s - loss: 0.4333 - accuracy: 0.7668 - val_loss: 0.3996 - val_accuracy: 0.7728 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 15s - loss: 0.4339 - accuracy: 0.7669 - val_loss: 0.3998 - val_accuracy: 0.7710 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 15s - loss: 0.4318 - accuracy: 0.7680 - val_loss: 0.3994 - val_accuracy: 0.7747 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 15s - loss: 0.4321 - accuracy: 0.7673 - val_loss: 0.3996 - val_accuracy: 0.7747 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 15s - loss: 0.4319 - accuracy: 0.7690 - val_loss: 0.3993 - val_accuracy: 0.7746 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 15s - loss: 0.4322 - accuracy: 0.7690 - val_loss: 0.3995 - val_accuracy: 0.7763 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 15s - loss: 0.4310 - accuracy: 0.7689 - val_loss: 0.3992 - val_accuracy: 0.7746 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 15s - loss: 0.4311 - accuracy: 0.7689 - val_loss: 0.3996 - val_accuracy: 0.7750 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 15s - loss: 0.4312 - accuracy: 0.7681 - val_loss: 0.3992 - val_accuracy: 0.7746 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 15s - loss: 0.4301 - accuracy: 0.7700 - val_loss: 0.3991 - val_accuracy: 0.7724 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 15s - loss: 0.4301 - accuracy: 0.7689 - val_loss: 0.3994 - val_accuracy: 0.7737 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 15s - loss: 0.4296 - accuracy: 0.7685 - val_loss: 0.3994 - val_accuracy: 0.7730 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 15s - loss: 0.4295 - accuracy: 0.7682 - val_loss: 0.3991 - val_accuracy: 0.7746 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 15s - loss: 0.4296 - accuracy: 0.7681 - val_loss: 0.3991 - val_accuracy: 0.7737 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 15s - loss: 0.4291 - accuracy: 0.7683 - val_loss: 0.3991 - val_accuracy: 0.7744 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 15s - loss: 0.4292 - accuracy: 0.7682 - val_loss: 0.3991 - val_accuracy: 0.7724 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 15s - loss: 0.4283 - accuracy: 0.7685 - val_loss: 0.3990 - val_accuracy: 0.7736 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 15s - loss: 0.4285 - accuracy: 0.7679 - val_loss: 0.3991 - val_accuracy: 0.7730 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 15s - loss: 0.4279 - accuracy: 0.7701 - val_loss: 0.3991 - val_accuracy: 0.7729 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 15s - loss: 0.4281 - accuracy: 0.7685 - val_loss: 0.3991 - val_accuracy: 0.7726 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 15s - loss: 0.4285 - accuracy: 0.7684 - val_loss: 0.3991 - val_accuracy: 0.7730 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 15s - loss: 0.4286 - accuracy: 0.7685 - val_loss: 0.3991 - val_accuracy: 0.7731 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 15s - loss: 0.4282 - accuracy: 0.7692 - val_loss: 0.3991 - val_accuracy: 0.7731 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_43 (LSTM)              (None, 10, 32)            5888      \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 10, 32)            0         \n",
            "                                                                 \n",
            " lstm_44 (LSTM)              (None, 32)                8320      \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 13)                429       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,637\n",
            "Trainable params: 14,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13 11]\n",
            " [ 6  7  8 13  1]\n",
            " [ 6  7  8 13  2]\n",
            " ...\n",
            " [ 6  7  8 13  1]\n",
            " [ 6  7  8 13  2]\n",
            " [ 6  7  8 13  2]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13 11  1]\n",
            " [ 6  7  8 13  1 11]\n",
            " [ 6  7  8 13  2  4]\n",
            " ...\n",
            " [ 6  7  8 13  1 11]\n",
            " [ 6  7  8 13  2 11]\n",
            " [ 6  7  8 13  2  4]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ... 11  1  3]\n",
            " [ 6  7  8 ...  1 11  2]\n",
            " [ 6  7  8 ...  2  4  3]\n",
            " ...\n",
            " [ 6  7  8 ...  1 11  4]\n",
            " [ 6  7  8 ...  2 11  1]\n",
            " [ 6  7  8 ...  2  4  3]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  1  3  4]\n",
            " [ 6  7  8 ... 11  2  3]\n",
            " [ 6  7  8 ...  4  3 11]\n",
            " ...\n",
            " [ 6  7  8 ... 11  4  3]\n",
            " [ 6  7  8 ... 11  1  3]\n",
            " [ 6  7  8 ...  4  3  1]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  3  4  2]\n",
            " [ 6  7  8 ...  2  3  4]\n",
            " [ 6  7  8 ...  3 11  1]\n",
            " ...\n",
            " [ 6  7  8 ...  4  3  2]\n",
            " [ 6  7  8 ...  1  3  4]\n",
            " [ 6  7  8 ...  3  1 11]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  4  2 10]\n",
            " [ 6  7  8 ...  3  4 10]\n",
            " [ 6  7  8 ... 11  1 10]\n",
            " ...\n",
            " [ 6  7  8 ...  3  2 10]\n",
            " [ 6  7  8 ...  3  4 10]\n",
            " [ 6  7  8 ...  1 11 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  1 10  5]\n",
            " ...\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ... 11 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_45 (LSTM)              (None, 10, 32)            5888      \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 10, 32)            0         \n",
            "                                                                 \n",
            " lstm_46 (LSTM)              (None, 32)                8320      \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 13)                429       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,637\n",
            "Trainable params: 14,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 20s - loss: 0.8330 - accuracy: 0.6320 - val_loss: 0.6910 - val_accuracy: 0.6717 - lr: 0.0050 - 20s/epoch - 23ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 15s - loss: 0.6768 - accuracy: 0.6915 - val_loss: 0.5707 - val_accuracy: 0.7451 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 15s - loss: 0.5974 - accuracy: 0.7343 - val_loss: 0.5127 - val_accuracy: 0.7579 - lr: 0.0050 - 15s/epoch - 16ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 15s - loss: 0.5514 - accuracy: 0.7485 - val_loss: 0.4700 - val_accuracy: 0.7762 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 15s - loss: 0.5246 - accuracy: 0.7581 - val_loss: 0.4523 - val_accuracy: 0.7726 - lr: 0.0050 - 15s/epoch - 16ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 15s - loss: 0.5057 - accuracy: 0.7591 - val_loss: 0.4442 - val_accuracy: 0.7730 - lr: 0.0050 - 15s/epoch - 16ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 15s - loss: 0.4994 - accuracy: 0.7613 - val_loss: 0.4377 - val_accuracy: 0.7736 - lr: 0.0050 - 15s/epoch - 16ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 15s - loss: 0.4892 - accuracy: 0.7646 - val_loss: 0.4324 - val_accuracy: 0.7716 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 15s - loss: 0.4867 - accuracy: 0.7618 - val_loss: 0.4367 - val_accuracy: 0.7737 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 15s - loss: 0.4833 - accuracy: 0.7631 - val_loss: 0.4327 - val_accuracy: 0.7730 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 15s - loss: 0.4816 - accuracy: 0.7646 - val_loss: 0.4318 - val_accuracy: 0.7734 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 15s - loss: 0.4970 - accuracy: 0.7631 - val_loss: 0.4413 - val_accuracy: 0.7719 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 15s - loss: 0.4830 - accuracy: 0.7649 - val_loss: 0.4364 - val_accuracy: 0.7749 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 15s - loss: 0.4781 - accuracy: 0.7648 - val_loss: 0.4344 - val_accuracy: 0.7712 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 15s - loss: 0.4708 - accuracy: 0.7650 - val_loss: 0.4312 - val_accuracy: 0.7721 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 15s - loss: 0.4689 - accuracy: 0.7654 - val_loss: 0.4304 - val_accuracy: 0.7723 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 15s - loss: 0.4666 - accuracy: 0.7660 - val_loss: 0.4282 - val_accuracy: 0.7738 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 16s - loss: 0.4647 - accuracy: 0.7666 - val_loss: 0.4273 - val_accuracy: 0.7728 - lr: 0.0025 - 16s/epoch - 17ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 15s - loss: 0.4631 - accuracy: 0.7675 - val_loss: 0.4256 - val_accuracy: 0.7738 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 15s - loss: 0.4644 - accuracy: 0.7673 - val_loss: 0.4271 - val_accuracy: 0.7752 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 15s - loss: 0.4630 - accuracy: 0.7672 - val_loss: 0.4256 - val_accuracy: 0.7735 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 15s - loss: 0.4608 - accuracy: 0.7664 - val_loss: 0.4247 - val_accuracy: 0.7726 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 15s - loss: 0.4610 - accuracy: 0.7663 - val_loss: 0.4256 - val_accuracy: 0.7715 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 15s - loss: 0.4603 - accuracy: 0.7666 - val_loss: 0.4243 - val_accuracy: 0.7741 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 15s - loss: 0.4577 - accuracy: 0.7686 - val_loss: 0.4232 - val_accuracy: 0.7714 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 15s - loss: 0.4574 - accuracy: 0.7679 - val_loss: 0.4245 - val_accuracy: 0.7751 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 15s - loss: 0.4584 - accuracy: 0.7672 - val_loss: 0.4227 - val_accuracy: 0.7752 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 15s - loss: 0.4579 - accuracy: 0.7687 - val_loss: 0.4242 - val_accuracy: 0.7735 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 15s - loss: 0.4569 - accuracy: 0.7673 - val_loss: 0.4229 - val_accuracy: 0.7748 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 16s - loss: 0.4568 - accuracy: 0.7687 - val_loss: 0.4226 - val_accuracy: 0.7737 - lr: 0.0025 - 16s/epoch - 17ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 15s - loss: 0.4531 - accuracy: 0.7694 - val_loss: 0.4213 - val_accuracy: 0.7715 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 17s - loss: 0.4523 - accuracy: 0.7672 - val_loss: 0.4209 - val_accuracy: 0.7724 - lr: 0.0012 - 17s/epoch - 19ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 15s - loss: 0.4528 - accuracy: 0.7682 - val_loss: 0.4205 - val_accuracy: 0.7726 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 15s - loss: 0.4511 - accuracy: 0.7686 - val_loss: 0.4201 - val_accuracy: 0.7750 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 15s - loss: 0.4513 - accuracy: 0.7685 - val_loss: 0.4196 - val_accuracy: 0.7726 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 15s - loss: 0.4508 - accuracy: 0.7683 - val_loss: 0.4196 - val_accuracy: 0.7738 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 16s - loss: 0.4497 - accuracy: 0.7691 - val_loss: 0.4192 - val_accuracy: 0.7746 - lr: 0.0012 - 16s/epoch - 17ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 15s - loss: 0.4488 - accuracy: 0.7694 - val_loss: 0.4194 - val_accuracy: 0.7755 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 16s - loss: 0.4506 - accuracy: 0.7696 - val_loss: 0.4196 - val_accuracy: 0.7754 - lr: 0.0012 - 16s/epoch - 18ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 15s - loss: 0.4491 - accuracy: 0.7691 - val_loss: 0.4190 - val_accuracy: 0.7745 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 15s - loss: 0.4492 - accuracy: 0.7684 - val_loss: 0.4188 - val_accuracy: 0.7737 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 15s - loss: 0.4496 - accuracy: 0.7697 - val_loss: 0.4193 - val_accuracy: 0.7756 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 15s - loss: 0.4491 - accuracy: 0.7689 - val_loss: 0.4185 - val_accuracy: 0.7758 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 15s - loss: 0.4473 - accuracy: 0.7706 - val_loss: 0.4187 - val_accuracy: 0.7747 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 15s - loss: 0.4472 - accuracy: 0.7712 - val_loss: 0.4184 - val_accuracy: 0.7740 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 15s - loss: 0.4477 - accuracy: 0.7684 - val_loss: 0.4178 - val_accuracy: 0.7742 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 15s - loss: 0.4476 - accuracy: 0.7693 - val_loss: 0.4186 - val_accuracy: 0.7730 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 15s - loss: 0.4469 - accuracy: 0.7695 - val_loss: 0.4182 - val_accuracy: 0.7745 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 15s - loss: 0.4472 - accuracy: 0.7712 - val_loss: 0.4182 - val_accuracy: 0.7730 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 15s - loss: 0.4459 - accuracy: 0.7698 - val_loss: 0.4178 - val_accuracy: 0.7737 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 15s - loss: 0.4450 - accuracy: 0.7712 - val_loss: 0.4176 - val_accuracy: 0.7737 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 15s - loss: 0.4452 - accuracy: 0.7696 - val_loss: 0.4172 - val_accuracy: 0.7752 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 15s - loss: 0.4449 - accuracy: 0.7699 - val_loss: 0.4169 - val_accuracy: 0.7742 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 15s - loss: 0.4445 - accuracy: 0.7713 - val_loss: 0.4170 - val_accuracy: 0.7725 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 15s - loss: 0.4451 - accuracy: 0.7689 - val_loss: 0.4166 - val_accuracy: 0.7750 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 15s - loss: 0.4454 - accuracy: 0.7704 - val_loss: 0.4168 - val_accuracy: 0.7745 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 15s - loss: 0.4446 - accuracy: 0.7703 - val_loss: 0.4165 - val_accuracy: 0.7746 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 15s - loss: 0.4449 - accuracy: 0.7686 - val_loss: 0.4168 - val_accuracy: 0.7731 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 15s - loss: 0.4442 - accuracy: 0.7710 - val_loss: 0.4165 - val_accuracy: 0.7746 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 15s - loss: 0.4453 - accuracy: 0.7700 - val_loss: 0.4166 - val_accuracy: 0.7731 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 15s - loss: 0.4441 - accuracy: 0.7693 - val_loss: 0.4162 - val_accuracy: 0.7726 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 15s - loss: 0.4435 - accuracy: 0.7712 - val_loss: 0.4161 - val_accuracy: 0.7747 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 15s - loss: 0.4443 - accuracy: 0.7708 - val_loss: 0.4160 - val_accuracy: 0.7723 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 15s - loss: 0.4435 - accuracy: 0.7704 - val_loss: 0.4158 - val_accuracy: 0.7741 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 15s - loss: 0.4426 - accuracy: 0.7703 - val_loss: 0.4160 - val_accuracy: 0.7715 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 15s - loss: 0.4432 - accuracy: 0.7711 - val_loss: 0.4158 - val_accuracy: 0.7741 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 15s - loss: 0.4435 - accuracy: 0.7698 - val_loss: 0.4158 - val_accuracy: 0.7725 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 15s - loss: 0.4434 - accuracy: 0.7703 - val_loss: 0.4157 - val_accuracy: 0.7747 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 15s - loss: 0.4426 - accuracy: 0.7711 - val_loss: 0.4156 - val_accuracy: 0.7752 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 70/600\n",
            "892/892 - 15s - loss: 0.4421 - accuracy: 0.7731 - val_loss: 0.4155 - val_accuracy: 0.7735 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 71/600\n",
            "892/892 - 15s - loss: 0.4423 - accuracy: 0.7711 - val_loss: 0.4154 - val_accuracy: 0.7739 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 72/600\n",
            "892/892 - 15s - loss: 0.4422 - accuracy: 0.7719 - val_loss: 0.4154 - val_accuracy: 0.7730 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 73/600\n",
            "892/892 - 15s - loss: 0.4428 - accuracy: 0.7711 - val_loss: 0.4154 - val_accuracy: 0.7734 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 74/600\n",
            "892/892 - 15s - loss: 0.4430 - accuracy: 0.7697 - val_loss: 0.4154 - val_accuracy: 0.7749 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 75/600\n",
            "892/892 - 15s - loss: 0.4430 - accuracy: 0.7700 - val_loss: 0.4153 - val_accuracy: 0.7759 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 76/600\n",
            "892/892 - 15s - loss: 0.4424 - accuracy: 0.7704 - val_loss: 0.4153 - val_accuracy: 0.7746 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 77/600\n",
            "892/892 - 15s - loss: 0.4423 - accuracy: 0.7713 - val_loss: 0.4153 - val_accuracy: 0.7725 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 78/600\n",
            "892/892 - 15s - loss: 0.4416 - accuracy: 0.7728 - val_loss: 0.4153 - val_accuracy: 0.7740 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 79/600\n",
            "892/892 - 15s - loss: 0.4420 - accuracy: 0.7712 - val_loss: 0.4152 - val_accuracy: 0.7742 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 80/600\n",
            "892/892 - 15s - loss: 0.4424 - accuracy: 0.7703 - val_loss: 0.4152 - val_accuracy: 0.7743 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 81/600\n",
            "892/892 - 16s - loss: 0.4417 - accuracy: 0.7713 - val_loss: 0.4152 - val_accuracy: 0.7745 - lr: 3.9062e-05 - 16s/epoch - 18ms/step\n",
            "Epoch 82/600\n",
            "892/892 - 15s - loss: 0.4422 - accuracy: 0.7695 - val_loss: 0.4152 - val_accuracy: 0.7743 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 83/600\n",
            "892/892 - 15s - loss: 0.4417 - accuracy: 0.7715 - val_loss: 0.4152 - val_accuracy: 0.7743 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 84/600\n",
            "892/892 - 15s - loss: 0.4419 - accuracy: 0.7714 - val_loss: 0.4152 - val_accuracy: 0.7744 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 85/600\n",
            "892/892 - 15s - loss: 0.4423 - accuracy: 0.7720 - val_loss: 0.4152 - val_accuracy: 0.7743 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 86/600\n",
            "892/892 - 15s - loss: 0.4421 - accuracy: 0.7710 - val_loss: 0.4152 - val_accuracy: 0.7742 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 87/600\n",
            "892/892 - 15s - loss: 0.4421 - accuracy: 0.7702 - val_loss: 0.4151 - val_accuracy: 0.7742 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 88/600\n",
            "892/892 - 15s - loss: 0.4417 - accuracy: 0.7703 - val_loss: 0.4151 - val_accuracy: 0.7742 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 89/600\n",
            "892/892 - 15s - loss: 0.4414 - accuracy: 0.7708 - val_loss: 0.4151 - val_accuracy: 0.7741 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 90/600\n",
            "892/892 - 15s - loss: 0.4425 - accuracy: 0.7701 - val_loss: 0.4151 - val_accuracy: 0.7741 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 91/600\n",
            "892/892 - 15s - loss: 0.4416 - accuracy: 0.7716 - val_loss: 0.4151 - val_accuracy: 0.7742 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 92/600\n",
            "892/892 - 15s - loss: 0.4422 - accuracy: 0.7713 - val_loss: 0.4151 - val_accuracy: 0.7741 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 93/600\n",
            "892/892 - 15s - loss: 0.4422 - accuracy: 0.7713 - val_loss: 0.4151 - val_accuracy: 0.7741 - lr: 2.4414e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 94/600\n",
            "892/892 - 15s - loss: 0.4428 - accuracy: 0.7699 - val_loss: 0.4151 - val_accuracy: 0.7741 - lr: 2.4414e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 95/600\n",
            "892/892 - 15s - loss: 0.4424 - accuracy: 0.7707 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 2.4414e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 96/600\n",
            "892/892 - 15s - loss: 0.4422 - accuracy: 0.7700 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 1.2207e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 97/600\n",
            "892/892 - 15s - loss: 0.4419 - accuracy: 0.7711 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 1.2207e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 98/600\n",
            "892/892 - 15s - loss: 0.4417 - accuracy: 0.7707 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 1.2207e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 99/600\n",
            "892/892 - 15s - loss: 0.4414 - accuracy: 0.7712 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 6.1035e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 100/600\n",
            "892/892 - 15s - loss: 0.4423 - accuracy: 0.7712 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 6.1035e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 101/600\n",
            "892/892 - 15s - loss: 0.4408 - accuracy: 0.7715 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 6.1035e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 102/600\n",
            "892/892 - 15s - loss: 0.4416 - accuracy: 0.7712 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 3.0518e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 103/600\n",
            "892/892 - 15s - loss: 0.4413 - accuracy: 0.7714 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 3.0518e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 104/600\n",
            "892/892 - 15s - loss: 0.4419 - accuracy: 0.7702 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 3.0518e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 105/600\n",
            "892/892 - 15s - loss: 0.4423 - accuracy: 0.7710 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 1.5259e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 106/600\n",
            "892/892 - 15s - loss: 0.4419 - accuracy: 0.7715 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 1.5259e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 107/600\n",
            "892/892 - 15s - loss: 0.4422 - accuracy: 0.7718 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 1.5259e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 108/600\n",
            "892/892 - 15s - loss: 0.4424 - accuracy: 0.7697 - val_loss: 0.4151 - val_accuracy: 0.7750 - lr: 7.6294e-08 - 15s/epoch - 17ms/step\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_45 (LSTM)              (None, 10, 32)            5888      \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 10, 32)            0         \n",
            "                                                                 \n",
            " lstm_46 (LSTM)              (None, 32)                8320      \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 13)                429       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,637\n",
            "Trainable params: 14,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13  4]\n",
            " [ 6  7  8 13  4]\n",
            " [ 6  7  8 13  2]\n",
            " ...\n",
            " [ 6  7  8 13  1]\n",
            " [ 6  7  8 13  1]\n",
            " [ 6  7  8 13  4]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13  4  1]\n",
            " [ 6  7  8 13  4  2]\n",
            " [ 6  7  8 13  2  1]\n",
            " ...\n",
            " [ 6  7  8 13  1  2]\n",
            " [ 6  7  8 13  1  4]\n",
            " [ 6  7  8 13  4  3]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ...  4  1  3]\n",
            " [ 6  7  8 ...  4  2  1]\n",
            " [ 6  7  8 ...  2  1 11]\n",
            " ...\n",
            " [ 6  7  8 ...  1  2 11]\n",
            " [ 6  7  8 ...  1  4  3]\n",
            " [ 6  7  8 ...  4  3  2]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  1  3 11]\n",
            " [ 6  7  8 ...  2  1 11]\n",
            " [ 6  7  8 ...  1 11  3]\n",
            " ...\n",
            " [ 6  7  8 ...  2 11  3]\n",
            " [ 6  7  8 ...  4  3 11]\n",
            " [ 6  7  8 ...  3  2 11]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  3 11  2]\n",
            " [ 6  7  8 ...  1 11  3]\n",
            " [ 6  7  8 ... 11  3  4]\n",
            " ...\n",
            " [ 6  7  8 ... 11  3  4]\n",
            " [ 6  7  8 ...  3 11  2]\n",
            " [ 6  7  8 ...  2 11  1]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ... 11  2 10]\n",
            " [ 6  7  8 ... 11  3 10]\n",
            " [ 6  7  8 ...  3  4 10]\n",
            " ...\n",
            " [ 6  7  8 ...  3  4 10]\n",
            " [ 6  7  8 ... 11  2 10]\n",
            " [ 6  7  8 ... 11  1 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ...  3 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " ...\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ...  1 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_47 (LSTM)              (None, 10, 32)            5888      \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 10, 32)            0         \n",
            "                                                                 \n",
            " lstm_48 (LSTM)              (None, 32)                8320      \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 13)                429       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,637\n",
            "Trainable params: 14,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 21s - loss: 0.9715 - accuracy: 0.6086 - val_loss: 0.7642 - val_accuracy: 0.6667 - lr: 0.0050 - 21s/epoch - 23ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 15s - loss: 0.7650 - accuracy: 0.6797 - val_loss: 0.6828 - val_accuracy: 0.7196 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 15s - loss: 0.7124 - accuracy: 0.7016 - val_loss: 0.6431 - val_accuracy: 0.7278 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 15s - loss: 0.6876 - accuracy: 0.7158 - val_loss: 0.6200 - val_accuracy: 0.7333 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 15s - loss: 0.6542 - accuracy: 0.7267 - val_loss: 0.5813 - val_accuracy: 0.7730 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 15s - loss: 0.6369 - accuracy: 0.7404 - val_loss: 0.5540 - val_accuracy: 0.7736 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 15s - loss: 0.6325 - accuracy: 0.7455 - val_loss: 0.5422 - val_accuracy: 0.7733 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 15s - loss: 0.6156 - accuracy: 0.7498 - val_loss: 0.5559 - val_accuracy: 0.7722 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 15s - loss: 0.5990 - accuracy: 0.7534 - val_loss: 0.5162 - val_accuracy: 0.7755 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 15s - loss: 0.5881 - accuracy: 0.7545 - val_loss: 0.5032 - val_accuracy: 0.7702 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 15s - loss: 0.5916 - accuracy: 0.7568 - val_loss: 0.4977 - val_accuracy: 0.7724 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 15s - loss: 0.5740 - accuracy: 0.7578 - val_loss: 0.4985 - val_accuracy: 0.7734 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 15s - loss: 0.5889 - accuracy: 0.7559 - val_loss: 0.4880 - val_accuracy: 0.7738 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 15s - loss: 0.5715 - accuracy: 0.7579 - val_loss: 0.4898 - val_accuracy: 0.7743 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 15s - loss: 0.5694 - accuracy: 0.7596 - val_loss: 0.4986 - val_accuracy: 0.7743 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 15s - loss: 0.5758 - accuracy: 0.7592 - val_loss: 0.5256 - val_accuracy: 0.7758 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 15s - loss: 0.5703 - accuracy: 0.7627 - val_loss: 0.4891 - val_accuracy: 0.7735 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 15s - loss: 0.5503 - accuracy: 0.7611 - val_loss: 0.4817 - val_accuracy: 0.7746 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 15s - loss: 0.5473 - accuracy: 0.7616 - val_loss: 0.4778 - val_accuracy: 0.7758 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 15s - loss: 0.5440 - accuracy: 0.7614 - val_loss: 0.4760 - val_accuracy: 0.7751 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 15s - loss: 0.5422 - accuracy: 0.7611 - val_loss: 0.4774 - val_accuracy: 0.7742 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 15s - loss: 0.5426 - accuracy: 0.7608 - val_loss: 0.4773 - val_accuracy: 0.7735 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 15s - loss: 0.5373 - accuracy: 0.7631 - val_loss: 0.4730 - val_accuracy: 0.7738 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 15s - loss: 0.5400 - accuracy: 0.7622 - val_loss: 0.4775 - val_accuracy: 0.7764 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 15s - loss: 0.5408 - accuracy: 0.7614 - val_loss: 0.4768 - val_accuracy: 0.7766 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 15s - loss: 0.5338 - accuracy: 0.7629 - val_loss: 0.4726 - val_accuracy: 0.7719 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 15s - loss: 0.5347 - accuracy: 0.7630 - val_loss: 0.4735 - val_accuracy: 0.7777 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 15s - loss: 0.5344 - accuracy: 0.7613 - val_loss: 0.4726 - val_accuracy: 0.7722 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 15s - loss: 0.5313 - accuracy: 0.7650 - val_loss: 0.4753 - val_accuracy: 0.7733 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 15s - loss: 0.5273 - accuracy: 0.7637 - val_loss: 0.4667 - val_accuracy: 0.7743 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 15s - loss: 0.5252 - accuracy: 0.7657 - val_loss: 0.4660 - val_accuracy: 0.7750 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 15s - loss: 0.5233 - accuracy: 0.7632 - val_loss: 0.4651 - val_accuracy: 0.7732 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 15s - loss: 0.5217 - accuracy: 0.7646 - val_loss: 0.4649 - val_accuracy: 0.7739 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 16s - loss: 0.5226 - accuracy: 0.7644 - val_loss: 0.4651 - val_accuracy: 0.7731 - lr: 0.0012 - 16s/epoch - 17ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 15s - loss: 0.5231 - accuracy: 0.7635 - val_loss: 0.4637 - val_accuracy: 0.7762 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 15s - loss: 0.5210 - accuracy: 0.7635 - val_loss: 0.4646 - val_accuracy: 0.7745 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 15s - loss: 0.5237 - accuracy: 0.7642 - val_loss: 0.4653 - val_accuracy: 0.7749 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 15s - loss: 0.5205 - accuracy: 0.7653 - val_loss: 0.4627 - val_accuracy: 0.7747 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 15s - loss: 0.5188 - accuracy: 0.7644 - val_loss: 0.4624 - val_accuracy: 0.7765 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 15s - loss: 0.5213 - accuracy: 0.7644 - val_loss: 0.4648 - val_accuracy: 0.7733 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 15s - loss: 0.5195 - accuracy: 0.7650 - val_loss: 0.4623 - val_accuracy: 0.7751 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 15s - loss: 0.5184 - accuracy: 0.7657 - val_loss: 0.4612 - val_accuracy: 0.7766 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 15s - loss: 0.5218 - accuracy: 0.7631 - val_loss: 0.4636 - val_accuracy: 0.7725 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 15s - loss: 0.5208 - accuracy: 0.7639 - val_loss: 0.4626 - val_accuracy: 0.7739 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 15s - loss: 0.5187 - accuracy: 0.7658 - val_loss: 0.4623 - val_accuracy: 0.7750 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 15s - loss: 0.5157 - accuracy: 0.7646 - val_loss: 0.4601 - val_accuracy: 0.7758 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 15s - loss: 0.5139 - accuracy: 0.7655 - val_loss: 0.4598 - val_accuracy: 0.7766 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 15s - loss: 0.5139 - accuracy: 0.7652 - val_loss: 0.4592 - val_accuracy: 0.7744 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 15s - loss: 0.5134 - accuracy: 0.7662 - val_loss: 0.4584 - val_accuracy: 0.7762 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 15s - loss: 0.5129 - accuracy: 0.7664 - val_loss: 0.4588 - val_accuracy: 0.7714 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 15s - loss: 0.5126 - accuracy: 0.7659 - val_loss: 0.4579 - val_accuracy: 0.7744 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 15s - loss: 0.5124 - accuracy: 0.7650 - val_loss: 0.4578 - val_accuracy: 0.7731 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 15s - loss: 0.5127 - accuracy: 0.7648 - val_loss: 0.4579 - val_accuracy: 0.7759 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 15s - loss: 0.5114 - accuracy: 0.7656 - val_loss: 0.4578 - val_accuracy: 0.7752 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 15s - loss: 0.5098 - accuracy: 0.7666 - val_loss: 0.4583 - val_accuracy: 0.7727 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 15s - loss: 0.5085 - accuracy: 0.7662 - val_loss: 0.4564 - val_accuracy: 0.7715 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 15s - loss: 0.5091 - accuracy: 0.7663 - val_loss: 0.4556 - val_accuracy: 0.7754 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 15s - loss: 0.5092 - accuracy: 0.7659 - val_loss: 0.4562 - val_accuracy: 0.7714 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 15s - loss: 0.5079 - accuracy: 0.7669 - val_loss: 0.4555 - val_accuracy: 0.7746 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 15s - loss: 0.5088 - accuracy: 0.7656 - val_loss: 0.4556 - val_accuracy: 0.7722 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 15s - loss: 0.5087 - accuracy: 0.7653 - val_loss: 0.4552 - val_accuracy: 0.7736 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 15s - loss: 0.5080 - accuracy: 0.7662 - val_loss: 0.4552 - val_accuracy: 0.7738 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 15s - loss: 0.5089 - accuracy: 0.7670 - val_loss: 0.4552 - val_accuracy: 0.7718 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 15s - loss: 0.5085 - accuracy: 0.7656 - val_loss: 0.4547 - val_accuracy: 0.7760 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 15s - loss: 0.5087 - accuracy: 0.7657 - val_loss: 0.4560 - val_accuracy: 0.7734 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 15s - loss: 0.5073 - accuracy: 0.7671 - val_loss: 0.4547 - val_accuracy: 0.7735 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 15s - loss: 0.5059 - accuracy: 0.7674 - val_loss: 0.4552 - val_accuracy: 0.7737 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 15s - loss: 0.5073 - accuracy: 0.7658 - val_loss: 0.4539 - val_accuracy: 0.7743 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 15s - loss: 0.5074 - accuracy: 0.7656 - val_loss: 0.4540 - val_accuracy: 0.7717 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 70/600\n",
            "892/892 - 15s - loss: 0.5060 - accuracy: 0.7663 - val_loss: 0.4536 - val_accuracy: 0.7720 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 71/600\n",
            "892/892 - 15s - loss: 0.5055 - accuracy: 0.7665 - val_loss: 0.4539 - val_accuracy: 0.7729 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 72/600\n",
            "892/892 - 15s - loss: 0.5069 - accuracy: 0.7658 - val_loss: 0.4539 - val_accuracy: 0.7725 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 73/600\n",
            "892/892 - 15s - loss: 0.5068 - accuracy: 0.7658 - val_loss: 0.4538 - val_accuracy: 0.7746 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 74/600\n",
            "892/892 - 15s - loss: 0.5048 - accuracy: 0.7683 - val_loss: 0.4532 - val_accuracy: 0.7735 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 75/600\n",
            "892/892 - 15s - loss: 0.5063 - accuracy: 0.7659 - val_loss: 0.4531 - val_accuracy: 0.7751 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 76/600\n",
            "892/892 - 15s - loss: 0.5073 - accuracy: 0.7657 - val_loss: 0.4533 - val_accuracy: 0.7729 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 77/600\n",
            "892/892 - 15s - loss: 0.5059 - accuracy: 0.7653 - val_loss: 0.4533 - val_accuracy: 0.7726 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 78/600\n",
            "892/892 - 15s - loss: 0.5046 - accuracy: 0.7674 - val_loss: 0.4531 - val_accuracy: 0.7738 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 79/600\n",
            "892/892 - 15s - loss: 0.5046 - accuracy: 0.7665 - val_loss: 0.4531 - val_accuracy: 0.7735 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 80/600\n",
            "892/892 - 15s - loss: 0.5058 - accuracy: 0.7669 - val_loss: 0.4532 - val_accuracy: 0.7711 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 81/600\n",
            "892/892 - 15s - loss: 0.5062 - accuracy: 0.7670 - val_loss: 0.4529 - val_accuracy: 0.7733 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 82/600\n",
            "892/892 - 15s - loss: 0.5044 - accuracy: 0.7668 - val_loss: 0.4529 - val_accuracy: 0.7725 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 83/600\n",
            "892/892 - 15s - loss: 0.5051 - accuracy: 0.7657 - val_loss: 0.4528 - val_accuracy: 0.7735 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 84/600\n",
            "892/892 - 15s - loss: 0.5046 - accuracy: 0.7664 - val_loss: 0.4528 - val_accuracy: 0.7732 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 85/600\n",
            "892/892 - 15s - loss: 0.5048 - accuracy: 0.7664 - val_loss: 0.4527 - val_accuracy: 0.7748 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 86/600\n",
            "892/892 - 15s - loss: 0.5049 - accuracy: 0.7666 - val_loss: 0.4527 - val_accuracy: 0.7753 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 87/600\n",
            "892/892 - 15s - loss: 0.5044 - accuracy: 0.7664 - val_loss: 0.4528 - val_accuracy: 0.7730 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 88/600\n",
            "892/892 - 15s - loss: 0.5049 - accuracy: 0.7670 - val_loss: 0.4527 - val_accuracy: 0.7732 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 89/600\n",
            "892/892 - 15s - loss: 0.5056 - accuracy: 0.7660 - val_loss: 0.4528 - val_accuracy: 0.7736 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 90/600\n",
            "892/892 - 15s - loss: 0.5060 - accuracy: 0.7649 - val_loss: 0.4528 - val_accuracy: 0.7728 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 91/600\n",
            "892/892 - 15s - loss: 0.5047 - accuracy: 0.7667 - val_loss: 0.4527 - val_accuracy: 0.7725 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 92/600\n",
            "892/892 - 15s - loss: 0.5050 - accuracy: 0.7653 - val_loss: 0.4527 - val_accuracy: 0.7724 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 93/600\n",
            "892/892 - 15s - loss: 0.5047 - accuracy: 0.7663 - val_loss: 0.4528 - val_accuracy: 0.7724 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 94/600\n",
            "892/892 - 15s - loss: 0.5058 - accuracy: 0.7653 - val_loss: 0.4528 - val_accuracy: 0.7730 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 95/600\n",
            "892/892 - 15s - loss: 0.5056 - accuracy: 0.7662 - val_loss: 0.4527 - val_accuracy: 0.7728 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 96/600\n",
            "892/892 - 15s - loss: 0.5057 - accuracy: 0.7656 - val_loss: 0.4527 - val_accuracy: 0.7725 - lr: 2.4414e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 97/600\n",
            "892/892 - 15s - loss: 0.5070 - accuracy: 0.7653 - val_loss: 0.4527 - val_accuracy: 0.7725 - lr: 2.4414e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 98/600\n",
            "892/892 - 15s - loss: 0.5040 - accuracy: 0.7671 - val_loss: 0.4527 - val_accuracy: 0.7730 - lr: 2.4414e-06 - 15s/epoch - 17ms/step\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_47 (LSTM)              (None, 10, 32)            5888      \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 10, 32)            0         \n",
            "                                                                 \n",
            " lstm_48 (LSTM)              (None, 32)                8320      \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 13)                429       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,637\n",
            "Trainable params: 14,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13  3]\n",
            " [ 6  7  8 13  3]\n",
            " [ 6  7  8 13  3]\n",
            " ...\n",
            " [ 6  7  8 13  2]\n",
            " [ 6  7  8 13  3]\n",
            " [ 6  7  8 13  2]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13  3  2]\n",
            " [ 6  7  8 13  3  2]\n",
            " [ 6  7  8 13  3 11]\n",
            " ...\n",
            " [ 6  7  8 13  2 11]\n",
            " [ 6  7  8 13  3  2]\n",
            " [ 6  7  8 13  2  4]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ...  3  2  4]\n",
            " [ 6  7  8 ...  3  2 11]\n",
            " [ 6  7  8 ...  3 11  1]\n",
            " ...\n",
            " [ 6  7  8 ...  2 11  1]\n",
            " [ 6  7  8 ...  3  2  1]\n",
            " [ 6  7  8 ...  2  4  3]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  2  4  1]\n",
            " [ 6  7  8 ...  2 11  1]\n",
            " [ 6  7  8 ... 11  1  2]\n",
            " ...\n",
            " [ 6  7  8 ... 11  1  4]\n",
            " [ 6  7  8 ...  2  1  4]\n",
            " [ 6  7  8 ...  4  3  1]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  4  1 11]\n",
            " [ 6  7  8 ... 11  1  4]\n",
            " [ 6  7  8 ...  1  2  4]\n",
            " ...\n",
            " [ 6  7  8 ...  1  4  3]\n",
            " [ 6  7  8 ...  1  4 11]\n",
            " [ 6  7  8 ...  3  1 11]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  1 11 10]\n",
            " [ 6  7  8 ...  1  4 10]\n",
            " [ 6  7  8 ...  2  4 10]\n",
            " ...\n",
            " [ 6  7  8 ...  4  3 10]\n",
            " [ 6  7  8 ...  4 11 10]\n",
            " [ 6  7  8 ...  1 11 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ... 11 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " ...\n",
            " [ 6  7  8 ...  3 10  5]\n",
            " [ 6  7  8 ... 11 10  5]\n",
            " [ 6  7  8 ... 11 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_49 (LSTM)              (None, 10, 32)            5888      \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 10, 32)            0         \n",
            "                                                                 \n",
            " lstm_50 (LSTM)              (None, 32)                8320      \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 13)                429       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,637\n",
            "Trainable params: 14,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 21s - loss: 1.2186 - accuracy: 0.5613 - val_loss: 0.8320 - val_accuracy: 0.6651 - lr: 0.0050 - 21s/epoch - 24ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 15s - loss: 0.9205 - accuracy: 0.6368 - val_loss: 0.8056 - val_accuracy: 0.6667 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 15s - loss: 0.8759 - accuracy: 0.6439 - val_loss: 0.7747 - val_accuracy: 0.6667 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 15s - loss: 0.8550 - accuracy: 0.6493 - val_loss: 0.7762 - val_accuracy: 0.6667 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 15s - loss: 0.8334 - accuracy: 0.6511 - val_loss: 0.7669 - val_accuracy: 0.6683 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 15s - loss: 0.8212 - accuracy: 0.6525 - val_loss: 0.7604 - val_accuracy: 0.6667 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 15s - loss: 0.8287 - accuracy: 0.6529 - val_loss: 0.7579 - val_accuracy: 0.6667 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 15s - loss: 0.8160 - accuracy: 0.6536 - val_loss: 0.7616 - val_accuracy: 0.6667 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 15s - loss: 0.8359 - accuracy: 0.6504 - val_loss: 0.7666 - val_accuracy: 0.6711 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 15s - loss: 0.8010 - accuracy: 0.6562 - val_loss: 0.7522 - val_accuracy: 0.6667 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 15s - loss: 0.8233 - accuracy: 0.6535 - val_loss: 0.7495 - val_accuracy: 0.6674 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 15s - loss: 0.7976 - accuracy: 0.6565 - val_loss: 0.7476 - val_accuracy: 0.6673 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 15s - loss: 0.7996 - accuracy: 0.6560 - val_loss: 0.7596 - val_accuracy: 0.6667 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 15s - loss: 0.8202 - accuracy: 0.6532 - val_loss: 0.7487 - val_accuracy: 0.6667 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 15s - loss: 0.7948 - accuracy: 0.6565 - val_loss: 0.7487 - val_accuracy: 0.6667 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 15s - loss: 0.7681 - accuracy: 0.6619 - val_loss: 0.7326 - val_accuracy: 0.6834 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 15s - loss: 0.7707 - accuracy: 0.6613 - val_loss: 0.7325 - val_accuracy: 0.6983 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 15s - loss: 0.7675 - accuracy: 0.6730 - val_loss: 0.7071 - val_accuracy: 0.7114 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 16s - loss: 0.7505 - accuracy: 0.6886 - val_loss: 0.7017 - val_accuracy: 0.7094 - lr: 0.0025 - 16s/epoch - 18ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 15s - loss: 0.7468 - accuracy: 0.6898 - val_loss: 0.6947 - val_accuracy: 0.7085 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 16s - loss: 0.7358 - accuracy: 0.6918 - val_loss: 0.6954 - val_accuracy: 0.7040 - lr: 0.0025 - 16s/epoch - 18ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 15s - loss: 0.7446 - accuracy: 0.6899 - val_loss: 0.6904 - val_accuracy: 0.7078 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 15s - loss: 0.7300 - accuracy: 0.6912 - val_loss: 0.6868 - val_accuracy: 0.7094 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 15s - loss: 0.7291 - accuracy: 0.6926 - val_loss: 0.6736 - val_accuracy: 0.7094 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 15s - loss: 0.7330 - accuracy: 0.6926 - val_loss: 0.6911 - val_accuracy: 0.6966 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 15s - loss: 0.7272 - accuracy: 0.6925 - val_loss: 0.6759 - val_accuracy: 0.7085 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 15s - loss: 0.7267 - accuracy: 0.6914 - val_loss: 0.6826 - val_accuracy: 0.7102 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 15s - loss: 0.7123 - accuracy: 0.6947 - val_loss: 0.6694 - val_accuracy: 0.7079 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 15s - loss: 0.7101 - accuracy: 0.6945 - val_loss: 0.6660 - val_accuracy: 0.7043 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 15s - loss: 0.7131 - accuracy: 0.6934 - val_loss: 0.6719 - val_accuracy: 0.7024 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 15s - loss: 0.7086 - accuracy: 0.6945 - val_loss: 0.6714 - val_accuracy: 0.6978 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 15s - loss: 0.7103 - accuracy: 0.6941 - val_loss: 0.6652 - val_accuracy: 0.7033 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 15s - loss: 0.7099 - accuracy: 0.6942 - val_loss: 0.6624 - val_accuracy: 0.7093 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 15s - loss: 0.7088 - accuracy: 0.6949 - val_loss: 0.6648 - val_accuracy: 0.7087 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 15s - loss: 0.7098 - accuracy: 0.6934 - val_loss: 0.6680 - val_accuracy: 0.7037 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 15s - loss: 0.7091 - accuracy: 0.6940 - val_loss: 0.6647 - val_accuracy: 0.7073 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 15s - loss: 0.6998 - accuracy: 0.6959 - val_loss: 0.6604 - val_accuracy: 0.7085 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 15s - loss: 0.7000 - accuracy: 0.6968 - val_loss: 0.6635 - val_accuracy: 0.7037 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 15s - loss: 0.6983 - accuracy: 0.6965 - val_loss: 0.6577 - val_accuracy: 0.7086 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 15s - loss: 0.6977 - accuracy: 0.6954 - val_loss: 0.6582 - val_accuracy: 0.7096 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 15s - loss: 0.7001 - accuracy: 0.6945 - val_loss: 0.6568 - val_accuracy: 0.7087 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 15s - loss: 0.6984 - accuracy: 0.6972 - val_loss: 0.6588 - val_accuracy: 0.7100 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 15s - loss: 0.6980 - accuracy: 0.6952 - val_loss: 0.6570 - val_accuracy: 0.7086 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 15s - loss: 0.6966 - accuracy: 0.6969 - val_loss: 0.6651 - val_accuracy: 0.7000 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 16s - loss: 0.6924 - accuracy: 0.6979 - val_loss: 0.6564 - val_accuracy: 0.7085 - lr: 3.1250e-04 - 16s/epoch - 17ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 16s - loss: 0.6931 - accuracy: 0.6976 - val_loss: 0.6543 - val_accuracy: 0.7098 - lr: 3.1250e-04 - 16s/epoch - 18ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 16s - loss: 0.6927 - accuracy: 0.6970 - val_loss: 0.6557 - val_accuracy: 0.7075 - lr: 3.1250e-04 - 16s/epoch - 18ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 15s - loss: 0.6928 - accuracy: 0.6985 - val_loss: 0.6580 - val_accuracy: 0.7100 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 15s - loss: 0.6921 - accuracy: 0.6968 - val_loss: 0.6550 - val_accuracy: 0.7100 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 15s - loss: 0.6901 - accuracy: 0.6983 - val_loss: 0.6533 - val_accuracy: 0.7100 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 15s - loss: 0.6897 - accuracy: 0.6986 - val_loss: 0.6532 - val_accuracy: 0.7085 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 15s - loss: 0.6888 - accuracy: 0.6985 - val_loss: 0.6533 - val_accuracy: 0.7086 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 15s - loss: 0.6895 - accuracy: 0.6984 - val_loss: 0.6536 - val_accuracy: 0.7098 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 15s - loss: 0.6896 - accuracy: 0.6972 - val_loss: 0.6537 - val_accuracy: 0.7074 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 15s - loss: 0.6891 - accuracy: 0.6983 - val_loss: 0.6528 - val_accuracy: 0.7094 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 15s - loss: 0.6885 - accuracy: 0.6986 - val_loss: 0.6536 - val_accuracy: 0.7101 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 15s - loss: 0.6871 - accuracy: 0.6994 - val_loss: 0.6521 - val_accuracy: 0.7084 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 15s - loss: 0.6880 - accuracy: 0.6993 - val_loss: 0.6517 - val_accuracy: 0.7097 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 16s - loss: 0.6889 - accuracy: 0.6980 - val_loss: 0.6521 - val_accuracy: 0.7100 - lr: 7.8125e-05 - 16s/epoch - 18ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 16s - loss: 0.6872 - accuracy: 0.6988 - val_loss: 0.6521 - val_accuracy: 0.7100 - lr: 7.8125e-05 - 16s/epoch - 18ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 15s - loss: 0.6886 - accuracy: 0.6981 - val_loss: 0.6520 - val_accuracy: 0.7100 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 15s - loss: 0.6867 - accuracy: 0.6980 - val_loss: 0.6522 - val_accuracy: 0.7100 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 15s - loss: 0.6873 - accuracy: 0.6988 - val_loss: 0.6516 - val_accuracy: 0.7098 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 15s - loss: 0.6882 - accuracy: 0.6980 - val_loss: 0.6519 - val_accuracy: 0.7098 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 15s - loss: 0.6881 - accuracy: 0.6987 - val_loss: 0.6518 - val_accuracy: 0.7098 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 15s - loss: 0.6869 - accuracy: 0.6988 - val_loss: 0.6516 - val_accuracy: 0.7102 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 15s - loss: 0.6868 - accuracy: 0.6997 - val_loss: 0.6518 - val_accuracy: 0.7102 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 15s - loss: 0.6862 - accuracy: 0.6983 - val_loss: 0.6513 - val_accuracy: 0.7098 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 15s - loss: 0.6866 - accuracy: 0.6989 - val_loss: 0.6521 - val_accuracy: 0.7098 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 70/600\n",
            "892/892 - 15s - loss: 0.6865 - accuracy: 0.6986 - val_loss: 0.6513 - val_accuracy: 0.7098 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 71/600\n",
            "892/892 - 15s - loss: 0.6868 - accuracy: 0.6995 - val_loss: 0.6517 - val_accuracy: 0.7098 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 72/600\n",
            "892/892 - 15s - loss: 0.6870 - accuracy: 0.6992 - val_loss: 0.6517 - val_accuracy: 0.7098 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 73/600\n",
            "892/892 - 15s - loss: 0.6866 - accuracy: 0.6985 - val_loss: 0.6516 - val_accuracy: 0.7098 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 74/600\n",
            "892/892 - 15s - loss: 0.6857 - accuracy: 0.6987 - val_loss: 0.6514 - val_accuracy: 0.7098 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_49 (LSTM)              (None, 10, 32)            5888      \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 10, 32)            0         \n",
            "                                                                 \n",
            " lstm_50 (LSTM)              (None, 32)                8320      \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 13)                429       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,637\n",
            "Trainable params: 14,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13  1]\n",
            " [ 6  7  8 13  2]\n",
            " [ 6  7  8 13  4]\n",
            " ...\n",
            " [ 6  7  8 13  1]\n",
            " [ 6  7  8 13  3]\n",
            " [ 6  7  8 13 11]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13  1  2]\n",
            " [ 6  7  8 13  2 11]\n",
            " [ 6  7  8 13  4  4]\n",
            " ...\n",
            " [ 6  7  8 13  1 11]\n",
            " [ 6  7  8 13  3  1]\n",
            " [ 6  7  8 13 11  3]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ...  1  2  2]\n",
            " [ 6  7  8 ...  2 11  3]\n",
            " [ 6  7  8 ...  4  4 11]\n",
            " ...\n",
            " [ 6  7  8 ...  1 11  3]\n",
            " [ 6  7  8 ...  3  1  3]\n",
            " [ 6  7  8 ... 11  3  1]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  2  2  4]\n",
            " [ 6  7  8 ... 11  3  1]\n",
            " [ 6  7  8 ...  4 11  3]\n",
            " ...\n",
            " [ 6  7  8 ... 11  3  3]\n",
            " [ 6  7  8 ...  1  3  4]\n",
            " [ 6  7  8 ...  3  1  2]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  2  4  4]\n",
            " [ 6  7  8 ...  3  1 11]\n",
            " [ 6  7  8 ... 11  3  1]\n",
            " ...\n",
            " [ 6  7  8 ...  3  3 11]\n",
            " [ 6  7  8 ...  3  4 11]\n",
            " [ 6  7  8 ...  1  2  4]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  4  4 10]\n",
            " [ 6  7  8 ...  1 11 10]\n",
            " [ 6  7  8 ...  3  1 10]\n",
            " ...\n",
            " [ 6  7  8 ...  3 11 10]\n",
            " [ 6  7  8 ...  4 11 10]\n",
            " [ 6  7  8 ...  2  4 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ... 11 10  5]\n",
            " [ 6  7  8 ...  1 10  5]\n",
            " ...\n",
            " [ 6  7  8 ... 11 10  5]\n",
            " [ 6  7  8 ... 11 10  5]\n",
            " [ 6  7  8 ...  4 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_51 (LSTM)              (None, 10, 32)            5888      \n",
            "                                                                 \n",
            " dropout_51 (Dropout)        (None, 10, 32)            0         \n",
            "                                                                 \n",
            " lstm_52 (LSTM)              (None, 32)                8320      \n",
            "                                                                 \n",
            " dropout_52 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 13)                429       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,637\n",
            "Trainable params: 14,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 20s - loss: 2.7806 - accuracy: 0.0841 - val_loss: 2.5719 - val_accuracy: 0.0833 - lr: 0.0050 - 20s/epoch - 23ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 15s - loss: 2.5737 - accuracy: 0.0829 - val_loss: 2.5706 - val_accuracy: 0.0833 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 15s - loss: 2.5731 - accuracy: 0.0810 - val_loss: 2.5733 - val_accuracy: 0.0833 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 15s - loss: 2.5726 - accuracy: 0.0815 - val_loss: 2.5707 - val_accuracy: 0.0833 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 15s - loss: 2.5724 - accuracy: 0.0830 - val_loss: 2.5712 - val_accuracy: 0.0833 - lr: 0.0050 - 15s/epoch - 17ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 15s - loss: 2.5249 - accuracy: 0.0830 - val_loss: 2.5226 - val_accuracy: 0.0833 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 15s - loss: 2.5251 - accuracy: 0.0825 - val_loss: 2.5267 - val_accuracy: 0.0833 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 15s - loss: 2.5251 - accuracy: 0.0830 - val_loss: 2.5234 - val_accuracy: 0.0833 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 15s - loss: 2.5251 - accuracy: 0.0826 - val_loss: 2.5237 - val_accuracy: 0.0833 - lr: 0.0025 - 15s/epoch - 17ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 15s - loss: 2.5054 - accuracy: 0.0827 - val_loss: 2.5050 - val_accuracy: 0.0833 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 15s - loss: 2.5056 - accuracy: 0.0835 - val_loss: 2.5054 - val_accuracy: 0.0833 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 15s - loss: 2.5063 - accuracy: 0.0840 - val_loss: 2.5065 - val_accuracy: 0.0833 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 15s - loss: 2.5064 - accuracy: 0.0807 - val_loss: 2.5044 - val_accuracy: 0.0833 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 15s - loss: 2.5064 - accuracy: 0.0817 - val_loss: 2.5057 - val_accuracy: 0.0833 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 15s - loss: 2.5063 - accuracy: 0.0823 - val_loss: 2.5065 - val_accuracy: 0.0833 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 15s - loss: 2.5064 - accuracy: 0.0832 - val_loss: 2.5045 - val_accuracy: 0.0833 - lr: 0.0012 - 15s/epoch - 17ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 15s - loss: 2.4951 - accuracy: 0.0826 - val_loss: 2.4951 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 15s - loss: 2.4947 - accuracy: 0.0821 - val_loss: 2.4943 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 15s - loss: 2.4948 - accuracy: 0.0815 - val_loss: 2.4944 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 15s - loss: 2.4948 - accuracy: 0.0820 - val_loss: 2.4939 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 15s - loss: 2.4951 - accuracy: 0.0821 - val_loss: 2.4960 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 15s - loss: 2.4952 - accuracy: 0.0822 - val_loss: 2.4937 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 15s - loss: 2.4952 - accuracy: 0.0819 - val_loss: 2.4950 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 15s - loss: 2.4952 - accuracy: 0.0816 - val_loss: 2.4960 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 15s - loss: 2.4952 - accuracy: 0.0836 - val_loss: 2.4937 - val_accuracy: 0.0833 - lr: 6.2500e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 15s - loss: 2.4899 - accuracy: 0.0826 - val_loss: 2.4899 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 15s - loss: 2.4899 - accuracy: 0.0820 - val_loss: 2.4903 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 15s - loss: 2.4899 - accuracy: 0.0833 - val_loss: 2.4896 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 15s - loss: 2.4899 - accuracy: 0.0828 - val_loss: 2.4899 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 15s - loss: 2.4899 - accuracy: 0.0821 - val_loss: 2.4900 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 15s - loss: 2.4899 - accuracy: 0.0827 - val_loss: 2.4892 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 15s - loss: 2.4900 - accuracy: 0.0832 - val_loss: 2.4899 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 15s - loss: 2.4900 - accuracy: 0.0832 - val_loss: 2.4903 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 15s - loss: 2.4900 - accuracy: 0.0820 - val_loss: 2.4892 - val_accuracy: 0.0833 - lr: 3.1250e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 15s - loss: 2.4873 - accuracy: 0.0823 - val_loss: 2.4870 - val_accuracy: 0.0833 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 15s - loss: 2.4873 - accuracy: 0.0816 - val_loss: 2.4874 - val_accuracy: 0.0833 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 15s - loss: 2.4873 - accuracy: 0.0819 - val_loss: 2.4873 - val_accuracy: 0.0833 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 15s - loss: 2.4873 - accuracy: 0.0821 - val_loss: 2.4872 - val_accuracy: 0.0833 - lr: 1.5625e-04 - 15s/epoch - 17ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 15s - loss: 2.4861 - accuracy: 0.0827 - val_loss: 2.4862 - val_accuracy: 0.0833 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 15s - loss: 2.4861 - accuracy: 0.0817 - val_loss: 2.4860 - val_accuracy: 0.0833 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 15s - loss: 2.4861 - accuracy: 0.0826 - val_loss: 2.4860 - val_accuracy: 0.0833 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 15s - loss: 2.4861 - accuracy: 0.0828 - val_loss: 2.4860 - val_accuracy: 0.0833 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 15s - loss: 2.4861 - accuracy: 0.0822 - val_loss: 2.4861 - val_accuracy: 0.0833 - lr: 7.8125e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 15s - loss: 2.4855 - accuracy: 0.0831 - val_loss: 2.4855 - val_accuracy: 0.0833 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 16s - loss: 2.4855 - accuracy: 0.0824 - val_loss: 2.4855 - val_accuracy: 0.0833 - lr: 3.9062e-05 - 16s/epoch - 18ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 16s - loss: 2.4855 - accuracy: 0.0838 - val_loss: 2.4855 - val_accuracy: 0.0833 - lr: 3.9062e-05 - 16s/epoch - 18ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 15s - loss: 2.4855 - accuracy: 0.0820 - val_loss: 2.4855 - val_accuracy: 0.0833 - lr: 3.9062e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 16s - loss: 2.4852 - accuracy: 0.0839 - val_loss: 2.4852 - val_accuracy: 0.0833 - lr: 1.9531e-05 - 16s/epoch - 17ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 15s - loss: 2.4852 - accuracy: 0.0846 - val_loss: 2.4852 - val_accuracy: 0.0833 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 15s - loss: 2.4852 - accuracy: 0.0830 - val_loss: 2.4852 - val_accuracy: 0.0833 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 15s - loss: 2.4852 - accuracy: 0.0834 - val_loss: 2.4852 - val_accuracy: 0.0833 - lr: 1.9531e-05 - 15s/epoch - 17ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 15s - loss: 2.4851 - accuracy: 0.0829 - val_loss: 2.4851 - val_accuracy: 0.0833 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 15s - loss: 2.4851 - accuracy: 0.0833 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 15s - loss: 2.4851 - accuracy: 0.0832 - val_loss: 2.4851 - val_accuracy: 0.0833 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 15s - loss: 2.4851 - accuracy: 0.0840 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 9.7656e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 15s - loss: 2.4850 - accuracy: 0.0836 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 15s - loss: 2.4850 - accuracy: 0.0826 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 15s - loss: 2.4850 - accuracy: 0.0823 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 15s - loss: 2.4850 - accuracy: 0.0825 - val_loss: 2.4850 - val_accuracy: 0.0833 - lr: 4.8828e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 16s - loss: 2.4850 - accuracy: 0.0823 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 2.4414e-06 - 16s/epoch - 17ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0822 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 2.4414e-06 - 16s/epoch - 17ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 16s - loss: 2.4850 - accuracy: 0.0830 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 2.4414e-06 - 16s/epoch - 18ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0841 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.2207e-06 - 15s/epoch - 17ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0821 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.2207e-06 - 16s/epoch - 17ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0827 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.2207e-06 - 16s/epoch - 18ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0823 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 6.1035e-07 - 16s/epoch - 18ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0841 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 6.1035e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0839 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 6.1035e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0840 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.0518e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 70/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0846 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.0518e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 71/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0832 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.0518e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 72/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0829 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.5259e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 73/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0841 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.5259e-07 - 15s/epoch - 17ms/step\n",
            "Epoch 74/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0819 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.5259e-07 - 16s/epoch - 17ms/step\n",
            "Epoch 75/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0839 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 7.6294e-08 - 15s/epoch - 17ms/step\n",
            "Epoch 76/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0840 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 7.6294e-08 - 15s/epoch - 17ms/step\n",
            "Epoch 77/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0842 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 7.6294e-08 - 15s/epoch - 17ms/step\n",
            "Epoch 78/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0838 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.8147e-08 - 16s/epoch - 17ms/step\n",
            "Epoch 79/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0850 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.8147e-08 - 15s/epoch - 17ms/step\n",
            "Epoch 80/600\n",
            "892/892 - 15s - loss: 2.4849 - accuracy: 0.0854 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 3.8147e-08 - 15s/epoch - 17ms/step\n",
            "Epoch 81/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0826 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.9073e-08 - 16s/epoch - 17ms/step\n",
            "Epoch 82/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0834 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.9073e-08 - 16s/epoch - 17ms/step\n",
            "Epoch 83/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0829 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 1.9073e-08 - 16s/epoch - 18ms/step\n",
            "Epoch 84/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0834 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 9.5367e-09 - 16s/epoch - 18ms/step\n",
            "Epoch 85/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0849 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 9.5367e-09 - 16s/epoch - 18ms/step\n",
            "Epoch 86/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0839 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 9.5367e-09 - 16s/epoch - 18ms/step\n",
            "Epoch 87/600\n",
            "892/892 - 16s - loss: 2.4849 - accuracy: 0.0826 - val_loss: 2.4849 - val_accuracy: 0.0833 - lr: 4.7684e-09 - 16s/epoch - 18ms/step\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_51 (LSTM)              (None, 10, 32)            5888      \n",
            "                                                                 \n",
            " dropout_51 (Dropout)        (None, 10, 32)            0         \n",
            "                                                                 \n",
            " lstm_52 (LSTM)              (None, 32)                8320      \n",
            "                                                                 \n",
            " dropout_52 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 13)                429       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,637\n",
            "Trainable params: 14,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[ 6 13]\n",
            " [ 6  2]\n",
            " [ 6 13]\n",
            " ...\n",
            " [ 6 11]\n",
            " [ 6  1]\n",
            " [ 6  1]]\n",
            "finding activity nr 4\n",
            "[[ 6 13  1]\n",
            " [ 6  2  5]\n",
            " [ 6 13  3]\n",
            " ...\n",
            " [ 6 11  9]\n",
            " [ 6  1  1]\n",
            " [ 6  1  7]]\n",
            "finding activity nr 5\n",
            "[[ 6 13  1 13]\n",
            " [ 6  2  5  9]\n",
            " [ 6 13  3  2]\n",
            " ...\n",
            " [ 6 11  9  1]\n",
            " [ 6  1  1  3]\n",
            " [ 6  1  7 11]]\n",
            "finding activity nr 6\n",
            "[[ 6 13  1 13  1]\n",
            " [ 6  2  5  9  1]\n",
            " [ 6 13  3  2 13]\n",
            " ...\n",
            " [ 6 11  9  1 12]\n",
            " [ 6  1  1  3  2]\n",
            " [ 6  1  7 11  3]]\n",
            "finding activity nr 7\n",
            "[[ 6 13  1 13  1 13]\n",
            " [ 6  2  5  9  1  7]\n",
            " [ 6 13  3  2 13 11]\n",
            " ...\n",
            " [ 6 11  9  1 12  4]\n",
            " [ 6  1  1  3  2  5]\n",
            " [ 6  1  7 11  3 11]]\n",
            "finding activity nr 8\n",
            "[[ 6 13  1 ...  1 13 11]\n",
            " [ 6  2  5 ...  1  7  1]\n",
            " [ 6 13  3 ... 13 11  8]\n",
            " ...\n",
            " [ 6 11  9 ... 12  4  7]\n",
            " [ 6  1  1 ...  2  5 12]\n",
            " [ 6  1  7 ...  3 11 12]]\n",
            "finding activity nr 9\n",
            "[[ 6 13  1 ... 13 11  7]\n",
            " [ 6  2  5 ...  7  1 10]\n",
            " [ 6 13  3 ... 11  8  9]\n",
            " ...\n",
            " [ 6 11  9 ...  4  7  9]\n",
            " [ 6  1  1 ...  5 12  8]\n",
            " [ 6  1  7 ... 11 12 13]]\n",
            "finding activity nr 10\n",
            "[[ 6 13  1 ... 11  7 11]\n",
            " [ 6  2  5 ...  1 10  7]\n",
            " [ 6 13  3 ...  8  9  4]\n",
            " ...\n",
            " [ 6 11  9 ...  7  9  5]\n",
            " [ 6  1  1 ... 12  8  7]\n",
            " [ 6  1  7 ... 12 13  5]]\n",
            "finding activity nr 11\n",
            "[[ 6 13  1 ...  7 11  9]\n",
            " [ 6  2  5 ... 10  7  8]\n",
            " [ 6 13  3 ...  9  4  8]\n",
            " ...\n",
            " [ 6 11  9 ...  9  5  2]\n",
            " [ 6  1  1 ...  8  7 10]\n",
            " [ 6  1  7 ... 13  5  7]]\n",
            "finding activity nr 12\n",
            "[[ 6 13  1 ... 11  9  4]\n",
            " [ 6  2  5 ...  7  8 13]\n",
            " [ 6 13  3 ...  4  8 11]\n",
            " ...\n",
            " [ 6 11  9 ...  5  2 13]\n",
            " [ 6  1  1 ...  7 10  2]\n",
            " [ 6  1  7 ...  5  7  3]]\n",
            "finding activity nr 13\n",
            "[[ 6 13  1 ...  9  4  8]\n",
            " [ 6  2  5 ...  8 13 11]\n",
            " [ 6 13  3 ...  8 11  7]\n",
            " ...\n",
            " [ 6 11  9 ...  2 13 11]\n",
            " [ 6  1  1 ... 10  2  1]\n",
            " [ 6  1  7 ...  7  3 12]]\n",
            "[[ 6 13  1 ...  4  8  5]\n",
            " [ 6  2  5 ... 13 11 13]\n",
            " [ 6 13  3 ... 11  7  1]\n",
            " ...\n",
            " [ 6 11  9 ... 13 11  3]\n",
            " [ 6  1  1 ...  2  1  4]\n",
            " [ 6  1  7 ...  3 12 10]]\n",
            "(12000, 13)\n",
            "Already exists: 2 64 0.0 0.0\n",
            "Already exists: 2 64 0.0 1e-05\n",
            "Already exists: 2 64 0.0 0.0001\n",
            "Already exists: 2 64 0.0 0.001\n",
            "Already exists: 2 64 0.0 0.01\n",
            "Already exists: 2 64 0.2 0.0\n",
            "Already exists: 2 64 0.2 1e-05\n",
            "Already exists: 2 64 0.2 0.0001\n",
            "Already exists: 2 64 0.2 0.001\n",
            "Already exists: 2 64 0.2 0.01\n",
            "Already exists: 2 64 0.4 0.0\n",
            "Already exists: 2 64 0.4 1e-05\n",
            "Already exists: 2 64 0.4 0.0001\n",
            "Already exists: 2 64 0.4 0.001\n",
            "Already exists: 2 64 0.4 0.01\n",
            "Already exists: 2 64 0.6 0.0\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_53 (LSTM)              (None, 10, 64)            19968     \n",
            "                                                                 \n",
            " dropout_53 (Dropout)        (None, 10, 64)            0         \n",
            "                                                                 \n",
            " lstm_54 (LSTM)              (None, 64)                33024     \n",
            "                                                                 \n",
            " dropout_54 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53,837\n",
            "Trainable params: 53,837\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "892/892 - 32s - loss: 0.7977 - accuracy: 0.6569 - val_loss: 0.6046 - val_accuracy: 0.7591 - lr: 0.0050 - 32s/epoch - 36ms/step\n",
            "Epoch 2/600\n",
            "892/892 - 27s - loss: 0.5500 - accuracy: 0.7622 - val_loss: 0.4571 - val_accuracy: 0.7749 - lr: 0.0050 - 27s/epoch - 30ms/step\n",
            "Epoch 3/600\n",
            "892/892 - 26s - loss: 0.4831 - accuracy: 0.7728 - val_loss: 0.4445 - val_accuracy: 0.7763 - lr: 0.0050 - 26s/epoch - 30ms/step\n",
            "Epoch 4/600\n",
            "892/892 - 27s - loss: 0.4869 - accuracy: 0.7719 - val_loss: 0.4603 - val_accuracy: 0.7732 - lr: 0.0050 - 27s/epoch - 30ms/step\n",
            "Epoch 5/600\n",
            "892/892 - 26s - loss: 0.4722 - accuracy: 0.7715 - val_loss: 0.4447 - val_accuracy: 0.7741 - lr: 0.0050 - 26s/epoch - 30ms/step\n",
            "Epoch 6/600\n",
            "892/892 - 26s - loss: 0.4861 - accuracy: 0.7711 - val_loss: 0.4504 - val_accuracy: 0.7750 - lr: 0.0050 - 26s/epoch - 30ms/step\n",
            "Epoch 7/600\n",
            "892/892 - 26s - loss: 0.4587 - accuracy: 0.7724 - val_loss: 0.4397 - val_accuracy: 0.7747 - lr: 0.0025 - 26s/epoch - 30ms/step\n",
            "Epoch 8/600\n",
            "892/892 - 26s - loss: 0.4521 - accuracy: 0.7732 - val_loss: 0.4359 - val_accuracy: 0.7742 - lr: 0.0025 - 26s/epoch - 30ms/step\n",
            "Epoch 9/600\n",
            "892/892 - 27s - loss: 0.4517 - accuracy: 0.7750 - val_loss: 0.4365 - val_accuracy: 0.7757 - lr: 0.0025 - 27s/epoch - 30ms/step\n",
            "Epoch 10/600\n",
            "892/892 - 27s - loss: 0.4495 - accuracy: 0.7719 - val_loss: 0.4324 - val_accuracy: 0.7738 - lr: 0.0025 - 27s/epoch - 30ms/step\n",
            "Epoch 11/600\n",
            "892/892 - 27s - loss: 0.4471 - accuracy: 0.7730 - val_loss: 0.4318 - val_accuracy: 0.7766 - lr: 0.0025 - 27s/epoch - 30ms/step\n",
            "Epoch 12/600\n",
            "892/892 - 26s - loss: 0.4478 - accuracy: 0.7724 - val_loss: 0.4315 - val_accuracy: 0.7725 - lr: 0.0025 - 26s/epoch - 30ms/step\n",
            "Epoch 13/600\n",
            "892/892 - 27s - loss: 0.4464 - accuracy: 0.7735 - val_loss: 0.4340 - val_accuracy: 0.7736 - lr: 0.0025 - 27s/epoch - 30ms/step\n",
            "Epoch 14/600\n",
            "892/892 - 27s - loss: 0.4514 - accuracy: 0.7719 - val_loss: 0.4386 - val_accuracy: 0.7740 - lr: 0.0025 - 27s/epoch - 30ms/step\n",
            "Epoch 15/600\n",
            "892/892 - 26s - loss: 0.4463 - accuracy: 0.7736 - val_loss: 0.4325 - val_accuracy: 0.7741 - lr: 0.0025 - 26s/epoch - 30ms/step\n",
            "Epoch 16/600\n",
            "892/892 - 26s - loss: 0.4402 - accuracy: 0.7738 - val_loss: 0.4287 - val_accuracy: 0.7735 - lr: 0.0012 - 26s/epoch - 30ms/step\n",
            "Epoch 17/600\n",
            "892/892 - 26s - loss: 0.4372 - accuracy: 0.7731 - val_loss: 0.4260 - val_accuracy: 0.7744 - lr: 0.0012 - 26s/epoch - 30ms/step\n",
            "Epoch 18/600\n",
            "892/892 - 27s - loss: 0.4359 - accuracy: 0.7736 - val_loss: 0.4248 - val_accuracy: 0.7746 - lr: 0.0012 - 27s/epoch - 30ms/step\n",
            "Epoch 19/600\n",
            "892/892 - 26s - loss: 0.4346 - accuracy: 0.7738 - val_loss: 0.4243 - val_accuracy: 0.7735 - lr: 0.0012 - 26s/epoch - 30ms/step\n",
            "Epoch 20/600\n",
            "892/892 - 27s - loss: 0.4357 - accuracy: 0.7730 - val_loss: 0.4287 - val_accuracy: 0.7724 - lr: 0.0012 - 27s/epoch - 30ms/step\n",
            "Epoch 21/600\n",
            "892/892 - 27s - loss: 0.4348 - accuracy: 0.7733 - val_loss: 0.4239 - val_accuracy: 0.7723 - lr: 0.0012 - 27s/epoch - 30ms/step\n",
            "Epoch 22/600\n",
            "892/892 - 27s - loss: 0.4326 - accuracy: 0.7730 - val_loss: 0.4222 - val_accuracy: 0.7760 - lr: 0.0012 - 27s/epoch - 30ms/step\n",
            "Epoch 23/600\n",
            "892/892 - 27s - loss: 0.4321 - accuracy: 0.7736 - val_loss: 0.4215 - val_accuracy: 0.7750 - lr: 0.0012 - 27s/epoch - 30ms/step\n",
            "Epoch 24/600\n",
            "892/892 - 27s - loss: 0.4336 - accuracy: 0.7737 - val_loss: 0.4259 - val_accuracy: 0.7720 - lr: 0.0012 - 27s/epoch - 30ms/step\n",
            "Epoch 25/600\n",
            "892/892 - 27s - loss: 0.4352 - accuracy: 0.7730 - val_loss: 0.4228 - val_accuracy: 0.7740 - lr: 0.0012 - 27s/epoch - 30ms/step\n",
            "Epoch 26/600\n",
            "892/892 - 26s - loss: 0.4317 - accuracy: 0.7743 - val_loss: 0.4227 - val_accuracy: 0.7754 - lr: 0.0012 - 26s/epoch - 30ms/step\n",
            "Epoch 27/600\n",
            "892/892 - 27s - loss: 0.4300 - accuracy: 0.7743 - val_loss: 0.4206 - val_accuracy: 0.7725 - lr: 6.2500e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 28/600\n",
            "892/892 - 27s - loss: 0.4288 - accuracy: 0.7731 - val_loss: 0.4196 - val_accuracy: 0.7736 - lr: 6.2500e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 29/600\n",
            "892/892 - 27s - loss: 0.4283 - accuracy: 0.7740 - val_loss: 0.4192 - val_accuracy: 0.7738 - lr: 6.2500e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 30/600\n",
            "892/892 - 27s - loss: 0.4285 - accuracy: 0.7741 - val_loss: 0.4191 - val_accuracy: 0.7740 - lr: 6.2500e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 31/600\n",
            "892/892 - 27s - loss: 0.4272 - accuracy: 0.7747 - val_loss: 0.4185 - val_accuracy: 0.7755 - lr: 6.2500e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 32/600\n",
            "892/892 - 26s - loss: 0.4273 - accuracy: 0.7724 - val_loss: 0.4184 - val_accuracy: 0.7730 - lr: 6.2500e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 33/600\n",
            "892/892 - 26s - loss: 0.4273 - accuracy: 0.7739 - val_loss: 0.4186 - val_accuracy: 0.7728 - lr: 6.2500e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 34/600\n",
            "892/892 - 26s - loss: 0.4262 - accuracy: 0.7745 - val_loss: 0.4182 - val_accuracy: 0.7746 - lr: 6.2500e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 35/600\n",
            "892/892 - 26s - loss: 0.4278 - accuracy: 0.7719 - val_loss: 0.4186 - val_accuracy: 0.7762 - lr: 6.2500e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 36/600\n",
            "892/892 - 26s - loss: 0.4264 - accuracy: 0.7733 - val_loss: 0.4178 - val_accuracy: 0.7728 - lr: 6.2500e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 37/600\n",
            "892/892 - 26s - loss: 0.4262 - accuracy: 0.7738 - val_loss: 0.4173 - val_accuracy: 0.7759 - lr: 6.2500e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 38/600\n",
            "892/892 - 26s - loss: 0.4257 - accuracy: 0.7736 - val_loss: 0.4172 - val_accuracy: 0.7760 - lr: 6.2500e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 39/600\n",
            "892/892 - 26s - loss: 0.4277 - accuracy: 0.7726 - val_loss: 0.4192 - val_accuracy: 0.7728 - lr: 6.2500e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 40/600\n",
            "892/892 - 26s - loss: 0.4259 - accuracy: 0.7740 - val_loss: 0.4179 - val_accuracy: 0.7763 - lr: 6.2500e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 41/600\n",
            "892/892 - 26s - loss: 0.4247 - accuracy: 0.7743 - val_loss: 0.4169 - val_accuracy: 0.7746 - lr: 3.1250e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 42/600\n",
            "892/892 - 27s - loss: 0.4242 - accuracy: 0.7747 - val_loss: 0.4164 - val_accuracy: 0.7759 - lr: 3.1250e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 43/600\n",
            "892/892 - 27s - loss: 0.4241 - accuracy: 0.7745 - val_loss: 0.4162 - val_accuracy: 0.7752 - lr: 3.1250e-04 - 27s/epoch - 31ms/step\n",
            "Epoch 44/600\n",
            "892/892 - 27s - loss: 0.4236 - accuracy: 0.7735 - val_loss: 0.4161 - val_accuracy: 0.7730 - lr: 3.1250e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 45/600\n",
            "892/892 - 27s - loss: 0.4227 - accuracy: 0.7755 - val_loss: 0.4158 - val_accuracy: 0.7728 - lr: 3.1250e-04 - 27s/epoch - 30ms/step\n",
            "Epoch 46/600\n",
            "892/892 - 26s - loss: 0.4238 - accuracy: 0.7736 - val_loss: 0.4157 - val_accuracy: 0.7758 - lr: 3.1250e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 47/600\n",
            "892/892 - 26s - loss: 0.4230 - accuracy: 0.7738 - val_loss: 0.4156 - val_accuracy: 0.7759 - lr: 3.1250e-04 - 26s/epoch - 30ms/step\n",
            "Epoch 48/600\n",
            "892/892 - 26s - loss: 0.4226 - accuracy: 0.7746 - val_loss: 0.4155 - val_accuracy: 0.7760 - lr: 3.1250e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 49/600\n",
            "892/892 - 26s - loss: 0.4229 - accuracy: 0.7741 - val_loss: 0.4160 - val_accuracy: 0.7733 - lr: 3.1250e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 50/600\n",
            "892/892 - 26s - loss: 0.4229 - accuracy: 0.7739 - val_loss: 0.4157 - val_accuracy: 0.7737 - lr: 3.1250e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 51/600\n",
            "892/892 - 26s - loss: 0.4223 - accuracy: 0.7744 - val_loss: 0.4150 - val_accuracy: 0.7739 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 52/600\n",
            "892/892 - 26s - loss: 0.4219 - accuracy: 0.7744 - val_loss: 0.4149 - val_accuracy: 0.7749 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 53/600\n",
            "892/892 - 26s - loss: 0.4215 - accuracy: 0.7753 - val_loss: 0.4149 - val_accuracy: 0.7735 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 54/600\n",
            "892/892 - 26s - loss: 0.4217 - accuracy: 0.7734 - val_loss: 0.4148 - val_accuracy: 0.7757 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 55/600\n",
            "892/892 - 26s - loss: 0.4219 - accuracy: 0.7741 - val_loss: 0.4146 - val_accuracy: 0.7748 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 56/600\n",
            "892/892 - 26s - loss: 0.4213 - accuracy: 0.7762 - val_loss: 0.4148 - val_accuracy: 0.7733 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 57/600\n",
            "892/892 - 26s - loss: 0.4217 - accuracy: 0.7726 - val_loss: 0.4147 - val_accuracy: 0.7756 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 58/600\n",
            "892/892 - 26s - loss: 0.4216 - accuracy: 0.7732 - val_loss: 0.4145 - val_accuracy: 0.7750 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 59/600\n",
            "892/892 - 26s - loss: 0.4217 - accuracy: 0.7739 - val_loss: 0.4144 - val_accuracy: 0.7755 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 60/600\n",
            "892/892 - 26s - loss: 0.4214 - accuracy: 0.7736 - val_loss: 0.4147 - val_accuracy: 0.7731 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 61/600\n",
            "892/892 - 26s - loss: 0.4211 - accuracy: 0.7742 - val_loss: 0.4143 - val_accuracy: 0.7743 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 62/600\n",
            "892/892 - 26s - loss: 0.4213 - accuracy: 0.7748 - val_loss: 0.4144 - val_accuracy: 0.7723 - lr: 1.5625e-04 - 26s/epoch - 29ms/step\n",
            "Epoch 63/600\n",
            "892/892 - 26s - loss: 0.4207 - accuracy: 0.7747 - val_loss: 0.4142 - val_accuracy: 0.7744 - lr: 7.8125e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 64/600\n",
            "892/892 - 26s - loss: 0.4205 - accuracy: 0.7739 - val_loss: 0.4141 - val_accuracy: 0.7751 - lr: 7.8125e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 65/600\n",
            "892/892 - 26s - loss: 0.4208 - accuracy: 0.7742 - val_loss: 0.4140 - val_accuracy: 0.7749 - lr: 7.8125e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 66/600\n",
            "892/892 - 26s - loss: 0.4208 - accuracy: 0.7740 - val_loss: 0.4141 - val_accuracy: 0.7730 - lr: 7.8125e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 67/600\n",
            "892/892 - 26s - loss: 0.4205 - accuracy: 0.7749 - val_loss: 0.4140 - val_accuracy: 0.7745 - lr: 7.8125e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 68/600\n",
            "892/892 - 26s - loss: 0.4208 - accuracy: 0.7745 - val_loss: 0.4141 - val_accuracy: 0.7736 - lr: 7.8125e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 69/600\n",
            "892/892 - 26s - loss: 0.4210 - accuracy: 0.7739 - val_loss: 0.4140 - val_accuracy: 0.7741 - lr: 7.8125e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 70/600\n",
            "892/892 - 26s - loss: 0.4206 - accuracy: 0.7748 - val_loss: 0.4139 - val_accuracy: 0.7735 - lr: 7.8125e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 71/600\n",
            "892/892 - 26s - loss: 0.4209 - accuracy: 0.7738 - val_loss: 0.4138 - val_accuracy: 0.7749 - lr: 3.9062e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 72/600\n",
            "892/892 - 26s - loss: 0.4202 - accuracy: 0.7743 - val_loss: 0.4138 - val_accuracy: 0.7741 - lr: 3.9062e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 73/600\n",
            "892/892 - 26s - loss: 0.4203 - accuracy: 0.7742 - val_loss: 0.4138 - val_accuracy: 0.7738 - lr: 3.9062e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 74/600\n",
            "892/892 - 26s - loss: 0.4208 - accuracy: 0.7745 - val_loss: 0.4138 - val_accuracy: 0.7749 - lr: 3.9062e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 75/600\n",
            "892/892 - 26s - loss: 0.4203 - accuracy: 0.7750 - val_loss: 0.4137 - val_accuracy: 0.7738 - lr: 1.9531e-05 - 26s/epoch - 30ms/step\n",
            "Epoch 76/600\n",
            "892/892 - 26s - loss: 0.4202 - accuracy: 0.7742 - val_loss: 0.4137 - val_accuracy: 0.7736 - lr: 1.9531e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 77/600\n",
            "892/892 - 26s - loss: 0.4200 - accuracy: 0.7759 - val_loss: 0.4137 - val_accuracy: 0.7736 - lr: 1.9531e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 78/600\n",
            "892/892 - 26s - loss: 0.4204 - accuracy: 0.7750 - val_loss: 0.4137 - val_accuracy: 0.7740 - lr: 1.9531e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 79/600\n",
            "892/892 - 26s - loss: 0.4197 - accuracy: 0.7754 - val_loss: 0.4137 - val_accuracy: 0.7729 - lr: 1.9531e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 80/600\n",
            "892/892 - 26s - loss: 0.4201 - accuracy: 0.7751 - val_loss: 0.4137 - val_accuracy: 0.7749 - lr: 1.9531e-05 - 26s/epoch - 29ms/step\n",
            "Epoch 81/600\n",
            "892/892 - 26s - loss: 0.4200 - accuracy: 0.7755 - val_loss: 0.4136 - val_accuracy: 0.7732 - lr: 9.7656e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 82/600\n",
            "892/892 - 26s - loss: 0.4201 - accuracy: 0.7744 - val_loss: 0.4136 - val_accuracy: 0.7744 - lr: 9.7656e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 83/600\n",
            "892/892 - 26s - loss: 0.4199 - accuracy: 0.7761 - val_loss: 0.4136 - val_accuracy: 0.7736 - lr: 9.7656e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 84/600\n",
            "892/892 - 26s - loss: 0.4201 - accuracy: 0.7749 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 4.8828e-06 - 26s/epoch - 30ms/step\n",
            "Epoch 85/600\n",
            "892/892 - 26s - loss: 0.4203 - accuracy: 0.7751 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 4.8828e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 86/600\n",
            "892/892 - 26s - loss: 0.4201 - accuracy: 0.7747 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 4.8828e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 87/600\n",
            "892/892 - 26s - loss: 0.4199 - accuracy: 0.7757 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 2.4414e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 88/600\n",
            "892/892 - 26s - loss: 0.4201 - accuracy: 0.7755 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 2.4414e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 89/600\n",
            "892/892 - 26s - loss: 0.4203 - accuracy: 0.7746 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 2.4414e-06 - 26s/epoch - 30ms/step\n",
            "Epoch 90/600\n",
            "892/892 - 26s - loss: 0.4201 - accuracy: 0.7749 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 2.4414e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 91/600\n",
            "892/892 - 26s - loss: 0.4197 - accuracy: 0.7743 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 2.4414e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 92/600\n",
            "892/892 - 26s - loss: 0.4199 - accuracy: 0.7747 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 2.4414e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 93/600\n",
            "892/892 - 26s - loss: 0.4198 - accuracy: 0.7756 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 1.2207e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 94/600\n",
            "892/892 - 26s - loss: 0.4200 - accuracy: 0.7745 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 1.2207e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 95/600\n",
            "892/892 - 26s - loss: 0.4203 - accuracy: 0.7750 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 1.2207e-06 - 26s/epoch - 29ms/step\n",
            "Epoch 96/600\n",
            "892/892 - 26s - loss: 0.4197 - accuracy: 0.7755 - val_loss: 0.4136 - val_accuracy: 0.7735 - lr: 6.1035e-07 - 26s/epoch - 29ms/step\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_53 (LSTM)              (None, 10, 64)            19968     \n",
            "                                                                 \n",
            " dropout_53 (Dropout)        (None, 10, 64)            0         \n",
            "                                                                 \n",
            " lstm_54 (LSTM)              (None, 64)                33024     \n",
            "                                                                 \n",
            " dropout_54 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 13)                845       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53,837\n",
            "Trainable params: 53,837\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " ...\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]]\n",
            "finding activity nr 2\n",
            "[[6]\n",
            " [6]\n",
            " [6]\n",
            " ...\n",
            " [6]\n",
            " [6]\n",
            " [6]]\n",
            "finding activity nr 3\n",
            "[[6 7]\n",
            " [6 7]\n",
            " [6 7]\n",
            " ...\n",
            " [6 7]\n",
            " [6 7]\n",
            " [6 7]]\n",
            "finding activity nr 4\n",
            "[[6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " ...\n",
            " [6 7 8]\n",
            " [6 7 8]\n",
            " [6 7 8]]\n",
            "finding activity nr 5\n",
            "[[ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " ...\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]\n",
            " [ 6  7  8 13]]\n",
            "finding activity nr 6\n",
            "[[ 6  7  8 13  1]\n",
            " [ 6  7  8 13  3]\n",
            " [ 6  7  8 13  3]\n",
            " ...\n",
            " [ 6  7  8 13  2]\n",
            " [ 6  7  8 13  4]\n",
            " [ 6  7  8 13 11]]\n",
            "finding activity nr 7\n",
            "[[ 6  7  8 13  1  2]\n",
            " [ 6  7  8 13  3  2]\n",
            " [ 6  7  8 13  3 11]\n",
            " ...\n",
            " [ 6  7  8 13  2 11]\n",
            " [ 6  7  8 13  4  1]\n",
            " [ 6  7  8 13 11  1]]\n",
            "finding activity nr 8\n",
            "[[ 6  7  8 ...  1  2  3]\n",
            " [ 6  7  8 ...  3  2 11]\n",
            " [ 6  7  8 ...  3 11  1]\n",
            " ...\n",
            " [ 6  7  8 ...  2 11  4]\n",
            " [ 6  7  8 ...  4  1 11]\n",
            " [ 6  7  8 ... 11  1  4]]\n",
            "finding activity nr 9\n",
            "[[ 6  7  8 ...  2  3  4]\n",
            " [ 6  7  8 ...  2 11  1]\n",
            " [ 6  7  8 ... 11  1  4]\n",
            " ...\n",
            " [ 6  7  8 ... 11  4  3]\n",
            " [ 6  7  8 ...  1 11  3]\n",
            " [ 6  7  8 ...  1  4  2]]\n",
            "finding activity nr 10\n",
            "[[ 6  7  8 ...  3  4 11]\n",
            " [ 6  7  8 ... 11  1  4]\n",
            " [ 6  7  8 ...  1  4  2]\n",
            " ...\n",
            " [ 6  7  8 ...  4  3  1]\n",
            " [ 6  7  8 ... 11  3  2]\n",
            " [ 6  7  8 ...  4  2  3]]\n",
            "finding activity nr 11\n",
            "[[ 6  7  8 ...  4 11 10]\n",
            " [ 6  7  8 ...  1  4 10]\n",
            " [ 6  7  8 ...  4  2 10]\n",
            " ...\n",
            " [ 6  7  8 ...  3  1 10]\n",
            " [ 6  7  8 ...  3  2 10]\n",
            " [ 6  7  8 ...  2  3 10]]\n",
            "finding activity nr 12\n",
            "[[ 6  7  8 ... 11 10  5]\n",
            " [ 6  7  8 ...  4 10  5]\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " ...\n",
            " [ 6  7  8 ...  1 10  5]\n",
            " [ 6  7  8 ...  2 10  5]\n",
            " [ 6  7  8 ...  3 10  5]]\n",
            "finding activity nr 13\n",
            "[[ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " ...\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]\n",
            " [ 6  7  8 ... 10  5 12]]\n",
            "[[ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " ...\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]\n",
            " [ 6  7  8 ...  5 12  9]]\n",
            "(12000, 13)\n",
            "Already exists: 2 64 0.6 0.0001\n",
            "Already exists: 2 64 0.6 0.001\n",
            "Already exists: 2 64 0.6 0.01\n"
          ]
        }
      ],
      "source": [
        "do_grid_search_without_embedding_one_variant(3, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QYxCo8qvuyRU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Hyper_VAR3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}