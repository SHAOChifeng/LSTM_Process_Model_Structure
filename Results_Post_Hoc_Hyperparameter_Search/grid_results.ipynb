{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate fitness, precision, generalization and average for the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappingfilename = 'Parallel/mapping.txt' \n",
    "with open(mappingfilename) as f:\n",
    "    mapping = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E2': 1, 'E1': 2, 'E3': 3, 'E4': 4, 'G': 5, 'A': 6, 'B': 7, 'C': 8, 'I': 9, 'F': 10, 'E5': 11, 'H': 12, 'D': 13}\n"
     ]
    }
   ],
   "source": [
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_log(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return(df.values.tolist())\n",
    "\n",
    "def remove_nan(lists):\n",
    "    newlists = []\n",
    "    for tr in lists:\n",
    "        newlists.append([int(x) for x in tr if str(x) != 'nan'])\n",
    "    return(newlists)\n",
    "\n",
    "\n",
    "\n",
    "def delete_variant(log, variant):\n",
    "    return([trace for trace in log if trace != variant])\n",
    "\n",
    "def get_variants_list(lst): #get all of the variants in a list, return as list\n",
    "    st = set(tuple(i) for i in lst) #convert list into set of tuples\n",
    "    lst2 = list(st) #convert set of tuples into lsit of tuples\n",
    "    return [list(e) for e in lst2] \n",
    "\n",
    "def count_variant(log, variant): #count how many times a variant comes up in list\n",
    "    c = 0\n",
    "    for trace in log:\n",
    "        if trace == variant:\n",
    "            c += 1\n",
    "    return(c)\n",
    "\n",
    "def compare_variants(var1, var2): #compare two logs, what comes up in the other \n",
    "    s1 = set(tuple(i) for i in var1)\n",
    "    s2 = set(tuple(i) for i in var2)\n",
    "    \n",
    "   # print(\"Missing values in second list:\", (s1.difference(s2))) \n",
    "   # print(\"Additional values in second list:\", (s2.difference(s1))) \n",
    "    \n",
    "    return([list(e) for e in list(s1.difference(s2))],[list(e) for e in list(s2.difference(s1))])\n",
    "\n",
    "def demap_trace(t, mapping): #unmap trace, from number encoding to activity label\n",
    "    map = {v: k for k, v in mapping.items()}\n",
    "    return [map[a] for a in t]\n",
    "\n",
    "\n",
    "\n",
    "def apply_integer_map(log, map):\n",
    "    return [[map[a['concept:name']] for a in t] for t in log]\n",
    "\n",
    "#fucntion gets the counts of all of the variants \n",
    "\n",
    "def get_counts(log, variants):\n",
    "    counts = []\n",
    "    for var in variants:\n",
    "        counts.append(count_variant(log, var))\n",
    "    return counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitness(occ_each_trvar_sim, occ_each_trvar_tr):\n",
    "    arr = [min(occ_each_trvar_sim[i], occ_each_trvar_tr[i])/sum(occ_each_trvar_tr) for i in range(0, len(occ_each_trvar_sim))]\n",
    "    return sum(arr)\n",
    "\n",
    "def get_precision(occ_each_simvar_sim, occ_each_simvar_trte):\n",
    "    arr = [min(occ_each_simvar_sim[i], occ_each_simvar_trte[i])/sum(occ_each_simvar_sim) for i in range(0, len(occ_each_simvar_sim))]\n",
    "    return sum(arr)\n",
    "\n",
    "def get_generalization(occ_each_tevar_sim, occ_each_tevar_te):\n",
    "    arr = [min(occ_each_tevar_sim[i], occ_each_tevar_te[i])/sum(occ_each_tevar_te) for i in range(0, len(occ_each_tevar_sim))]\n",
    "    return sum(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = remove_nan(import_log('Parallel/variants.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['BiDirec','Num_layers', 'Size', 'Dropout', 'Reg', 'Gen.', 'Prec.', 'Fit.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 1 16 0.0 0.0\n",
      "True 1 16 0.0 1e-05\n",
      "True 1 16 0.0 0.0001\n",
      "True 1 16 0.0 0.001\n",
      "True 1 16 0.0 0.01\n",
      "True 1 16 0.2 0.0\n",
      "True 1 16 0.2 1e-05\n",
      "True 1 16 0.2 0.0001\n",
      "True 1 16 0.2 0.001\n",
      "True 1 16 0.2 0.01\n",
      "True 1 16 0.4 0.0\n",
      "True 1 16 0.4 1e-05\n",
      "True 1 16 0.4 0.0001\n",
      "True 1 16 0.4 0.001\n",
      "True 1 16 0.4 0.01\n",
      "True 1 16 0.6 0.0\n",
      "True 1 16 0.6 1e-05\n",
      "True 1 16 0.6 0.0001\n",
      "True 1 16 0.6 0.001\n",
      "True 1 16 0.6 0.01\n",
      "True 1 32 0.0 0.0\n",
      "True 1 32 0.0 1e-05\n",
      "True 1 32 0.0 0.0001\n",
      "True 1 32 0.0 0.001\n",
      "True 1 32 0.0 0.01\n",
      "True 1 32 0.2 0.0\n",
      "True 1 32 0.2 1e-05\n",
      "True 1 32 0.2 0.0001\n",
      "True 1 32 0.2 0.001\n",
      "True 1 32 0.2 0.01\n",
      "True 1 32 0.4 0.0\n",
      "True 1 32 0.4 1e-05\n",
      "True 1 32 0.4 0.0001\n",
      "True 1 32 0.4 0.001\n",
      "True 1 32 0.4 0.01\n",
      "True 1 32 0.6 0.0\n",
      "True 1 32 0.6 1e-05\n",
      "True 1 32 0.6 0.0001\n",
      "True 1 32 0.6 0.001\n",
      "True 1 32 0.6 0.01\n",
      "True 1 64 0.0 0.0\n",
      "True 1 64 0.0 1e-05\n",
      "True 1 64 0.0 0.0001\n",
      "True 1 64 0.0 0.001\n",
      "True 1 64 0.0 0.01\n",
      "True 1 64 0.2 0.0\n",
      "True 1 64 0.2 1e-05\n",
      "True 1 64 0.2 0.0001\n",
      "True 1 64 0.2 0.001\n",
      "True 1 64 0.2 0.01\n",
      "True 1 64 0.4 0.0\n",
      "True 1 64 0.4 1e-05\n",
      "True 1 64 0.4 0.0001\n",
      "True 1 64 0.4 0.001\n",
      "True 1 64 0.4 0.01\n",
      "True 1 64 0.6 0.0\n",
      "True 1 64 0.6 1e-05\n",
      "True 1 64 0.6 0.0001\n",
      "True 1 64 0.6 0.001\n",
      "True 1 64 0.6 0.01\n",
      "True 2 16 0.0 0.0\n",
      "True 2 16 0.0 1e-05\n",
      "True 2 16 0.0 0.0001\n",
      "True 2 16 0.0 0.001\n",
      "True 2 16 0.0 0.01\n",
      "True 2 16 0.2 0.0\n",
      "True 2 16 0.2 1e-05\n",
      "True 2 16 0.2 0.0001\n",
      "True 2 16 0.2 0.001\n",
      "True 2 16 0.2 0.01\n",
      "True 2 16 0.4 0.0\n",
      "True 2 16 0.4 1e-05\n",
      "True 2 16 0.4 0.0001\n",
      "True 2 16 0.4 0.001\n",
      "True 2 16 0.4 0.01\n",
      "True 2 16 0.6 0.0\n",
      "True 2 16 0.6 1e-05\n",
      "True 2 16 0.6 0.0001\n",
      "True 2 16 0.6 0.001\n",
      "True 2 16 0.6 0.01\n",
      "True 2 32 0.0 0.0\n",
      "True 2 32 0.0 1e-05\n",
      "True 2 32 0.0 0.0001\n",
      "True 2 32 0.0 0.001\n",
      "True 2 32 0.0 0.01\n",
      "True 2 32 0.2 0.0\n",
      "True 2 32 0.2 1e-05\n",
      "True 2 32 0.2 0.0001\n",
      "True 2 32 0.2 0.001\n",
      "True 2 32 0.2 0.01\n",
      "True 2 32 0.4 0.0\n",
      "True 2 32 0.4 1e-05\n",
      "True 2 32 0.4 0.0001\n",
      "True 2 32 0.4 0.001\n",
      "True 2 32 0.4 0.01\n",
      "True 2 32 0.6 0.0\n",
      "True 2 32 0.6 1e-05\n",
      "True 2 32 0.6 0.0001\n",
      "True 2 32 0.6 0.001\n",
      "True 2 32 0.6 0.01\n",
      "True 2 64 0.0 0.0\n",
      "True 2 64 0.0 1e-05\n",
      "True 2 64 0.0 0.0001\n",
      "True 2 64 0.0 0.001\n",
      "True 2 64 0.0 0.01\n",
      "True 2 64 0.2 0.0\n",
      "True 2 64 0.2 1e-05\n",
      "True 2 64 0.2 0.0001\n",
      "True 2 64 0.2 0.001\n",
      "True 2 64 0.2 0.01\n",
      "True 2 64 0.4 0.0\n",
      "True 2 64 0.4 1e-05\n",
      "True 2 64 0.4 0.0001\n",
      "True 2 64 0.4 0.001\n",
      "True 2 64 0.4 0.01\n",
      "True 2 64 0.6 0.0\n",
      "True 2 64 0.6 1e-05\n",
      "True 2 64 0.6 0.0001\n",
      "True 2 64 0.6 0.001\n",
      "True 2 64 0.6 0.01\n",
      "False 1 16 0.0 0.0\n",
      "False 1 16 0.0 1e-05\n",
      "False 1 16 0.0 0.0001\n",
      "False 1 16 0.0 0.001\n",
      "False 1 16 0.0 0.01\n",
      "False 1 16 0.2 0.0\n",
      "False 1 16 0.2 1e-05\n",
      "False 1 16 0.2 0.0001\n",
      "False 1 16 0.2 0.001\n",
      "False 1 16 0.2 0.01\n",
      "False 1 16 0.4 0.0\n",
      "False 1 16 0.4 1e-05\n",
      "False 1 16 0.4 0.0001\n",
      "False 1 16 0.4 0.001\n",
      "False 1 16 0.4 0.01\n",
      "False 1 16 0.6 0.0\n",
      "False 1 16 0.6 1e-05\n",
      "False 1 16 0.6 0.0001\n",
      "False 1 16 0.6 0.001\n",
      "False 1 16 0.6 0.01\n",
      "False 1 32 0.0 0.0\n",
      "False 1 32 0.0 1e-05\n",
      "False 1 32 0.0 0.0001\n",
      "False 1 32 0.0 0.001\n",
      "False 1 32 0.0 0.01\n",
      "False 1 32 0.2 0.0\n",
      "False 1 32 0.2 1e-05\n",
      "False 1 32 0.2 0.0001\n",
      "False 1 32 0.2 0.001\n",
      "False 1 32 0.2 0.01\n",
      "False 1 32 0.4 0.0\n",
      "False 1 32 0.4 1e-05\n",
      "False 1 32 0.4 0.0001\n",
      "False 1 32 0.4 0.001\n",
      "False 1 32 0.4 0.01\n",
      "False 1 32 0.6 0.0\n",
      "False 1 32 0.6 1e-05\n",
      "False 1 32 0.6 0.0001\n",
      "False 1 32 0.6 0.001\n",
      "False 1 32 0.6 0.01\n",
      "False 1 64 0.0 0.0\n",
      "False 1 64 0.0 1e-05\n",
      "False 1 64 0.0 0.0001\n",
      "False 1 64 0.0 0.001\n",
      "False 1 64 0.0 0.01\n",
      "False 1 64 0.2 0.0\n",
      "False 1 64 0.2 1e-05\n",
      "False 1 64 0.2 0.0001\n",
      "False 1 64 0.2 0.001\n",
      "False 1 64 0.2 0.01\n",
      "False 1 64 0.4 0.0\n",
      "False 1 64 0.4 1e-05\n",
      "False 1 64 0.4 0.0001\n",
      "False 1 64 0.4 0.001\n",
      "False 1 64 0.4 0.01\n",
      "False 1 64 0.6 0.0\n",
      "False 1 64 0.6 1e-05\n",
      "False 1 64 0.6 0.0001\n",
      "False 1 64 0.6 0.001\n",
      "False 1 64 0.6 0.01\n",
      "False 2 16 0.0 0.0\n",
      "False 2 16 0.0 1e-05\n",
      "False 2 16 0.0 0.0001\n",
      "False 2 16 0.0 0.001\n",
      "False 2 16 0.0 0.01\n",
      "False 2 16 0.2 0.0\n",
      "False 2 16 0.2 1e-05\n",
      "False 2 16 0.2 0.0001\n",
      "False 2 16 0.2 0.001\n",
      "False 2 16 0.2 0.01\n",
      "False 2 16 0.4 0.0\n",
      "False 2 16 0.4 1e-05\n",
      "False 2 16 0.4 0.0001\n",
      "False 2 16 0.4 0.001\n",
      "False 2 16 0.4 0.01\n",
      "False 2 16 0.6 0.0\n",
      "False 2 16 0.6 1e-05\n",
      "False 2 16 0.6 0.0001\n",
      "False 2 16 0.6 0.001\n",
      "False 2 16 0.6 0.01\n",
      "False 2 32 0.0 0.0\n",
      "False 2 32 0.0 1e-05\n",
      "False 2 32 0.0 0.0001\n",
      "False 2 32 0.0 0.001\n",
      "False 2 32 0.0 0.01\n",
      "False 2 32 0.2 0.0\n",
      "False 2 32 0.2 1e-05\n",
      "False 2 32 0.2 0.0001\n",
      "False 2 32 0.2 0.001\n",
      "False 2 32 0.2 0.01\n",
      "False 2 32 0.4 0.0\n",
      "False 2 32 0.4 1e-05\n",
      "False 2 32 0.4 0.0001\n",
      "False 2 32 0.4 0.001\n",
      "False 2 32 0.4 0.01\n",
      "False 2 32 0.6 0.0\n",
      "False 2 32 0.6 1e-05\n",
      "False 2 32 0.6 0.0001\n",
      "False 2 32 0.6 0.001\n",
      "False 2 32 0.6 0.01\n",
      "False 2 64 0.0 0.0\n",
      "False 2 64 0.0 1e-05\n",
      "False 2 64 0.0 0.0001\n",
      "False 2 64 0.0 0.001\n",
      "False 2 64 0.0 0.01\n",
      "False 2 64 0.2 0.0\n",
      "False 2 64 0.2 1e-05\n",
      "False 2 64 0.2 0.0001\n",
      "False 2 64 0.2 0.001\n",
      "False 2 64 0.2 0.01\n",
      "False 2 64 0.4 0.0\n",
      "False 2 64 0.4 1e-05\n",
      "False 2 64 0.4 0.0001\n",
      "False 2 64 0.4 0.001\n",
      "False 2 64 0.4 0.01\n",
      "False 2 64 0.6 0.0\n",
      "False 2 64 0.6 1e-05\n",
      "False 2 64 0.6 0.0001\n",
      "False 2 64 0.6 0.001\n",
      "False 2 64 0.6 0.01\n"
     ]
    }
   ],
   "source": [
    "bidirec = [True, False]\n",
    "grid_nr_layers =  [1, 2]\n",
    "grid_layersize = [16, 32, 64]\n",
    "grid_dropout = [0.0, 0.2, 0.4, 0.6]\n",
    "grid_reg = [0.0, 0.00001, 0.0001, 0.001, 0.01]\n",
    "\n",
    "for bi in bidirec:\n",
    "    for num_layers in grid_nr_layers:\n",
    "        for layersize in grid_layersize:\n",
    "            for dropout in grid_dropout:\n",
    "                for reg in grid_reg:\n",
    "                \n",
    "                    fitness_arr = []\n",
    "                    precision_arr = []\n",
    "                    generalization_arr = []\n",
    "                    print(bi, num_layers, layersize, dropout, reg)\n",
    "\n",
    "                    for variant in range(1,9):\n",
    "                        trainname = 'Parallel/Training_logs/log_'+str(variant)+'.csv'\n",
    "                        trainlog = remove_nan(import_log(trainname))\n",
    "                        \n",
    "                        if bi == False:\n",
    "                            SimLogName = 'Parallel/Simulated_logs/Var'+str(variant)+'/SIMLOG_NL'+str(num_layers)+'emb'+'N'+'LS'+str(layersize)+'D'+str(dropout).replace('0.', '')+'R'+str(reg).replace('.', '')+'.csv'\n",
    "                        else:\n",
    "                            SimLogName = 'Parallel/Simulated_logs/Var'+str(variant)+'/SIMLOG_BIDIREC_NL'+str(num_layers)+'emb'+'N'+'LS'+str(layersize)+'D'+str(dropout).replace('0.', '')+'R'+str(reg).replace('.', '')+'.csv'\n",
    "\n",
    "\n",
    "                        simlog = remove_nan(import_log(SimLogName))\n",
    "\n",
    "                        #need to do lines below because didn't save testlog explicilty in setting up leave-one-out\n",
    "                        traintestlog = copy.deepcopy(trainlog)\n",
    "                        testlog = []\n",
    "                        for i in range(0, 12000-len(trainlog)):\n",
    "                            testlog.append(variants[variant])\n",
    "                            traintestlog.append(variants[variant])\n",
    "\n",
    "                        trvar = get_variants_list(trainlog)\n",
    "                        simvar = get_variants_list(simlog)\n",
    "                        tevar = [variants[variant]] #this is unique tot eh one-hot seting, needs to be altered when using bigger test set\n",
    "\n",
    "                        #get counts for the simulated log\n",
    "                        occ_each_trvar_sim = get_counts(simlog, trvar)\n",
    "                        occ_each_tevar_sim = get_counts(simlog, tevar)\n",
    "                        occ_each_simvar_sim = get_counts(simlog, simvar)\n",
    "\n",
    "                        #get counts for the train log\n",
    "                        occ_each_trvar_tr = get_counts(trainlog, trvar)\n",
    "\n",
    "                        #get counts for the test log\n",
    "                        occ_each_tevar_te = get_counts(testlog, tevar)\n",
    "\n",
    "                        #get counts for the train test log\n",
    "                        occ_each_simvar_trte = get_counts(traintestlog, simvar)\n",
    "\n",
    "                        fitness_arr.append(get_fitness(occ_each_trvar_sim, occ_each_trvar_tr))\n",
    "                        precision_arr.append(get_precision(occ_each_simvar_sim, occ_each_simvar_trte))\n",
    "                        generalization_arr.append(get_generalization(occ_each_tevar_sim, occ_each_tevar_te))\n",
    "\n",
    "                    av_gen = sum(generalization_arr)/8.0\n",
    "                    av_prec = sum(precision_arr)/8.0\n",
    "                    av_fit = sum(fitness_arr)/8.0\n",
    "\n",
    "                    new_row = {'BiDirec':str(bi),'Num_layers':num_layers, 'Size':layersize, 'Dropout':dropout, 'Reg':reg, 'Gen.':av_gen, 'Prec.':av_prec, 'Fit.':av_fit}\n",
    "\n",
    "                    df = df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average'] = df[['Gen.', 'Prec.', 'Fit.']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prec = df.sort_values('Prec.', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = df.sort_values('Gen.', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit = df.sort_values('Fit.', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_average = df.sort_values('average', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BiDirec Num_layers Size  Dropout      Reg      Gen.     Prec.      Fit.  \\\n",
      "140   False          1   32      0.0  0.00000  0.008831  0.950646  0.958492   \n",
      "180   False          2   16      0.0  0.00000  0.001087  0.948000  0.955887   \n",
      "40     True          1   64      0.0  0.00000  0.025378  0.948031  0.955726   \n",
      "221   False          2   64      0.0  0.00001  0.031826  0.947719  0.955350   \n",
      "160   False          1   64      0.0  0.00000  0.001330  0.947292  0.955170   \n",
      "120   False          1   16      0.0  0.00000  0.015171  0.947073  0.954837   \n",
      "205   False          2   32      0.2  0.00000  0.000000  0.946885  0.954773   \n",
      "121   False          1   16      0.0  0.00001  0.125307  0.947740  0.954563   \n",
      "60     True          2   16      0.0  0.00000  0.040053  0.946917  0.954425   \n",
      "45     True          1   64      0.2  0.00000  0.014086  0.946271  0.954035   \n",
      "\n",
      "      average  \n",
      "140  0.639323  \n",
      "180  0.634991  \n",
      "40   0.643045  \n",
      "221  0.644965  \n",
      "160  0.634597  \n",
      "120  0.639027  \n",
      "205  0.633886  \n",
      "121  0.675870  \n",
      "60   0.647132  \n",
      "45   0.638131  \n"
     ]
    }
   ],
   "source": [
    "print(df_fit[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BiDirec Num_layers Size  Dropout      Reg      Gen.     Prec.      Fit.  \\\n",
      "136   False          1   16      0.6  0.00001  0.560815  0.884219  0.886977   \n",
      "98     True          2   32      0.6  0.00100  0.751368  0.880063  0.881209   \n",
      "223   False          2   64      0.0  0.00100  0.597618  0.867198  0.869433   \n",
      "78     True          2   16      0.6  0.00100  0.740026  0.858719  0.859720   \n",
      "169   False          1   64      0.2  0.01000  0.745886  0.810229  0.810819   \n",
      "203   False          2   32      0.0  0.00100  0.536531  0.805760  0.808019   \n",
      "129   False          1   16      0.2  0.01000  0.733395  0.775406  0.775775   \n",
      "137   False          1   16      0.6  0.00010  0.528078  0.773229  0.775327   \n",
      "183   False          2   16      0.0  0.00100  0.531767  0.770010  0.771998   \n",
      "174   False          1   64      0.4  0.01000  0.717175  0.752125  0.752445   \n",
      "138   False          1   16      0.6  0.00100  0.561673  0.736354  0.737868   \n",
      "49     True          1   64      0.2  0.01000  0.602309  0.701906  0.702819   \n",
      "208   False          2   32      0.2  0.00100  0.522315  0.686438  0.687820   \n",
      "188   False          2   16      0.2  0.00100  0.510343  0.623594  0.624593   \n",
      "29     True          1   32      0.2  0.01000  0.542598  0.604385  0.604911   \n",
      "196   False          2   16      0.6  0.00001  0.408955  0.580865  0.582296   \n",
      "9      True          1   16      0.2  0.01000  0.413406  0.564573  0.565850   \n",
      "154   False          1   32      0.4  0.01000  0.467280  0.537813  0.538425   \n",
      "195   False          2   16      0.6  0.00000  0.327776  0.511354  0.512855   \n",
      "233   False          2   64      0.4  0.00100  0.396941  0.476896  0.477551   \n",
      "54     True          1   64      0.4  0.01000  0.362983  0.472729  0.473679   \n",
      "14     True          1   16      0.4  0.01000  0.425657  0.425687  0.425724   \n",
      "34     True          1   32      0.4  0.01000  0.339536  0.324198  0.324131   \n",
      "59     True          1   64      0.6  0.01000  0.277168  0.272958  0.272934   \n",
      "213   False          2   32      0.4  0.00100  0.239473  0.257333  0.257487   \n",
      "39     True          1   32      0.6  0.01000  0.173385  0.244208  0.244824   \n",
      "197   False          2   16      0.6  0.00010  0.260491  0.235990  0.235798   \n",
      "19     True          1   16      0.6  0.01000  0.204029  0.195406  0.195325   \n",
      "134   False          1   16      0.4  0.01000  0.142705  0.158083  0.158219   \n",
      "193   False          2   16      0.4  0.00100  0.096001  0.117146  0.117334   \n",
      "179   False          1   64      0.6  0.01000  0.074659  0.088917  0.089037   \n",
      "238   False          2   64      0.6  0.00100  0.092022  0.084458  0.084405   \n",
      "218   False          2   32      0.6  0.00100  0.077967  0.071448  0.071395   \n",
      "198   False          2   16      0.6  0.00100  0.046598  0.046000  0.045998   \n",
      "139   False          1   16      0.6  0.01000  0.039617  0.039625  0.039617   \n",
      "159   False          1   32      0.6  0.01000  0.030372  0.035865  0.035910   \n",
      "84     True          2   32      0.0  0.01000  0.000000  0.000000  0.000000   \n",
      "69     True          2   16      0.2  0.01000  0.000000  0.000000  0.000000   \n",
      "229   False          2   64      0.2  0.01000  0.000000  0.000000  0.000000   \n",
      "64     True          2   16      0.0  0.01000  0.000000  0.000000  0.000000   \n",
      "74     True          2   16      0.4  0.01000  0.000000  0.000000  0.000000   \n",
      "194   False          2   16      0.4  0.01000  0.000000  0.000000  0.000000   \n",
      "234   False          2   64      0.4  0.01000  0.000000  0.000000  0.000000   \n",
      "189   False          2   16      0.2  0.01000  0.000000  0.000000  0.000000   \n",
      "79     True          2   16      0.6  0.01000  0.000000  0.000000  0.000000   \n",
      "119    True          2   64      0.6  0.01000  0.000000  0.000000  0.000000   \n",
      "224   False          2   64      0.0  0.01000  0.000000  0.000000  0.000000   \n",
      "89     True          2   32      0.2  0.01000  0.000000  0.000000  0.000000   \n",
      "219   False          2   32      0.6  0.01000  0.000000  0.000000  0.000000   \n",
      "199   False          2   16      0.6  0.01000  0.000000  0.000000  0.000000   \n",
      "94     True          2   32      0.4  0.01000  0.000000  0.000000  0.000000   \n",
      "214   False          2   32      0.4  0.01000  0.000000  0.000000  0.000000   \n",
      "184   False          2   16      0.0  0.01000  0.000000  0.000000  0.000000   \n",
      "99     True          2   32      0.6  0.01000  0.000000  0.000000  0.000000   \n",
      "104    True          2   64      0.0  0.01000  0.000000  0.000000  0.000000   \n",
      "109    True          2   64      0.2  0.01000  0.000000  0.000000  0.000000   \n",
      "209   False          2   32      0.2  0.01000  0.000000  0.000000  0.000000   \n",
      "204   False          2   32      0.0  0.01000  0.000000  0.000000  0.000000   \n",
      "114    True          2   64      0.4  0.01000  0.000000  0.000000  0.000000   \n",
      "239   False          2   64      0.6  0.01000  0.000000  0.000000  0.000000   \n",
      "\n",
      "      average  \n",
      "136  0.777337  \n",
      "98   0.837547  \n",
      "223  0.778083  \n",
      "78   0.819488  \n",
      "169  0.788978  \n",
      "203  0.716770  \n",
      "129  0.761526  \n",
      "137  0.692211  \n",
      "183  0.691258  \n",
      "174  0.740582  \n",
      "138  0.678632  \n",
      "49   0.669011  \n",
      "208  0.632191  \n",
      "188  0.586176  \n",
      "29   0.583965  \n",
      "196  0.524038  \n",
      "9    0.514610  \n",
      "154  0.514506  \n",
      "195  0.450662  \n",
      "233  0.450463  \n",
      "54   0.436464  \n",
      "14   0.425689  \n",
      "34   0.329288  \n",
      "59   0.274353  \n",
      "213  0.251431  \n",
      "39   0.220806  \n",
      "197  0.244093  \n",
      "19   0.198253  \n",
      "134  0.153003  \n",
      "193  0.110160  \n",
      "179  0.084204  \n",
      "238  0.086962  \n",
      "218  0.073603  \n",
      "198  0.046199  \n",
      "139  0.039620  \n",
      "159  0.034049  \n",
      "84   0.000000  \n",
      "69   0.000000  \n",
      "229  0.000000  \n",
      "64   0.000000  \n",
      "74   0.000000  \n",
      "194  0.000000  \n",
      "234  0.000000  \n",
      "189  0.000000  \n",
      "79   0.000000  \n",
      "119  0.000000  \n",
      "224  0.000000  \n",
      "89   0.000000  \n",
      "219  0.000000  \n",
      "199  0.000000  \n",
      "94   0.000000  \n",
      "214  0.000000  \n",
      "184  0.000000  \n",
      "99   0.000000  \n",
      "104  0.000000  \n",
      "109  0.000000  \n",
      "209  0.000000  \n",
      "204  0.000000  \n",
      "114  0.000000  \n",
      "239  0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df_fit[-60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BiDirec Num_layers Size  Dropout      Reg      Gen.     Prec.      Fit.  \\\n",
      "140   False          1   32      0.0  0.00000  0.008831  0.950646  0.958492   \n",
      "40     True          1   64      0.0  0.00000  0.025378  0.948031  0.955726   \n",
      "180   False          2   16      0.0  0.00000  0.001087  0.948000  0.955887   \n",
      "121   False          1   16      0.0  0.00001  0.125307  0.947740  0.954563   \n",
      "221   False          2   64      0.0  0.00001  0.031826  0.947719  0.955350   \n",
      "160   False          1   64      0.0  0.00000  0.001330  0.947292  0.955170   \n",
      "120   False          1   16      0.0  0.00000  0.015171  0.947073  0.954837   \n",
      "161   False          1   64      0.0  0.00001  0.160904  0.947000  0.953523   \n",
      "60     True          2   16      0.0  0.00000  0.040053  0.946917  0.954425   \n",
      "205   False          2   32      0.2  0.00000  0.000000  0.946885  0.954773   \n",
      "56     True          1   64      0.6  0.00001  0.703229  0.946698  0.948756   \n",
      "57     True          1   64      0.6  0.00010  0.730734  0.946479  0.948336   \n",
      "31     True          1   32      0.4  0.00001  0.616395  0.946427  0.949195   \n",
      "171   False          1   64      0.4  0.00001  0.564459  0.946313  0.949553   \n",
      "45     True          1   64      0.2  0.00000  0.014086  0.946271  0.954035   \n",
      "5      True          1   16      0.2  0.00000  0.016410  0.946063  0.953814   \n",
      "51     True          1   64      0.4  0.00001  0.629043  0.946000  0.948668   \n",
      "110    True          2   64      0.4  0.00000  0.005746  0.945958  0.953795   \n",
      "116    True          2   64      0.6  0.00001  0.676560  0.945938  0.948227   \n",
      "20     True          1   32      0.0  0.00000  0.034784  0.945906  0.953525   \n",
      "41     True          1   64      0.0  0.00001  0.353849  0.945781  0.950698   \n",
      "11     True          1   16      0.4  0.00001  0.511481  0.945490  0.949162   \n",
      "151   False          1   32      0.4  0.00001  0.353139  0.945406  0.950372   \n",
      "141   False          1   32      0.0  0.00001  0.169410  0.945333  0.951800   \n",
      "130   False          1   16      0.4  0.00000  0.035690  0.945313  0.952905   \n",
      "165   False          1   64      0.2  0.00000  0.004630  0.945250  0.953095   \n",
      "201   False          2   32      0.0  0.00001  0.031250  0.945177  0.952799   \n",
      "176   False          1   64      0.6  0.00001  0.537030  0.945125  0.948564   \n",
      "50     True          1   64      0.4  0.00000  0.024347  0.945052  0.952723   \n",
      "112    True          2   64      0.4  0.00010  0.763038  0.944885  0.946411   \n",
      "\n",
      "      average  \n",
      "140  0.639323  \n",
      "40   0.643045  \n",
      "180  0.634991  \n",
      "121  0.675870  \n",
      "221  0.644965  \n",
      "160  0.634597  \n",
      "120  0.639027  \n",
      "161  0.687143  \n",
      "60   0.647132  \n",
      "205  0.633886  \n",
      "56   0.866228  \n",
      "57   0.875183  \n",
      "31   0.837339  \n",
      "171  0.820108  \n",
      "45   0.638131  \n",
      "5    0.638762  \n",
      "51   0.841237  \n",
      "110  0.635166  \n",
      "116  0.856908  \n",
      "20   0.644738  \n",
      "41   0.750109  \n",
      "11   0.802044  \n",
      "151  0.749639  \n",
      "141  0.688848  \n",
      "130  0.644636  \n",
      "165  0.634325  \n",
      "201  0.643075  \n",
      "176  0.810240  \n",
      "50   0.640707  \n",
      "112  0.884778  \n"
     ]
    }
   ],
   "source": [
    "print(df_prec[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BiDirec Num_layers Size  Dropout     Reg      Gen.     Prec.      Fit.  \\\n",
      "77     True          2   16      0.6  0.0001  0.855973  0.930375  0.931043   \n",
      "53     True          1   64      0.4  0.0010  0.855420  0.941802  0.942588   \n",
      "28     True          1   32      0.2  0.0010  0.849285  0.937323  0.938114   \n",
      "113    True          2   64      0.4  0.0010  0.847689  0.942260  0.943103   \n",
      "153   False          1   32      0.4  0.0010  0.844788  0.941958  0.942820   \n",
      "178   False          1   64      0.6  0.0010  0.842311  0.941542  0.942442   \n",
      "38     True          1   32      0.6  0.0010  0.841955  0.933323  0.934156   \n",
      "93     True          2   32      0.4  0.0010  0.840556  0.940958  0.941843   \n",
      "187   False          2   16      0.2  0.0001  0.834092  0.942375  0.943324   \n",
      "88     True          2   32      0.2  0.0010  0.832589  0.943240  0.944227   \n",
      "\n",
      "      average  \n",
      "77   0.905797  \n",
      "53   0.913270  \n",
      "28   0.908241  \n",
      "113  0.911017  \n",
      "153  0.909855  \n",
      "178  0.908765  \n",
      "38   0.903145  \n",
      "93   0.907786  \n",
      "187  0.906597  \n",
      "88   0.906685  \n"
     ]
    }
   ],
   "source": [
    "print(df_gen[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BiDirec Num_layers Size  Dropout   Reg      Gen.     Prec.      Fit.  \\\n",
      "35     True          1   32      0.6  0.00  0.003571  0.944781  0.952619   \n",
      "80     True          2   32      0.0  0.00  0.002917  0.943240  0.951073   \n",
      "115    True          2   64      0.6  0.00  0.002475  0.937896  0.945690   \n",
      "95     True          2   32      0.6  0.00  0.002381  0.939135  0.946938   \n",
      "170   False          1   64      0.4  0.00  0.001374  0.944479  0.952338   \n",
      "160   False          1   64      0.0  0.00  0.001330  0.947292  0.955170   \n",
      "105    True          2   64      0.2  0.00  0.001238  0.942167  0.950002   \n",
      "200   False          2   32      0.0  0.00  0.001190  0.943875  0.951726   \n",
      "180   False          2   16      0.0  0.00  0.001087  0.948000  0.955887   \n",
      "225   False          2   64      0.2  0.00  0.000000  0.943635  0.951499   \n",
      "90     True          2   32      0.4  0.00  0.000000  0.939510  0.947337   \n",
      "89     True          2   32      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "84     True          2   32      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "229   False          2   64      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "224   False          2   64      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "194   False          2   16      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "230   False          2   64      0.4  0.00  0.000000  0.941760  0.949606   \n",
      "79     True          2   16      0.6  0.01  0.000000  0.000000  0.000000   \n",
      "74     True          2   16      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "94     True          2   32      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "69     True          2   16      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "64     True          2   16      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "234   False          2   64      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "104    True          2   64      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "99     True          2   32      0.6  0.01  0.000000  0.000000  0.000000   \n",
      "220   False          2   64      0.0  0.00  0.000000  0.944542  0.952409   \n",
      "219   False          2   32      0.6  0.01  0.000000  0.000000  0.000000   \n",
      "100    True          2   64      0.0  0.00  0.000000  0.940365  0.948194   \n",
      "189   False          2   16      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "109    True          2   64      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "214   False          2   32      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "114    True          2   64      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "210   False          2   32      0.4  0.00  0.000000  0.940302  0.948133   \n",
      "209   False          2   32      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "119    True          2   64      0.6  0.01  0.000000  0.000000  0.000000   \n",
      "205   False          2   32      0.2  0.00  0.000000  0.946885  0.954773   \n",
      "204   False          2   32      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "199   False          2   16      0.6  0.01  0.000000  0.000000  0.000000   \n",
      "184   False          2   16      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "239   False          2   64      0.6  0.01  0.000000  0.000000  0.000000   \n",
      "\n",
      "      average  \n",
      "35   0.633657  \n",
      "80   0.632410  \n",
      "115  0.628687  \n",
      "95   0.629485  \n",
      "170  0.632730  \n",
      "160  0.634597  \n",
      "105  0.631135  \n",
      "200  0.632264  \n",
      "180  0.634991  \n",
      "225  0.631711  \n",
      "90   0.628949  \n",
      "89   0.000000  \n",
      "84   0.000000  \n",
      "229  0.000000  \n",
      "224  0.000000  \n",
      "194  0.000000  \n",
      "230  0.630455  \n",
      "79   0.000000  \n",
      "74   0.000000  \n",
      "94   0.000000  \n",
      "69   0.000000  \n",
      "64   0.000000  \n",
      "234  0.000000  \n",
      "104  0.000000  \n",
      "99   0.000000  \n",
      "220  0.632317  \n",
      "219  0.000000  \n",
      "100  0.629519  \n",
      "189  0.000000  \n",
      "109  0.000000  \n",
      "214  0.000000  \n",
      "114  0.000000  \n",
      "210  0.629478  \n",
      "209  0.000000  \n",
      "119  0.000000  \n",
      "205  0.633886  \n",
      "204  0.000000  \n",
      "199  0.000000  \n",
      "184  0.000000  \n",
      "239  0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df_gen[-40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BiDirec Num_layers Size  Dropout      Reg      Gen.     Prec.      Fit.  \\\n",
      "53     True          1   64      0.4  0.00100  0.855420  0.941802  0.942588   \n",
      "113    True          2   64      0.4  0.00100  0.847689  0.942260  0.943103   \n",
      "153   False          1   32      0.4  0.00100  0.844788  0.941958  0.942820   \n",
      "178   False          1   64      0.6  0.00100  0.842311  0.941542  0.942442   \n",
      "28     True          1   32      0.2  0.00100  0.849285  0.937323  0.938114   \n",
      "93     True          2   32      0.4  0.00100  0.840556  0.940958  0.941843   \n",
      "88     True          2   32      0.2  0.00100  0.832589  0.943240  0.944227   \n",
      "187   False          2   16      0.2  0.00010  0.834092  0.942375  0.943324   \n",
      "77     True          2   16      0.6  0.00010  0.855973  0.930375  0.931043   \n",
      "38     True          1   32      0.6  0.00100  0.841955  0.933323  0.934156   \n",
      "73     True          2   16      0.4  0.00100  0.830705  0.937542  0.938456   \n",
      "191   False          2   16      0.4  0.00001  0.818801  0.941875  0.942947   \n",
      "173   False          1   64      0.4  0.00100  0.817508  0.942365  0.943482   \n",
      "108    True          2   64      0.2  0.00100  0.829980  0.935635  0.936575   \n",
      "117    True          2   64      0.6  0.00010  0.809790  0.944802  0.945982   \n",
      "97     True          2   32      0.6  0.00010  0.810883  0.942635  0.943765   \n",
      "107    True          2   64      0.2  0.00010  0.802674  0.943260  0.944490   \n",
      "128   False          1   16      0.2  0.00100  0.803452  0.941188  0.942410   \n",
      "148   False          1   32      0.2  0.00100  0.797841  0.941927  0.943197   \n",
      "63     True          2   16      0.0  0.00100  0.802044  0.939500  0.940698   \n",
      "68     True          2   16      0.2  0.00100  0.798760  0.941031  0.942264   \n",
      "58     True          1   64      0.6  0.00100  0.801496  0.938490  0.939679   \n",
      "52     True          1   64      0.4  0.00010  0.789158  0.943427  0.944762   \n",
      "92     True          2   32      0.4  0.00010  0.787632  0.943115  0.944490   \n",
      "33     True          1   32      0.4  0.00100  0.796905  0.937760  0.938996   \n",
      "103    True          2   64      0.0  0.00100  0.790999  0.939969  0.941298   \n",
      "8      True          1   16      0.2  0.00100  0.789606  0.938385  0.939669   \n",
      "158   False          1   32      0.6  0.00100  0.797991  0.933198  0.934396   \n",
      "168   False          1   64      0.2  0.00100  0.778264  0.940573  0.942000   \n",
      "67     True          2   16      0.2  0.00010  0.772009  0.942542  0.944059   \n",
      "\n",
      "      average  \n",
      "53   0.913270  \n",
      "113  0.911017  \n",
      "153  0.909855  \n",
      "178  0.908765  \n",
      "28   0.908241  \n",
      "93   0.907786  \n",
      "88   0.906685  \n",
      "187  0.906597  \n",
      "77   0.905797  \n",
      "38   0.903145  \n",
      "73   0.902234  \n",
      "191  0.901208  \n",
      "173  0.901118  \n",
      "108  0.900730  \n",
      "117  0.900191  \n",
      "97   0.899095  \n",
      "107  0.896808  \n",
      "128  0.895683  \n",
      "148  0.894322  \n",
      "63   0.894081  \n",
      "68   0.894019  \n",
      "58   0.893222  \n",
      "52   0.892449  \n",
      "92   0.891745  \n",
      "33   0.891220  \n",
      "103  0.890755  \n",
      "8    0.889220  \n",
      "158  0.888528  \n",
      "168  0.886946  \n",
      "67   0.886203  \n"
     ]
    }
   ],
   "source": [
    "print(df_average[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Results_grid_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Size', ylabel='Prec.'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot('Size', 'Prec.', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
